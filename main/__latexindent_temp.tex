%\documentclass[oneside,reqno,12pt]{amsart}





\documentclass[b5paper,draft,openbib,12pt]{memoir} 
%\documentclass[b5paper,openbib,12pt]{memoir} 


 
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{bbm}
\usepackage{graphicx}
\usepackage{epsfig, float}
\usepackage{pgf,tikz,pgfplots}
\usepackage{slashed}
\usepackage{eurosym}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathabx}
\usepackage{enumitem}
\usepackage{longtable}
\usepackage[mathscr]{eucal}
\usepackage{cancel}
\usepackage{lipsum}


%commutative diagram
\usepackage{amsmath,amscd}
%picture
\usepackage{wrapfig}

\usepackage[unicode=true, pdfusetitle, bookmarks=true,
  bookmarksnumbered=false, bookmarksopen=false, breaklinks=true, 
  pdfborder={0 0 0}, backref=false, colorlinks=true, linkcolor=blue,
  citecolor=blue, urlcolor=blue]{hyperref}
\hypersetup{final}
%needed to have hyperlinks in draft mode


% \numberwithin{equation}{section}
\allowdisplaybreaks[1]

\newtheorem{Def}{Definition}
\newtheorem{axiom}[Def]{Axiom}
\newtheorem{Conj}[Def]{Conjecture}
\newtheorem{Thm}[Def]{Theorem}
\newtheorem{Prp}[Def]{Proposition}
\newtheorem{Lemma}[Def]{Lemma}
\newtheorem{Remark}[Def]{Remark}
\newtheorem{Corollary}[Def]{Corollary}
\newtheorem{Example}[Def]{Example}
\newtheorem{Assumption}[Def]{Assumption}
\newenvironment{Remarks}[1][Remarks:]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\D}{\dagger}
\newcommand{\tc}{\textcolor{red}}
\newcommand{\tb}{\textcolor{blue}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\id}{\mathbbm{1}}
\newcommand{\hastobe}{\stackrel{!}{=}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vb}{\mathbf{b}}

\newcommand{\ret}{{\textup ret}}
\newcommand{\sym}{{\textup sym}}
\newcommand{\adv}{{\textup adv}}
\newcommand{\free}{{\textup free}}
\newcommand{\Dirac}{{\textup Dirac}}
\newcommand{\vertiii}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}
\newcommand{\Banach}{\mathscr{B}}

\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator*{\supp}{supp}
\DeclareMathOperator*{\esssup}{ess \, sup}
\DeclareMathOperator{\past}{past}
\DeclareMathOperator{\future}{future}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\erf}{erf}
\DeclareMathOperator{\erfi}{erfi}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\dotCup}{\mathop{\dot{\bigcup}}}
\DeclareMathOperator{\dotcup}{\mathop{\dot{\cup}}}
\DeclareMathOperator{\ag}{ag}
\DeclareMathOperator{\AG}{AG}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\ev}{ev}
\DeclareMathOperator{\odd}{odd}

\newcommand{\equaltext}[1]{\ensuremath{\stackrel{\text{#1}}{=}}}
\newcommand{\letext}[1]{\ensuremath{\stackrel{\text{#1}}{\le}}}
\newcommand{\Conv}{\mathop{\scalebox{1.7}{\raisebox{-0.2ex}{\(\ast\)}}}}
\newcommand{\CONV}{\mathop{\scalebox{3.0}{\raisebox{-0.2ex}{\(\ast\)}}}}

% Annotations
\usepackage[colorinlistoftodos,shadow,textsize=scriptsize,textwidth=2.5cm]{todonotes}
\newcommand{\noch}[1]{ \todo[color=blue!20]{Todo: #1} }
\newcommand{\black}{ \color{black} }


\renewcommand\chapterheadstart{
\vspace *{\beforechapskip}
\hrulefill
\vskip 0pt
}

\renewcommand\afterchaptertitle{%
\vskip 0pt
\hrulefill
\par \nobreak  \vskip  \afterchapskip  } 


\setsecnumdepth{all}


%all divisions are numbered in the text body

\parindent 0cm

\begin{document}



\frontmatter
%
%\noindent
%\begin{center}
%    \textsc{ \small{Description of the dissertation project of Markus Nöth,
%        Ludwig-Maximilians University of Munich} \\
%        \smallskip
%\large{ Electron-Positron Pair Creation in External Fields} \\
%\small{Rigorous Control of the Scattering-Matrix Expansion}}
%    \vskip.3cm
%    \small
% supervisor:    D.-A.\ Deckert (LMU)
% \vskip0.4cm
\title{Electron-Positron Pair Creation in External~Fields}
%\subtitle{\footnotesize{Rigorous Control of the Scattering-Matrix Expansion}}
\author{M. Nöth}
\maketitle

\begin{abstract}
\lipsum[1] 
 \end{abstract}


%\vskip.5cm
%\thispagestyle{empty}


\tableofcontents

\newpage



\mainmatter

\chapter{Introduction}

%\show\afterchaptertitle
\noch{Historische Einleitung durch Anfänge relativistischer Quantenphysik, Strahlungskatastrophe (unbounded below). No potentials resultat Lukas}
\todo{general: replace \(S\) 
in later chapter something similar but different}
\lipsum[2-3]


\chapter[Direct Interaction in Relativistic Quantum Mechanics][Direct Interaction]{Direct Interaction in Relativistic Quantum Mechanics}
As we have seen in the last chapter, having interaction mediated by potentials 
in a set of Dirac equations does not seem to be a viable option.
One alternative approach to this problem is to reformulate Diracs equation as 
an integral equation and to introduce interaction analogous 
to how a potential would act in this formulation. 
This chapter is based on  the preprints \cite{selfDirac,selfKG},
the results presented in this chapter are a result of the joint
research of Matthias Lienert and the author of this thesis. 
While these results fall short of establishing the existence 
of a physically accurate alternative approach to 
a relativistic quantum mechanical theory, they do describe 
self consistent relativistic interacting quantum mechanical 
toy models living in three spacial and one temporal dimension.


For the benefit of the unfamiliar reader, we will first follow the 
heuristic derivation of this type of equation
in \cite{direct_interaction_quantum}, then briefly review 
the mathematical results that 
had been established in the past on this subject 
and finally 
discuss results appearing in \cite{selfDirac,selfKG}.


\section{Overview}
\subsection{Derivation}

We now follow the heuristic derivation of \cite{direct_interaction_quantum} 
of an equation for a multi-time wave function for two particles  
that expresses direct interaction along light-like configurations. 
The derivation is organised as follows: We start out 
reformulating Diracs equation
for a single particle 
as an integral equation. The reformulated version is then 
extended to  two particles
in an Poincaré invariant manner. Extending the 
equation is conveniently done in 
the framework of multi-time wavefunctions.


Diracs equation for one particle subject to an 
external potential \(V\) takes the form
\begin{equation}
	i \partial_t \phi(t,\vx) = \big( H^\free + V(t,\vx) \big) \phi(t,\vx),
	\label{eq:singlepartschroed}
\end{equation}
here \(\phi\) denotes the wavefunction in question, \(\vx\in\mathbb{R}^3, t\in\mathbb{R}\)
and \(H^\free\) is the Hamiltonian associated with a free Dirac particle.
We denote by \(S^\ret\) the retarded Green's function of the non interacting Dirac equation,
that is \(S^\ret\) satisfies
\begin{align}\label{eq:schroedgreensfn}
  \big( i \partial_t - H^\free \big) S^\ret &= \delta^4,\\
  S^\ret(t,\vx) &= 0 \quad \text{for }\, t<0.
\end{align}
Then inverting the differential operator \(i\partial_t -H^\free\) in  \eqref{eq:singlepartschroed}
results in 
\begin{equation}
	\phi(t,\vx) = \phi^\free(t,\vx) + \int_{t^0}^\infty \! dt' \int d^3 \vx' \, S^\ret(t-t',\vx-\vx') V(t',\vx') \phi(t',\vx'),
	\label{eq:singlepartint}
\end{equation}
where $\phi^\free$ 
is the solution of the non interacting equation subject to the 
initial condition \(\phi^\free(t_0)=\phi_0\). 
Equations \eqref{eq:singlepartint} and \eqref{eq:singlepartschroed} subject to 
\(\phi(t_0)=\phi_0\) yield equivalent 
descriptions, as can be verified directly: An action of \(i\partial_t -H^\free\) on 
\eqref{eq:singlepartint} shows that a solution thereof also solves \eqref{eq:singlepartschroed}.
Also the initial condition is fulfilled, as the integral term vanishes for \(t=t_0\).
Conversely equation \eqref{eq:singlepartschroed} can be considered a free Dirac equation 
involving an inhomogeneous term of the form \(V \phi\), whose solutions are known to be
of the form \eqref{eq:singlepartint}.
Executing the analogous procedure for the two particle Dirac equation

\begin{equation}
	i \partial_t \phi(t,\vx_1,\vx_2) = \big(H_1^\free + H_2^\free + V(t,\vx_1,\vx_2)\big) \phi(t,\vx_1,\vx_2),
	\label{eq:twopartschroed}
\end{equation}
subject to the initial condition  \(\phi(t_0)=\phi_0\), 
results in the integral equation
\begin{align}
	\phi(t,\vx_1,\vx_2) = \phi^\free&(t,\vx_1,\vx_2) + \int_{t_0}^\infty \! dt' \! \int d^3 \vx_1'\,  d^3 \vx_2'\, S_1^\ret(t-t',\vx_1-\vx_1') \nonumber\\
	\times&S_2^\ret(t-t',\vx_2-\vx_2') V(t',\vx_1',\vx_2') \phi(t',\vx_1',\vx_2'),
	\label{eq:twopartschroedint}
\end{align}
where  now,
$\phi^\free(t)$ is a solution to the free Dirac 
equation for two particles subject
to \(\phi^\free (t_0)=\phi_0\) and \(S_k^\ret\) is 
the retarded Green's function
of the free Dirac equation of particle number \(k\). 
Here it is crucial to notice that
The Green's function of the free two particle Dirac equation 
factorises into a product 
of two Green's functions of the Dirac equation for one particle.

Since equation \eqref{eq:twopartschroedint} contains 
only one temporal variable, but six spatial ones,
it is not obvious how it might be considered a 
relativistic equation at all. Now, we will 
generalise to two particles, but before we do so 
let us first rewrite equation 
\eqref{eq:singlepartint} in a more suggestive way:
\begin{equation}
	\psi(x) = \psi^\free(x) + \int d^4 x' \, S^\ret(x-x') V(x') \psi(x'),
	\label{eq:singlepartintspacetime}
\end{equation}
where non bold letters denote elements of Minkowski 
spacetime and we replaced \(\phi\)
by \(\psi\) in order to make a visible switch to 
a relativistic notation. 
Furthermore we replaced the lower bound in the 
temporal integral domain by \(-\infty\)
in order to render the total domain of integral Poincaré invariant.

Equation \eqref{eq:singlepartintspacetime} 
suggests to write down the
following generalisation
\begin{align}\label{eq:twopartintgeneral}
  \psi(x_1,x_2) &= \psi^\free(x_1,x_2) \\\nonumber
  &+ \int d^4 x_1'\, d^4 x_2' \, S^\ret_1(x_1-x_1') S^\ret_2(x_2-x_2') K(x_1',x_2') \psi(x_1',x_2').
\end{align}
we integrate over all of \(\mathbb{R}^8\) and \(\psi^\free\) a solution of the free 
Dirac equation both in \(x_1\) and \(x_2\) and their respective spinor indices:
\begin{align}
  D_1  \psi^\free(x_1,x_2) = 0,\\
  D_2  \psi^\free(x_1,x_2) = 0.
	\label{eq:freemultitime}
\end{align}
For the object \(K\), called the ''interaction kernel``, the optimal choice is not yet known.
However, for \eqref{eq:twopartintgeneral} to be Poincaré invariant, it should be invariant
itself. A simple way to ensure this is to let it only depend directly 
the squared Minkowski distance \((x_1-x_2)^2\). A choice that shows some resemblence of 
Wheeler-Feynman electrodynamics is
\begin{equation}
	K(x_1,x_2) = i\, \frac{e_1e_2}{4\pi} \gamma_1^\mu \gamma_{2,\mu} \; \delta((x_1-x_2)^2).
	\label{eq:twopartkernel}
\end{equation}
In an equation incorporating \eqref{eq:twopartkernel} the interaction between the particles 
happens along light-like distances. The constant in front of \eqref{eq:twopartkernel}
is fixed by the non-relativistic limit, recovering an equation very much like the Breit equation,
see \cite[section 3.6]{lienertfirst}.

Summarising, we arrived at the equation
\begin{align}\label{eq:twopartint}
  \psi&(x_1,x_2) = \psi^\free(x_1,x_2)+i\, \frac{e_1e_2}{4\pi} \int d^4 x_1' \, d^4 x_2' \\\nonumber
  & \times S^\ret_1(x_1-x_1') S^\ret_2(x_2-x_2') \, \gamma_1^\mu \gamma_{2,\mu}\delta((x_1'-x_2')^2) \psi(x_1',x_2').
\end{align}


Despite the fact that the motivation for \eqref{eq:twopartint} holds for Dirac particles, 
it is also conceivable to replace \(\psi\), \(S^\ret\) and all the constant factor by 
quantities related to the Klein Gordon equation and arrive at 

\begin{align}\label{eq:KGtwopartint}
  \psi&(x_1,x_2) = \psi^\free(x_1,x_2)+\lambda \int d^4 x_1' \, d^4 x_2' \\\nonumber
  & \times G^\ret_1(x_1-x_1') G^\ret_2(x_2-x_2') \,\delta((x_1'-x_2')^2) \psi(x_1',x_2'),
\end{align}
where \(\psi\) and \(\psi^\free\) are no longer spinor valued, \(\psi^\free\) is a
solution to the free Klein Gordone equation in both \(x_1\) and \(x_2\),
\begin{align}
  (\Box_{x_1} + m_1^2)\psi=0,\\
  (\Box_{x_2} + m_2^2)\psi=0
\end{align}
and \(G^\ret\) is the retarded Green's function of the Klein Gordon equation.

In fact,  most of the rigorous results about equations of a similar 
type as the ones motivated in this chapter are about the Klein 
Gordon version \eqref{eq:KGtwopartint}.


\subsection{Previous Results on Directly Interacting Particles}
In this section we summarise the most important existence results 
on equations of the type of 
\eqref{eq:twopartintgeneral}. Because this line of work is still 
fairely young,
it can still readily be summarised. The results are taken 
from \cite{lienertfirst} and 
\cite{lienertcurved}. The theorems are about the Klein-Gordon case, i.e.
slightly different versions of equations
of the type of \eqref{eq:KGtwopartint}. I tried to contain the 
necessary notation to within each
of the theorems. Mentioned below are only the theorems that are about a 
four dimensional spacetime; however,  
there are also results concerning lower dimensions, 
the interested reader 
is refered to \cite{direct_interaction_quantum,lienertcurved}. The versions of 
equation \eqref{eq:KGtwopartint} in the theorems are 
considerably modified:

\begin{enumerate}[label=\Alph*)]
\item \label{matt simplifying assumption 1} The spacetime of equation \eqref{eq:KGtwopartint} 
is \(\mathbb{R}^4\), i.e. Minkowski spacetime.
All the rigorous results concerning vanishing curvature so far 
are about
\(\mathbb{R}^+\times\mathbb{R}^3=:\frac{1}{2}\mathbb{M}\). 
That is, there is a beginning in time. This modification has 
technical reasons. However, as current 
cosmological models of our universe do have a beginning in 
time this modification does not necessarily
mean that the equation can no longer describe 
certain aspects of physics. As these cosmological models 
have nonzero curature the authors of \cite{lienertcurved} 
have shown existence of solutions of versions of 
equation \eqref{eq:KGtwopartint}
on Friedmann-Lemaître-Robertson-Walker (FLRW) spacetime. In 
section \ref{sec:direct dirac} and 
\ref{sec:KG lightcones} we will also employ this 
simplification and show existence on this spacetime. This is not 
an attempt to treat 
general curved spacetimes, it is done as an act of consistency. 
We introduce a beginning in time 
and try to justify this by cosmological arguments and hence 
we treat a spacetime commonly used
in cosmology.
\item \label{matt simplifying assumption 2} The interaction kernel \(K\) which we motivated to be 
proportional to \(\delta((x_1-x_2)^2)\)
is replaced by various less singular objects. This modification is 
purely technical and we do not 
justify it. The previous results approach the singular \(K\) 
introduced in the last section to different
degrees. In section \ref{sec:direct dirac} where we treat Dirac 
particles 
we will also use a rather soft interaction kernel. The new result 
about Klein Gordon particles presented in 
section \ref{sec:KG lightcones} employs the fully 
singular \(\delta((x_1-x_2)^2)\) kernel. 
\end{enumerate}

%\begin{Thm}[Thm 3.1 of \cite{lienertfirst}]
%Let \(T>0\), \(M,N\in\mathbb{N}\), consider 
%the Banach space \(\mathcal{B}=L^\infty ([0,T]^N,L^2(\mathbb{R}^M))\).
%Let \(\mathcal{L}:[0,T]^{2N}\rightarrow L^2(\mathbb{R}^{2M})\) such that 
%\begin{equation*}
%\sup_{t,t'\in[0,T]^{N}}\|\mathcal{L}(t,t')\|^2<\infty,
%\end{equation*}
%then for any \(f_0\in\mathcal{B}\) the equation
%\begin{equation*}
%f(t,x)=f_0(t,x)+ \int_0^t dt' \int dx' L(t,t',x,x') f(t',x')
%\end{equation*}
%has a unique solution \(f\in\mathcal{B}\).
%\end{Thm}

%\begin{Thm}[Thm 3.2 \((d=1)\) of \cite{lienertfirst}]
%Let \(T>0, \lambda\in\mathbb{C}, m_1,m_2\ge 0\), every
%\begin{equation*}
%\psi^\free \in \mathcal{B}_1=L^\infty ([0,T]^2,L^2(\mathbb{R}^2)),
%\end{equation*}
%and every essentially bounded \(K:\mathbb{R}^4\rightarrow \mathbb{C}\), the integral equation 
%\begin{align*}
%\psi(t_1,z_1,t_2,z_2)=\psi^\free(t_1,z_1,t_2,z_2)+\frac{\lambda}{4}\int_0^{t_1}dt_1'\int_0^{t_2}dt_2'\int dz_1'dz_2'\\
% \times H(t_1-t_1'-|z_1-z_1'|) J_0(m_1\sqrt{(t_1-t_1')^2-|z_1-z_1'|^2})\\
% \times H(t_2-t_2'-|z_2-z_2'|) J_0(m_2\sqrt{(t_2-t_2')^2-|z_2-z_2'|^2})\\
%  \times K(t_1',z_1',t_2',z_2')\psi(t_1',z_1',t_2',z_2')
%\end{align*}
%has a unique solution \(\psi \in \mathcal{B}_1\).
%\end{Thm}

%\begin{Thm}[Thm 3.3 \((d=2)\) of \cite{lienertfirst}]
%Let \(T>0, \lambda\in\mathbb{C}, m_1,m_2\ge 0\), for every essentially bounded \(K:\mathbb{R}^6\rightarrow\mathbb{C}\)
%and every \(\psi^\free \in \mathcal{B}_2=L^\infty([0,T]^2,L^2(\mathbb{R}^4))\) the equation
%\begin{align*}
%&\psi(t_1,\vx_1,t_2,\vx_2)=\psi^\free(t_1,\vx_1,t_2,\vx_2)+\frac{\lambda}{(2\pi)^2}\int_0^{t_1}dt_1'\int_0^{t_2}dt_2'\\
%&\times\int d^2\vx_1'd^2\vx_2' H(t_1-t_1'-|\vx_1-\vx_1'|)\frac{\cos(m_1\sqrt{(t_1-t_1')^2-|\vx_1-\vx_1'|^2})}{\sqrt{(t_1-t_1')^2-|\vx_1-\vx_1'|^2}}\\
%&\quad \times H(t_2-t_2'-|\vx_2-\vx_2'|)\frac{\cos(m_2\sqrt{(t_2-t_2')^2-|\vx_2-\vx_2'|^2})}{\sqrt{(t_2-t_2')^2-|\vx_2-\vx_2'|^2}} \\
%&\quad \quad\quad \quad \times K(t_1',\vx_1',t_2',\vx_2')\psi(t_1',\vx_1',t_2',\vx_2')
%\end{align*}
%has a unique solution \(\psi \in \mathcal{B}_2\).
%\end{Thm}

\begin{Thm}[Thm 3.4 \((d=3)\) of \cite{mtve}]
Let \(T>0, \lambda\in\mathbb{C}\), for every bounded \(K:\mathbb{R}^8\rightarrow\mathbb{C}\) and every
\(\psi^\free \in\mathcal{B}_3:=L^\infty([0,T]^2,L^2(\mathbb{R}^6))\)
the equation 

\begin{align*}
\psi(t_1,\vx_1,t_2,\vx_2)&=\psi^\free(t_1,\vx_1,t_2,\vx_2)+\frac{\lambda}{(4\pi)^2}\int d\vx_1' d\vx_2'\\
&\times \frac{H(t_1-|\vx_1-\vx_1'|)}{|\vx_1-\vx_1'|}\frac{H(t_2-|\vx_2-\vx_2'|)}{|\vx_2-\vx_2'|}\\
&\times K(t_1-|\vx_1-\vx_1'|,\vx_1',t_2-|\vx_2-\vx_2'|,\vx_2')\\
&\times \psi(t_1-|\vx_1-\vx_1'|,\vx_1',t_2-|\vx_2-\vx_2'|,\vx_2')
\end{align*}
has a unique solution \(\psi\in\mathcal{B}_3\).
\end{Thm}

\begin{Thm}[Thm 3.5 of \cite{mtve}]
  Let \(T>0, \lambda\in\mathbb{C}\), for every 
  bounded \(f:\mathbb{R}^8\rightarrow \mathbb{C}\) 
  and every \(\psi^\free \in \mathcal{B}_3:=L^\infty([0,T]^2,L^2(\mathbb{R}^6))\) the equation
\begin{align*}
\psi(t_1,\vx_1,t_2,\vx_2)&=\psi^\free(t_1,\vx_1,t_2,\vx_2) + \frac{\lambda}{(4\pi)^2}\int d^3\vx_1'd^3\vx_2'\\
& \times \frac{H(t_1-|\vx_1-\vx_1'|)}{|\vx_1-\vx_1'|} \frac{H(t_2-|\vx_2-\vx_2'|)}{|\vx_2-\vx_2'|}\\
&\times  \frac{f(t_1-|\vx_1-\vx_1'|,\vx_1',t_2-|\vx_2-\vx_2'|,\vx_2')}{|\vx_1'-\vx_1|}\\
& \times \psi(t_1-|\vx_1-\vx_1'|,\vx_1',t_2-|\vx_2-\vx_2'|,\vx_2'),
\end{align*}
has a unique solution \(\psi\in\mathcal{B}_3\).
\end{Thm}


The next results will be about the open FLRW spacetime.
There are also results about the closed
FLRW universe which we omit here, the reader is refered
to \cite[Thm 4.3]{lienertcurved}.
We have to introduce some notation before we can 
present them, in order to do so we follow \cite[sec 3.3]{selfKG}. 


We consider particles on a flat (FLRW) spacetime which is 
described by the metric
\begin{equation}
	ds^2 = a^2(\eta) \left( d\eta^2 - dr^2 - r^2 d \Omega^2 \right),
\end{equation}
where $\eta$ denotes conformal time, $d \Omega$ denotes the 
surface measure on $\mathbb{S}^2$ and $a(\eta)$ is the so-called 
\textit{scale function}, a continuous function with $a(0) = 0$ 
and $a(\eta) > 0$ for $\eta >0$. This form makes it obvious that 
the spacetime is conformally equivalent to a Minkowski half 
space $\tfrac{1}{2}\M$, with conformal factor $a(\eta)$.

In this spacetime the free wave equation takes the form 

\begin{equation}
	\left( \Box_g - \xi R \right) \chi = 0,
	\label{eq:conformalwaveeq}
\end{equation}
where $R$ denotes the Ricci scalar and in 1+3 dimensions $\xi = \frac{1}{6}$.

In this case, it is well-known that the Green's 
functions of \eqref{eq:conformalwaveeq} on the flat 
FLRW spacetime $\mathcal{M}$ can be obtained from those of the 
usual wave equation on $\tfrac{1}{2}\M$ as follows (using 
coordinates $x=(\eta,\vx)$ and $x'=(\eta',\vx')$ with 
$\eta,\eta' \in [0,\infty)$ and $\vx,\vx' \in \R^3$; see 
\cite{lienertcurved} for a more detailed explanation):

\begin{equation}
	G_{\mathcal{M}}(x,x') ~=~ \frac{1}{a(\eta)} \frac{1}{a(\eta')} G_{\frac{1}{2}\M}(x,x').
\end{equation}
Inserting the well-known expression for the retarded and symmetric 
Green's functions on $\tfrac{1}{2}\M$ (see \eqref{later...} below) 
yields:
\begin{align}
	G_{\mathcal{M}}^\ret(x,x') ~&=~ \frac{1}{4\pi} \frac{1}{a(\eta) a(\eta')} \frac{\delta(\eta - \eta' -|\vx-\vx'|)}{|\vx-\vx'|}\nonumber\\
G_{\mathcal{M}}^\sym(x,x') ~&=~ \frac{1}{4\pi} \frac{1}{a(\eta) a(\eta')} \delta((\eta-\eta')^2-|\vx-\vx'|^2).
\end{align}

With this information, we are ready to write down the integral 
equation on $\mathcal{M}$. The generalization of \eqref{eq:KGtwopartint} 
to curved spacetimes is straightforward: $\psi$ becomes a scalar 
function on $\mathcal{M}\times\mathcal{M}$, one exchanges the 
Minkowski spacetime volume element with 

\begin{equation}
	dV(x) = a^4(\eta) \, d\eta \, d^3 \vx,
\end{equation}
the invariant 4-volume 
element on $\mathcal{M}$, and the Green's functions on 
$\tfrac{1}{2}\M$ get replaced with those on $\mathcal{M}$ as well. 
As in the Minkowski case, the interaction kernel is given by the 
symmetric Green's function. With this, the relevant integral 
equation becomes:
\begin{align}\notag
  \psi(x,y) = \psi^\free(x,y) +& \lambda \int_{\mathcal{M}\times \mathcal{M}} dV(x) \, dV(y)~G_1^\ret(x,x')G_2^\ret(y,y') \\
  & \times G^\sym(x',y') \psi(x',y').
	\label{eq:inteqcurved}
\end{align}

For regular and only weakly singular interaction kernels $K(x,y)$ 
instead of $G^\sym(x',y')$, the problem of existence and uniqueness 
of solutions of this equation has been treated in 
\cite{lienertcurved}:
\todo{when copying KG results here omit this part here!}

\begin{Thm}[Thm 4.1 of \cite{lienertcurved}]
Let \(T>0\),\(\lambda\in\mathbb{C}\) and 
\(\mathcal{B}_3:=L^\infty([0,T]^2,L^2(\mathbb{R}^6))\). Furthermore, let \(a:[0,\infty)\rightarrow [0,\infty)\)
be a continuous function with \(a(0)=0\) and 
\(a(\eta)>0\) for \(\eta>0\), and 
\(\tilde{K}:([0,\infty)\times \mathbb{R}^{3})^2\rightarrow \mathbb{C}\)
be bounded. Then for every \(\psi^\free\) with 
\(a(\eta_1)a(\eta_2)\psi^\free\in \mathcal{B}_3 \),
the respective integral equation on the \(4-\)dimensional 
flat FLRW universe sith scale function \(a(\eta)\): 
\begin{align}\label{KG curved bounded}
  \psi(\eta_1, \vx_1,\eta_2,\vx_2)&=\psi^{\free}(\eta_1, \vx_1,\eta_2,\vx_2)
  +\frac{\lambda}{(4\pi)^2 a(\eta_1)a(\eta_2)}\\\notag
  &\quad \times \int d \vx_1' d \vx_2' 
  a^2(\eta_1-|\vx_1-\vx_1'|)a^2(\eta_2-|\vx_2-\vx_2'|)\\\notag
  &\quad \times \frac{H(\eta_1-|\vx_1-\vx_1'|)}{|\vx_1-\vx_1'|}
  \frac{H(\eta_2-|\vx_2-\vx_2'|)}{|\vx_2-\vx_2'|}\\\notag
  &\quad\times \tilde{K}(\eta_1-|\vx_1-\vx_1'|,\vx_1',\eta_2-|\vx_2-\vx_2'|,\vx_2')\\\notag
  &\quad\times \psi(\eta_1-|\vx_1-\vx_1'|,\vx_1',\eta_2-|\vx_2-\vx_2'|,\vx_2')
\end{align}
has a unique solution \(\psi\) with 
\(a(\eta_1)a(\eta_2)\psi\in \mathcal{B}_3\).
\end{Thm}

\begin{Thm}[Thm 4.2 of \cite{lienertcurved}]
  Let \(f:([0,\infty]\times \mathbb{R}^3)^2\rightarrow \mathbb{C}\) 
  be a bounded function. Then, under the same assumptions as in 
  the last theorem but with 
  \begin{equation}
  \tilde{K}(\eta_1,\vx_1,\eta_2,\vx_2)=\frac{f(\eta_1,\vx_1,\eta_1,\vx_2)}{|\vx_1-\vx_2|},
  \end{equation}
the integral equation \eqref{KG curved bounded} has a unique solution
\(\psi\) with 
\(a(\eta_1)a(\eta_2)\psi\in \mathcal{B}_3\). 
\end{Thm}


\section{Directly Interacting Dirac Particles}\label{sec:direct dirac}

In this section we prove the existence and uniqueness 
of solutions of a version of equation \eqref{eq:twopartint}
subject to similar modifications \ref{matt simplifying assumption 1}
and \ref{matt simplifying assumption 1}.
As with the results of the last chapter an analogous 
result is proven on FLRW spacetime to provide a justification 
for the cutoff in time.
We furthermore show that the solutions 
are determined by Cauchy data at the initial time; however, 
no Cauchy problem is admissible at other times. 
\subsection{Introduction}


The Dirac equation is perhaps the most important equation in relativistic quantum theory, thus it may seem surprising that no completely satisfactory mathematical mechanism of interaction has been found for it. Usually, interactions between many particles are implemented in one of the following ways: (a) adding a potential to the free Hamiltonian, (b) using a second quantized electromagnetic field which mediates the interaction. Both approaches face difficulties. Approach (a) corresponds to postulating the equation
\begin{equation}
	i \partial_t \varphi(t,\vx_1,\vx_2) = \left(H_1^\Dirac + H_2^\Dirac + V(t,\vx_1,\vx_2) \right) \varphi(t,\vx_1,\vx_2),
\label{eq:singletimedirac}
\end{equation}
where $V$ is a potential and $H_k^\Dirac$ the Dirac Hamiltonian acting on the variables of the $k$-th particle. Under appropriate circumstances, it is clear that \eqref{eq:singletimedirac} defines an interacting dynamics  (see e.g.\ \cite{dirk_martin_2018} and references therein). 
However, \eqref{eq:singletimedirac} is not Lorentz invariant.

Approach (b), on the other hand, easily leads to a Lorentz invariant 
dynamics. However, one encounters difficulties with ultraviolet 
divergences. These difficulties have led to the situation that, 
great efforts notwithstanding, it has so far only been possible to 
rigorously define a Lorentz invariant dynamics for toy models in 
1+1 and 1+2 spacetime dimensions (see e.g.\ \cite{thirring_model,
glimm_jaffe,jaffe_cft}). In 1+3 dimensions, it has been an open 
problem to prove the existence of the dynamics for any interacting 
and completely relativistic model.

In this paper, we pursue a new approach to defining interacting dynamics, neither via potentials nor via second quantized fields, but rather through \textit{direct interactions with time delay}, and prove the existence of dynamics for the simple case of two Dirac particles in 1+3 dimensions. The key innovation is to make use of \textit{multi-time wave functions}. This concept goes back to Dirac \cite{dirac_32}, played an important role in the works of Tomonaga \cite{tomonaga} and Schwinger \cite{schwinger}, has been studied by different authors over the years \cite{guenther_1952,marx_1974,schweber,drozvincent_1981,sazdjian_2bd,2bdem} and has recently undergone considerable developments \cite{nogo_potentials,qftmultitime,multitime_pair_creation,1d_model,nt_model,2bd_current_cons,deckert_nickel_2016,lpt_2017b,generalized_born,ibc_model,phd_nickel}; an overview can be found in \cite{dice_paper}. For two Dirac particles in Minkowski spacetime $\M$, a multi-time wave function is a map
\begin{equation}
	\psi : \M \times \M \rightarrow \CC^4 \otimes \CC^4\cong \CC^{16},~~~(x_1,x_2) \mapsto \psi(x_1,x_2).
\label{eq:multitimewavefn}
\end{equation}	
$\psi$ can be considered a generalization of the single-time wave function $\varphi$ in the Schr\"odinger picture, as in Eq.\ \eqref{eq:singletimedirac}. The relation of $\psi$ to $\varphi$ is straightforwardly given by
\begin{equation}
	\varphi(t,\vx_1,\vx_2) = \psi((t,\vx_1),(t,\vx_2)).
	\label{eq:singlemulti}
\end{equation}
Contrary to the single-time wave function $\varphi$ (which refers to a frame), $\psi$ is a manifestly covariant object. Under a Poincar\'{e} transformation $(a,\Lambda)$, $\psi$ transforms as
\begin{equation}
	\psi'(x_1,x_2) = S[\Lambda]\otimes S[\Lambda] \psi(\Lambda^{-1}(x_1-a), \Lambda^{-1}(x_2-a)),
\end{equation}
where $S[\Lambda]$ are the matrices appearing in the spinor representation of the Lorentz group.

For the present purposes, it is crucial that $\psi$ is defined on general space-time configurations $(x_1,x_2)\in \M\times \M$, not only on equal-time configurations as $\varphi$. By relating configurations $(x_1,x_2)$ with different time coordinates $x_1^0 \neq x_2^0$ one can express \textit{interactions with a time delay}. It has been pointed out in \cite{direct_interaction_quantum} that in this way, \textit{direct relativistic interactions} (unmediated by fields) can be expressed at the quantum level. 
In particular, it becomes possible to formulate a quantum analog of direct interactions along light cones, such as in the Wheeler-Feynman formulation of classical electrodynamics \cite{wf1,wf2}, using values of $\psi(x_1,x_2)$ with $(x_1-x_2)_\mu (x_1-x_2)^\mu = 0$. This is not directly feasible using just $\varphi$. We thus note that \textit{new kinds of interacting quantum dynamics can be defined using a multi-time wave function}.

An interesting class of such dynamics has recently been suggested 
in \cite{direct_interaction_quantum} and has been subsequently 
analyzed rigorously in \cite{mtve,lienertcurved}: 
\textit{multi-time integral equations}. 
But why study integral equations instead of PDEs?  To answer this question, note that the initial value problem $\varphi(0,\vx_1,\vx_2) = \psi_0(\vx_1,\vx_2)$ of the single-time Schr\"odinger equation \eqref{eq:singletimedirac} can equivalently be formulated as the following integral equation:
\begin{align}\notag
  \varphi(t,\vx_1,\vx_2) =& \varphi^\free(t,\vx_1,\vx_2) + \int_0^\infty \!\! dt' \int d^3 \vx_1' \, d^3 \vx_2' \,\\
   &\times\gamma_1^0 S_1^\ret(t-t',\vx_1-\vx_1') \gamma_2^0 S_2^\ret(t-t',\vx_2-\vx_2') \nonumber\\
&\times V(t',\vx_1',\vx_2')\varphi(t',\vx_1',\vx_2'),
	\label{eq:singletimediracint}
\end{align}
where $\varphi^\free$ is the solution of the same initial value problem of the free equation (\eqref{eq:singletimedirac} with $V=0$) and $S_k^\ret$ is the retarded Green's function of the $k$-th Dirac operator.

Now, contrary to the PDE \eqref{eq:singletimedirac}, the integral equation \eqref{eq:singletimediracint} possesses a straightforward manifestly covariant generalization in terms of a multi-time wave function, namely:
\begin{align}\notag
  \psi(x_1,x_2) = \psi^\free(x_1,x_2) &+ \int d^4 x_1' \, d^4 x_2' \, S_1(x_1-x_2') S_2(x_2-x_2')\\
  &\times  K(x_1',x_2') \psi(x_1',x_2'),
	\label{eq:inteq}
\end{align}
where $\psi^\free$ is a solution of the equations $D_1 \psi^\free = 0$, $D_2\psi^\free = 0$, $D_k = (i \gamma^\mu_k \partial_{k,\mu} - m_k)$ and $S_1, S_2$ are (retarded or other) Green's functions of $D_1, D_2$, respectively. $K(x_1,x_2)$ denotes the so-called \textit{interaction kernel}, a Poincar\'{e} invariant function (or distribution) which generalizes the potential in Eq.\ \eqref{eq:singletimediracint}. The crucial point is that \eqref{eq:inteq} incorporates interactions with time delay which cannot be expressed through a PDE. It has been demonstrated in \cite{direct_interaction_quantum} that for $K(x_1,x_2) \propto \delta((x_1-x_2)_\mu (x_1-x_2)^\mu)$, the Dirac delta distribution along the light cone, one re-obtains \eqref{eq:singletimedirac} with $V(t,\vx_1,\vx_2) \propto \frac{1}{|\vx_1-\vx_2|}$ if one neglects the time delay of the interaction. Thus, \eqref{eq:inteq} constitutes a natural generalization of \eqref{eq:singletimediracint}.

Further support for considering the integral equation \eqref{eq:inteq} comes from the fact that the Bethe-Salpeter (BS) equation of QFT \cite{bs_equation}, which is usually considered an effective equation for a bound state, has a similar form as \eqref{eq:inteq}. That being said, there are also significant physical and mathematical differences between the two equations (see \cite[sec. 3.3]{direct_interaction_quantum}).

%%%
\subsubsection{Previous results.} To the best of our knowledge, the first results about the existence and uniqueness of dynamics for Eq.\ \eqref{eq:inteq} have been obtained in \cite{mtve}, for the case of a Minkowski half-space and Klein-Gordon (KG) particles. A "Minkowski half-space" means to use $\frac{1}{2} \M \times \frac{1}{2}\M$ with $\frac{1}{2} \M = [0,\infty) \times \R^3$, i.e.\, Minkowski spacetime cut off before $t=0$, as the domain of integration in \eqref{eq:inteq}. The KG case refers to replacing $S_1, S_2$ with (retarded) Green's functions of the KG equation and $\psi^\free$ with a solution of $(\Box_k +m^2_k)\psi^\free = 0,~k=1,2$. The main result in \cite{mtve} was to show that for every $\psi^\free$ which is $L^2$ in the spatial directions and $L^\infty$ in the time directions there is a unique solution $\psi$ with the same properties. In addition, at $t_1=t_2=0$, $\psi^\free$ and $\psi$ agree so that one actually has a Cauchy problem at the initial time. In order to obtain that result, the interaction kernel was assumed to be either bounded or to just have a $1/|\vx_1-\vx_2|$ singularity. In 1+3 dimensions, only the massless case was treated. The proof was based on exploiting a Volterra property which appears for retarded Green's functions and $\frac{1}{2}\M$, i.e.\, the time integrations in \eqref{eq:inteq} reach only from 0 to $x_1^0 $ or $x_2^0$ (given by the time arguments of $\psi$ on the left hand side). This allowed an effective iteration scheme for Eq.\ \eqref{eq:inteq}, leading to a global existence and uniqueness result for a formidable-looking non-Markovian (history dependent) type of dynamics.

The cutoff of spacetime at $t=0$ was introduced in \cite{mtve} 
to obtain the Volterra property. While such a cutoff destroys 
Lorentz invariance, there could be physical justification for a 
beginning in time which is compatible with relativity. Such a 
justification has been provided in \cite{Linertcurved}. There, 
the integral equation was extended to curved spacetimes and 
analyzed in more detail for certain spacetimes which feature a 
Big Bang singularity, Friedman-Lema\^itre-Robertson-Walker (FLRW) 
spacetimes. The Big Bang then provides a natural cutoff in the 
cosmological time. In this way, the existence of certain classes 
of fully covariant dynamics for massless KG particles was 
demonstrated.

%%%
\subsubsection{Goal of the paper.} Here we would like to extend the previous results to the case of Dirac instead of KG particles. This is desirable as the Dirac equation describes actual elementary particles (fermions) while the KG equation is usually considered only a toy equation as its currents do not have the right properties to play the role of a probability current. Mathematically, the Dirac case is more challenging than the KG case as contrary to the latter, the Dirac Green's functions contain distributional derivatives. A Green's function of the Dirac equation is given by acting with the adjoint Dirac operator $\overline{D} = (-i \gamma^\mu \partial_\mu - m)$ on a Green's function $G(x)$ of the KG equation, i.e.\:
\begin{equation}
	S(x)= \overline{D}G(x).
\end{equation}
Consequently, one has to define the integral operator in \eqref{eq:inteq} on a function space where one can take certain weak derivatives. In contrast to most of non-relativistic physics, this also concerns the time derivatives here. The choice of function space can be a tricky issue, as the convergence of an iteration scheme (and of the Neumann series, our strategy of proof) requires the integral operator to preserve the regularity, so that the regularity needs to be in harmony with the structure of the integral equation (see Sec.\ \ref{sec:choiceofB}).


%%%
\subsubsection{Further motivation.}
\begin{enumerate}
	\item It is quite challenging to set up an interacting dynamics for multi-time wave functions. The issue here is not only Lorentz invariance but rather the mere compatibility of the time evolutions in the various time coordinates. A no-go theorem \cite{nogo_potentials,deckert_nickel_2016} for example rules out interaction potentials (which could be Poincar\'{e} invariant functions in the multi-time approach). Thus, interaction is more difficult to achieve for multi-time than for single-time wave functions. So far, the only rigorous, interacting and Lorentz invariant multi-time models for Dirac particles have been constructed in 1+1 spacetime dimensions \cite{1d_model,nt_model} (see, however, \cite{drozvincent_1981,sazdjian_2bd,2bdem} for non-rigorous Lorentz invariant models in 1+3 dimensions and \cite[chap.\ 3]{phd_nickel} for a not fully Lorentz invariant but rigorous model in 1+3 dimensions). Considering these difficulties, the multi-time aspect of our model is interesting in its own right.
%
	\item Eq.\ \eqref{eq:inteq} defines, in the case of retarded Green's functions, a new class of Volterra-type equations which may be interesting also for researchers specializing in integral equations. It provides a reason why a multi-dimensional Volterra-type equation would be relevant for physics, and shows which properties to expect for applications.
\end{enumerate}

%%%
\subsubsection{Overview.} The paper is structured as follows. 
In Sec.\ \ref{sec:setting}, we specify the integral equation 
\eqref{eq:inteq} in detail. The difficulties with understanding 
the distributional derivatives are discussed and a suitable 
function space is identified. Sec.\ \ref{sec:results} contains 
our main results. In Sec.\ \ref{sec:minkhalfspace}, we formulate 
an existence and uniqueness theorem (Thm.\ \ref{thm:minkhalfspace}) 
for Eq.\ \eqref{eq:inteq} on $\frac{1}{2}\M$. It is shown that 
the relevant initial data are equivalent to Cauchy data at $t=0$. 
In Sec.\ \ref{sec:flrw}, we provide a physical justification for 
the cutoff at $t=0$ by extending the results to a FLRW spacetime. 
In the massless case, we show that an existence and uniqueness 
theorem can be obtained from the one for $\frac{1}{2}\M$ via 
conformal invariance. The result, Thm.\ \ref{thm:flrw}, covers a 
fully relativistic interacting dynamics in 1+3 spacetime 
dimensions. The proofs are carried out in Sec.\ \ref{sec:proofs}. 
Sec.\ \ref{sec:discussion} contains a discussion and an outlook on 
future research.

%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
\subsection{Setting of the problem} \label{sec:setting}

%%%
\subsubsection{Definition of the integral operator on test functions} \label{sec:aontestfunctions}

In this section, we show how the integral operator in \eqref{eq:inteq} can be defined rigorously on test functions.
We consider the integral equation \eqref{eq:inteq} on the Minkowski half space $\frac{1}{2}\M := [0,\infty) \times \R^3$ equipped with the metric $g = \text{diag}(1,-1,-1,-1)$. We focus on retarded Green's functions of the Dirac equation, $S^\ret(x) = \overline{D} G^\ret(x)$ where $G^\ret(x)$ is the retarded Green's function of the KG equation. Explicitly,
\begin{equation}
	G^\ret(x) = \frac{1}{4\pi} \frac{\delta(x^0-|\vx|)}{|\vx|} - \frac{m}{4\pi} H(x^0-|\vx|) \frac{J_1(m \sqrt{x^2})}{\sqrt{x^2}}
\end{equation}
where $H$ denotes the Heaviside function, $J_1$ a Bessel function and $x^2 = (x^0)^2 - |\vx|^2$.

In order to define the meaning of the Green's functions as distributions, we introduce a suitable space of test functions:
\begin{equation}
	\mathscr{D} = \mathscr{S}\big( (\tfrac{1}{2}\M)^2,\CC^{16} \big),
\end{equation}
the space of 16-component Schwarz functions on $(\tfrac{1}{2}\M)^2$.
For a smooth interaction kernel $K$ and a test function $\psi \in \mathscr{D}$, we then understand \eqref{eq:inteq} by formally integrating by parts so that all partial derivatives act on $K \psi$:
\begin{align}
  \psi(x_1,x_2) =\, &\psi^\free(x_1,x_2) +\int_{\frac{1}{2}\M} d^4 x_1' \int_{\frac{1}{2}\M} d^4 x_2' \, G_1^\ret(x_1-x_1')\\
  & \times G_2^\ret(x_2-x_2') [D_1 D_2(K \psi)](x_1',x_2')\nonumber\\
&+\text{boundary terms},
\label{eq:inteqpi}
\end{align}
where $D_k = (i \gamma_k^\mu \partial_{x_k^\mu} - m_k)$, $k=1,2$. The boundary terms result from the fact that $\psi(x_1,x_2) \neq 0$ for $x_1^0 = 0$ or $x_2^0 =0$ and are given by:
\begin{align}
  &\int_{\R^3} d^3 \vx_1' \int_{\R^3} d^3 \vx_2'  i \gamma_1^0 G_1^\ret(x_1-x_1') i \gamma_2^0 G_2^\ret(x_2-x_2')\\
  &\hspace{3cm}\times  \left.(K \psi)(x_1',x_2') \right|_{{x_1^0}' = 0,\, {x_2^0}' = 0}\nonumber\\
+ &\int_{\R^3} d^3 \vx_1' \int_{\frac{1}{2} \M} d^4 x_2'  i \gamma_1^0 G_1^\ret(x_1-x_1') G_2^\ret(x_2-x_2')  \\
&\hspace{3cm}\times \left.D_2(K \psi)(x_1',x_2') \right|_{{x_1^0}' = 0}\nonumber\\
 +&\int_{\frac{1}{2} \M} d^4 x_1' \int_{\R^3} d^3 \vx_2'  G_1^\ret(x_1-x_1')  i \gamma_2^0 G_2^\ret(x_2-x_2') \\
&\hspace{3cm}\times \left.D_1 (K \psi)(x_1',x_2') \right|_{{x_2^0}' = 0}.
\label{eq:boundaryterms}
\end{align}
Now, $G_k^\ret$ still contains the $\delta$-distribution. We use the latter to cancel the integrals over ${x_k^0}'$, $k=1,2$ in \eqref{eq:inteqpi} in the following manner.
\begin{align}
	&\frac{1}{4\pi} \int_{\tfrac{1}{2}\M} d^4 x' \, \frac{\delta(x^0-{x^0}'-|\vx-\vx'|)}{|\vx-\vx'|} f(x')\\
= ~&\frac{1}{4\pi} \int_{B_{x^0}(\vx)} d^3 \vx' \, \frac{1}{|\vx-\vx'|} f(x')|_{{x^0}'=x^0-|\vx-\vx'|}\nonumber\\
= ~&\frac{1}{4\pi} \int_{B_{x^0}(0)} d^3 \vy\, \frac{1}{|\vy|} f(x +y)|_{y^0=-|\vy|}.
\end{align}
Moreover,
\begin{align}
	&\frac{m}{4\pi} \int_{\tfrac{1}{2}\M} \!\! d^4 x' ~ H(x^0 - {x^0}' - |\vx-\vx'|) \frac{J_1(m\sqrt{(x-x')^2})}{\sqrt{(x-x')^2}} f(x')\nonumber\\
= ~& \frac{m}{4\pi} \int_{[-x^0,\infty) \times \R^3} \!\!\!\! d^4 y ~ H(-y^0- |\vy|) \frac{J_1(m\sqrt{y^2})}{\sqrt{y^2}} f(x +y)\nonumber\\
= ~& \frac{m}{4\pi} \int_{-x^0}^0 dy^0 \int_{B_{|y^0|}(0)} d^3 \vy_k \, \frac{J_1(m\sqrt{y^2})}{\sqrt{y^2}} f(x+y).
\end{align}
For the boundary terms, we similarly use
\begin{align}
   \frac{i \gamma^0}{4\pi} \int_{\R^3}d^3 \vx'& ~ \frac{\delta(x^0-|\vx-\vx'|)}{|\vx-\vx'|}f(0,\vx') \\
   &~=~ \frac{i \gamma^0}{4\pi} \int_{\partial B_{x^0}(0)}d\sigma(\vy) ~ \frac{f(0,\vx+\vy)}{x^0}
  \end{align}
as well as
\begin{align}
	&i \gamma^0\frac{m}{4\pi} \int_{\R^3}d^3 \vx' ~ H(x^0 - {x^0}' - |\vx-\vx'|) \frac{J_1(m\sqrt{(x-x')^2})}{\sqrt{(x-x')^2}} f(x')|_{{x^0}' = 0}\nonumber\\
&= i \gamma^0\frac{m}{4\pi} \int_{B_{x^0}(0)}d^3 \vy ~ \frac{J_1(m\sqrt{(x^0)^2 - \vy^2})}{\sqrt{(x^0)^2 - \vy^2}} f(0,\vx+\vy).
\end{align}
This yields the form of the integral equation which shall be the basis of our investigation:
\begin{equation}
	\psi(x_1,x_2) = \psi^\free(x_1,x_2) + (A \psi)(x_1,x_2).
\label{eq:inteqschematic}
\end{equation}
The operator $A$ is first defined on test functions $\psi \in \mathscr{D}$ as
\begin{equation}
	A\psi ~=~ \prod_{j=1,2} \left(A_j^{(1)}(m) + A_j^{(2)}(m) + A_j^{(3)}(m) + A_j^{(4)}(m)\right)
 \label{eq:defa}   
\end{equation}
where for $j=1,2$, $k=1,2,3,4$ the operator \\
\(A_j^{(k)}(m) : \mathscr{D} \rightarrow C^\infty\big((\frac{1}{2}\M)^2,\CC^{16}\big)\)  
is defined by letting the respective operator $A^{(k)}(m)$, given 
below, act on the $j$-th 4-variable and spin index of 
$\psi(x_1,x_2),~ \psi \in \mathscr{D}$.
\footnote{We deliberately avoid using tensor products here, as 
the tensor product of Banach spaces is an ambiguous notion.}
\begin{align}
	\left(A^{(1)}(m) \,\psi \right)(x) ~&=~ \frac{1}{4\pi} \int_{B_{x^0}(0)} \! \!d^3 \vy ~ \frac{1}{|\vy|} \psi(x+y)|_{y^0 = -|\vy|}, \label{eq:a1}\\
	\left(A^{(2)}(m) \,\psi\right)(x) ~&=~ -\frac{m}{4\pi} \int_{-x^0}^0 dy^0 \int_{B_{|y^0|}(0)} d^3 \vy ~ \frac{J_1(m\sqrt{y^2})}{\sqrt{y^2}} \psi(x+y),\label{eq:a2}\\
	\left(A^{(3)}(m)\, \psi\right)(x) ~&=~ \frac{i \gamma^0}{4\pi} \int_{\partial B_{x^0}(0)}d\sigma(\vy) ~ \frac{\psi(0,\vx+\vy)}{x^0}, \label{eq:a3}\\\nonumber
  \left(A^{(4)}(m) \, \psi\right)(x) ~&=~ - i \gamma^0 \frac{m}{4\pi} \int_{B_{x^0}(0)}d^3 \vy ~ \frac{J_1(m\sqrt{(x^0)^2 - \vy^2})}{\sqrt{(x^0)^2 - \vy^2}}\\
  &\hspace{2cm} \times\psi(0,\vx+\vy).\label{eq:a4}
\end{align}
Here, the dependence of $A_j^{(1)}$ and $A_j^{(3)}$ on $m$ is only for notational convenience.\\
We now turn to the question of a suitable Banach space for Eq.\ \eqref{eq:inteqschematic}.

\subsubsection{Choice of Banach space} \label{sec:choiceofB}

In order to prove the existence and uniqueness of solutions, we would like to demonstrate the convergence of the Neumann series. First of all, this requires to extend the integral operator $A$ to an operator on a suitable Banach space $\Banach$. The behavior of solutions $\psi^\free(x_1,x_2)$ of the free Dirac equation in each spacetime variable $x_1,x_2$ suggests to choose the Bochner space
\begin{equation}
		\Banach_0 = L^\infty \left([0,\infty)^2_{(x_1^0,x_2^0)}, \,  L^2(\R^6,\CC^{16})_{(\vx_1,\vx_2)}\right)
	\label{eq:banach0}
\end{equation}
with norm
\begin{equation}
	\| \psi \|_{\Banach_0} = \esssup_{x_1^0,x_2^0 > 0} \, \| \psi(x_1^0,\cdot,x_2^0,\cdot)\|_{L^2}.
\end{equation}
The reason for choosing $\Banach_0$ is that the spatial norm $\| \psi^\free(x_1^0,\cdot,x_2^0,\cdot)\|_{L^2}$ of a solution of the free Dirac equations is constant in the two time variables $x_1^0,x_2^0$. A very similar space as $\Banach_0$ has been used for analyzing \eqref{eq:inteq} in the KG case \cite{mtve}.

However, as \eqref{eq:defa} involves the Dirac operators $D_1, D_2$, $\Banach_0$ is not sufficient for our problem. An appropriate Banach space $\Banach$ must allow us to take at least weak derivatives of $\psi$.  The choice of $\Banach$ is a delicate matter. One can easily go wrong with demanding too much regularity, as we shall illustrate now.

%%%
\paragraph{Possible problems with the choice of space.}


The problem can best be illustrated with an example which is structurally related to \eqref{eq:inteq} but otherwise simpler. Consider the equation
\begin{equation}
	f(t,x) = f^\free(t,z) + \int_0^t dz' \, K(z,z') \partial_t f(t,z'),
	\label{eq:modelinteq}
\end{equation}
where $f^\free, f, K : \R^2 \rightarrow \CC$ and $f^\free$ is given. \eqref{eq:modelinteq} is inspired by the term $A_1 D_1$ in \eqref{eq:defa}.

We would like to set up an iteration scheme for \eqref{eq:modelinteq}. As we cannot integrate by parts to shift the $t$-derivative to $K$, we must demand at least weak differentiability of $f$ with respect to $t$. This suggests using a Banach space such as $\Banach = H^1(\R^2)$.
To prove that the integral operator in \eqref{eq:modelinteq} maps $\Banach$ to $\Banach$ (the first step in every iteration scheme), we then have to estimate the $L^2$-norm of
\begin{equation}
	\partial_t \int_0^t dz' \, K(z,z') \partial_t f(t,z') = K(t,t) (\partial_t f)(t,t) + \int_0^t dz' \, K(z,z') \partial_t^2 f(t,z').
\end{equation}
This expression, however, contains $\partial_t^2 f$. For this to make sense, we must be allowed to take the second weak time derivative of $f$. This, in turn, requires to choose a different Sobolev space, such as $H^2(\R^2)$, and to estimate the $L^2$-norm of the second time derivative of the integral operator acting on $f$ which involves $\partial_t^3 f$, and so on. One is thus led to a Sobolev space where all weak $n$-th time derivatives have to exist. Such infinite-order Sobolev spaces have, in fact, been investigated in \cite{dubinskii_1991}. However, it does not seem realistic to get an iteration to converge on these spaces. We therefore take a different approach. 

%%%
\paragraph{A Banach space adapted to our integral equation.} Considering the form of the integral operator $A$ \eqref{eq:defa}, one can see that it is sufficient that the derivatives $D_1 \psi$, $D_2 \psi$ and $D_1 D_2 \psi$ exist in a weak sense. As we want to prove later that $A$ maps the Banach space to itself, we have to estimate, among other things, a suitable norm of $D_1 (A \psi)$. If $\psi \in \mathscr{D}$ is a test function and $K$ is smooth, we have
\begin{align}\nonumber
  D_1 (A \psi)(x_1,x_2) &= D_1 \int d^4 x_1' \, d^4 x_2' \, S_1(x_1-x_2') S_2(x_2-x_2')\\
  &\hspace{1cm}\times K(x_1',x_2') \psi(x_1',x_2')\nonumber\\
	&= \int d^4 x_2' \, S_2(x_2-x_2') K(x_1,x_2') \psi(x_1,x_2')
\label{eq:d1apsi}
\end{align}
where we have used $D_1 S_1(x_1-x_1') = \delta^{(4)}(x_1-x_1')$. The crucial point now is that \eqref{eq:d1apsi} does not contain higher-order derivatives such as $D_1^2 \psi$. The same holds true also for $D_2 (A \psi)$ and $D_1 D_2 (A\psi)$. Thus, the problem of the toy example \eqref{eq:modelinteq} is avoided.

Together with the previous considerations about $\Banach_0$ \eqref{eq:banach0}, we are led to define the Banach space $\Banach_g$ as the completion of $\mathscr{D}$ with respect to the following Sobolev-type norm:
\begin{equation}
	\| \psi \|^2_g = \esssup_{x_1^0,x_2^0 >0} \frac{1}{g(x_1^0)g(x_2^0)} [\psi]^2(x_1^0,x_2^0)
\label{eq:normpsi}
\end{equation}
where \(g : [0,\infty) \rightarrow (0,\infty) \) is a monotonically increasing function which is such that the function $1/g$ is bounded. We admit such a weight factor with hindsight. As we shall see, a suitable choice of $g$ will make a contraction mapping argument possible.

In \eqref{eq:normpsi} we use the notation
\begin{equation}
	[\psi]^2(x_1^0,x_2^0) ~= \sum_{k=0}^3 \| (\mathcal{D}_k \psi)(x_1^0, \cdot, x_2^0,\cdot)\|^2_{L^2(\R^6,\CC^{16})}
\label{eq:spatialnorm}
\end{equation}
with
\begin{equation}
	\mathcal{D}_k = \left\{ \begin{array}{cl} 1, &k=0\\ D_1, & k=1\\ D_2, & k=2 \\ D_1 D_2, & k=3 \end{array}\right. 
\label{eq:defdk}
\end{equation}

%%%
\begin{Remark} 
  One can see the purpose of integral equation \eqref{eq:inteq} in 
  determining an interacting correction to a solution $\psi^\free$ 
  of the free multi-time Dirac equations $D_i \psi^\free = 0,~i=1,2$. 
  Therefore, it is important to check that sufficiently many 
  solutions of these free equations lie in $\Banach_g$. This is 
  ensured by the following Lemma (see Sec. 
  \ref{sec:freesolutionsinbanach} for a proof).

\end{Remark}

\begin{Lemma}
  Let $\psi^\free$ be a solution of the free multi-time Dirac 
  equations $D_i \psi^\free = 0,~i=1,2$ with initial data 
  $\psi^\free(0,\cdot,0,\cdot) = \psi_0 \in C_c^\infty(\R^6,\CC^{16})$. 
  Furthermore, let $g : [0,\infty) \rightarrow (0,\infty)$ be a 
  monotonically increasing function with $g(t) \rightarrow \infty$ 
  for $t \rightarrow \infty$ and \(g(0)=1\). Then $\psi^\free$ lies 
  in $\Banach_g$.
	\label{thm:freesolutionsinbanach}
\end{Lemma}

Given the definition of $A$ on $\mathscr{D}$ as in Sec.\ 
\ref{sec:aontestfunctions}, we shall now proceed with showing that 
$A$ is bounded on this space. Furthermore, we show that for a 
suitable choice of the weight factor $g$ in $\Banach_g$, we can 
achieve $\| A\| < 1$ on $\mathscr{D}$. This allows to extend $A$ to 
a contraction on $\Banach_g$ so that the Neumann series 
$\psi = \sum_{k=0}^\infty A^k \psi^\free$ yields the unique 
solution of $\psi = \psi^\free + A\psi$.

%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
\subsection{Results} \label{sec:results}

%%%%%%%%%
\subsubsection{Results for a Minkowski half space} \label{sec:minkhalfspace}

The core of our results is the following Lemma which allows us to control the growth of the spatial norm of $\psi$ with the two time variables.

\begin{Lemma} \label{thm:boundsa} 
	Let $\psi \in \mathscr{D}$, $\slashed{\partial}_k = \gamma_k^\mu \partial_{k,\mu},~k=1,2$ and let $K \in C^2(\R^{8},\CC)$ with
	\begin{align}
    \| K \| := \sup_{x_1, x_2 \in \frac{1}{2}\M } \max \left\{ |K(x_1,x_2)|, |\slashed{\partial}_1 K(x_1,x_2)|,\right.\\
    \left. |\slashed{\partial}_2 K(x_1,x_2)|, |\slashed{\partial}_1 \slashed{\partial}_2 K(x_1,x_2)|\right\} < \infty.
	\label{eq:normk}
  \end{align}
Then we have:
\begin{align}
		[ A \psi]^2(x_1^0,x_2^0)  ~\leq ~& \|K\|^2 \prod_{j=1,2} \big( \id + 8\mathcal{A}_j(m_j) \big) \, [\psi]^2(x_1^0,x_2^0),
\label{eq:spatialnormapsi}
\end{align}
where $\mathcal{A}_j(m) = \sum_{k=1}^4\mathcal{A}_j^{(k)}(m)$ with 
$\mathcal{A}_j^{(k)}$ as defined in \eqref{eq:defcurlyoperators}. 
The expression $[\psi ]^2(x_1^0,x_2^0)$ is understood as a function in 
$C^\infty\big( (\tfrac{1}{2}\M)^2,\mathbb{R}^+_0\big)$ to which the operators in 
front of it are applied.
\end{Lemma}

The proof can be found in Sec.\ \ref{sec:proofboundsa}.

Lemma \ref{thm:boundsa} can now be used to identify (with some trial and error) a suitable weight factor $g$ which allows us to extend $A$ to a contraction on $\Banach_g$. Our main result is:

%%%
\begin{Thm}[Existence and uniqueness of dynamics on a Minkowski half space.] \label{thm:minkhalfspace}
	Let $0 < \| K \| < 1$, $\mu = \max \{ m_1,m_2\}$ and
	\begin{align} 
		g(t)& ~=~ \sqrt{1+b t^8}\, \exp(b t^8/16),	\label{eq:defg}\\
    		b& ~=~ \frac{\|K\|^4}{(1-\|K\|)^4} \left(6+\mu^4\right)^4. \label{eq:defb}
\end{align}
	Then for every $\psi^\free \in \Banach_g$, the equation $\psi = \psi^\free + A \psi$ possesses a unique solution $\psi \in \Banach_g$.
\end{Thm}

The proof is given in Sec.\ \ref{sec:proofminkhalfspace}.

%%%
\begin{Remark}
\begin{enumerate}
%
	\item Note that Thm. \ref{thm:minkhalfspace} establishes the existence and uniqueness of a global-in-time solution. The non-Markovian nature of the dynamics makes it necessary to prove such a result directly instead of concatenating short-time solutions. The key step in our proof which makes the global-in-time result possible is the suitable choice of the weight factor $g$.
%
	\item The main condition in Thm.\ \ref{thm:minkhalfspace} is $\| K \| < 1$. This means that the interaction must not be too strong (in a suitable sense). A condition of that kind is to be expected solely because of the contribution $\| (D_1 D_2 (A \psi))(x_1^0,\cdot, x_2^0,\cdot)\|_{L^2} = \| K \psi(x_1^0,\cdot,x_2^0,\cdot)\|_{L^2}$ to $[A \psi](x_1^0,x_2^0)$. Taking our
strategy for setting up the Banach space for granted, we therefore think that one cannot avoid a condition on the interaction strength. Note that conditions on the interaction strength also occur at other places in quantum theory (albeit in a different sense). For example, the Dirac Hamiltonian plus a Coulomb potential is only self-adjoint if the prefactor of the latter is smaller than a certain value.
%
	\item \textit{Cauchy problem.} Thm.\ \ref{thm:minkhalfspace} shows that $\psi^\free$ uniquely determines the solution $\psi$. However, specifying a whole function in $\Banach_g$ amounts to a lot of data. In case $\psi^\free$ is a solution of the free multi-time Dirac equations $D_1 \psi^\free = 0 = D_2 \psi^\free$ much less data are needed. $\psi^\free$ is then determined uniquely by Cauchy data, and hence $\psi$ is as well. 
Furthermore, if $\psi^\free$ is differentiable, \eqref{eq:inteq} yields
\begin{equation}
		\psi(0,\vx_1,0,\vx_2) ~=~ \psi^\free(0,\vx_1,0,\vx_2).
	\label{eq:cauchyproblem}
	\end{equation}
	Thus, Cauchy data for $\psi^\free$ at $x_1^0 = x_2^0 = 0$ are also Cauchy data for $\psi$. The procedure works for arbitrary Cauchy data which are appropriate for the free multi-time Dirac equations.  Note, however, that a Cauchy problem for $\psi$ for times $x_1^0 = t_0 = x_2^0$ with $t_0 > 0$ is not possible. The reason is that $\psi(t_0,\vx_1,t_0,\vx_2) \neq \psi^\free(t_0,\vx_1,t_0,\vx_2)$ in general (and contrary to \eqref{eq:cauchyproblem} the point-wise evaluation may not make sense for $\psi$).
\end{enumerate}

\end{Remark}

%%%%%%%%%
\subsubsection{Results for a FLRW universe with a Big Bang singularity} \label{sec:flrw}

In this section we show that a Big Bang singularity provides a 
natural and covariant justification for the cutoff at $t = 0$. As 
this justification is our main goal, we make the point at the 
example of a particular class of Friedman-Lema\^itre-Robertson-Walker 
(FLRW) spacetimes and do not strive to treat more general spacetimes 
here. The reason for studying these FLRW spacetimes is that they are 
conformally equivalent to $\frac{1}{2} \M$ \cite{ibison}. Together 
with the conformal invariance of the massless Dirac operator this 
allows for an efficient method of calculating the Green's functions 
which occur in the curved spacetime analog of the integral equation 
\eqref{eq:inteq}. By doing this, we show that the existence and 
uniqueness result on these spacetimes can be reduced to Thm.\ 
\ref{thm:minkhalfspace}.

As shown in \cite{lienertcurved}, Eq.\ \eqref{eq:inteq} possesses 
a natural generalization to curved spacetimes $\mathcal{M}$,
\begin{align}\nonumber
  \psi(x_1,x_2) = \psi^\free(x_1,x_2) &+ \int dV(x_1') \int dV(x_2') \, G_1(x_1,x_1') G_2(x_2,x_2')\\
  &\times K(x_1',x_2') \psi(x_1',x_2').
	\label{eq:inteqcurved}
\end{align}
Here, $dV(x)$ is the spacetime volume element, $S_i$ are (retarded) 
Green's functions of the respective free wave equation, i.e.\
\begin{equation}
	D G(x,x') = [-g(x)]^{-1/2} \, \delta^{(4)}(x,x'),
	\label{eq:greensfndefcurved}
\end{equation}
where $g(x)$ is the metric determinant, $D$ the covariant Dirac 
operator on $\mathcal{M}$, and $\psi$ a section of the tensor 
spinor bundle over $\mathcal{M} \times \mathcal{M}$.

In order to explicitly formulate \eqref{eq:inteqcurved}, we need 
to know the detailed form of $S^\ret$. Note that results for 
general classes of spacetimes showing that $S^\ret$ is a bounded 
operator on a suitable function space are not sufficient to obtain 
a strong (global in time) existence and uniqueness result. We 
therefore focus on the case of a flat FLRW universe where it is 
easy to determine the Green's functions explicitly. In that case, 
the metric is given by
\begin{equation}
	ds^2 = a^2(\eta) [d \eta^2 - d \vx^2]
	\label{eq:metricflrw}
\end{equation}
where $\eta$ is cosmological time and $a(\eta)$ denotes the 
so-called \textit{scale factor}. The coordinate ranges are given 
by $\eta \in [0,\infty)$ and $\vx \in \R^3$. For a FLRW universe 
with a Big Bang singularity, $a(\eta)$ is a continuous, 
monotonically increasing function of $\eta$ with $a(\eta)=0$, 
corresponding to the Big Bang singularity. The spacetime volume 
element reads
\begin{equation}
	dV(x) = a^4(\eta) \, d \eta\, d^3 \vx.
\end{equation}
The crucial point now is that according to \eqref{eq:metricflrw} 
the spacetime is globally conformally equivalent to $\frac{1}{2}\M$, 
with conformal factor
\begin{equation}
	\Omega(x) = a(\eta).
\end{equation}
In addition, for $m=0$, the Dirac equation is known to be 
conformally invariant (see e.g.\ \cite{penrose_rindler}). More 
accurately, consider two spacetimes $\mathcal{M}$ and 
$\widetilde{\mathcal{M}}$ with metrics
\begin{equation}
	\widetilde{g}_{ab} = \Omega^2 \, g_{ab}.
\end{equation}
Then the massless Dirac operator $D$ on $\mathcal{M}$ is related to 
the massless Dirac operator $\widetilde{D}$ on 
$\widetilde{\mathcal{M}}$ by (see \cite{haantjes}):
\begin{equation}
	\widetilde{D} = \Omega^{-5/2} \, D \, \Omega^{3/2}.
	\label{eq:diracoperatortrafo}
\end{equation}
This implies the following transformation behavior of the Green's 
functions:
\begin{equation}
	\widetilde{G}(x,x') = \Omega^{-3/2}(x) \,\Omega^{-3/2}(x') \, G(x,x').
\end{equation}
One can verify this easily using \eqref{eq:diracoperatortrafo} and 
the definition of Green's functions on curved spacetimes 
\eqref{eq:greensfndefcurved}.

Denoting the Green's functions of the Dirac operator on Minkowski 
spacetime by $G(x,x')=S(x-x')$ and using coordinates $\eta, \vx$ 
we thus obtain the Green's functions $\widetilde{G}$ on flat FLRW 
spacetimes as:
\begin{equation}
	\widetilde{G}(\eta,\vx; \eta', \vx') = a^{-3/2}(\eta) a^{-3/2}(\eta')\, S(\eta-\eta', \vx-\vx').
\end{equation}
With this result, we can write out in detail the multi-time integral 
equation \eqref{eq:inteqcurved} for massless Dirac particles on flat 
FLRW spacetimes (using retarded Green's functions):
\begin{align}\nonumber
  \psi(\eta_1,\vx_1,\eta_2,\vx_2) =& \psi^\free(\eta_1,\vx_1,\eta_2,\vx_2) + a^{-3/2}(\eta_1) a^{-3/2}(\eta_2) \\
  &\times \int_0^\infty d \eta_1' \int d^3 \vx_1' \int_0^\infty d \eta_2' \int d^3 \vx_2' \nonumber\\
  &\times a^{5/2}(\eta_1') a^{5/2}(\eta_2') \, S_1^\ret(\eta_1-\eta_1', \vx_1-\vx_1')\\
  &\times S_2^\ret(\eta_2-\eta_2',\vx_2-\vx_2') \, (K \psi)(\eta_1',\vx_1',\eta_2',\vx_2').
\end{align}
Note that we can regard $\psi$ as a map 
$\psi : (\frac{1}{2}\M)^2 \rightarrow \CC^{16}$ as the coordinates 
$\eta, \vx$ cover the flat FLRW spacetime manifold globally.

It seems reasonable to allow for a singularity of the interaction 
kernel, i.e.\
\begin{equation}
	K(\eta_1,\vx_1,\eta_2,\vx_2) = a^{-\alpha}(\eta_1)  a^{-\alpha}(\eta_1) \, \widetilde{K}(\eta_1,\vx_1,\eta_2,\vx_2).
	\label{eq:ksingularity}
\end{equation}
Here, $\alpha \geq 0$. The singular behavior is motivated by that of 
the Green's functions of the conformal wave equation
\footnote{The conformal wave equation reads $(\Box- R/6)\phi = 0$ 
where $\Box$ is the d'Alembertian and $R$ the Ricci scalar of the 
respective spacetime.}. Recall from the introduction that the most 
natural interaction kernel on $\frac{1}{2}\M$ would be 
$K(x_1,x_2)\propto \delta((x_1-x_2)_\mu(x_1-x_2)^\mu)$ which is a 
Green's function of the wave equation -- a concept that can be 
generalized to curved spacetimes using the conformal wave equation. 
Now, under conformal transformations, Green's functions of that 
equation transform as \cite{john}
\begin{equation}
	\widetilde{G}(x,x') = \Omega^{-1}(x) \,\Omega^{-1}(x') \, G(x,x'),
\end{equation}
which corresponds to $\alpha = 1$ in \eqref{eq:ksingularity}.

Considering \eqref{eq:ksingularity}, our integral equation becomes:
\begin{align}\nonumber
  \psi(\eta_1,\vx_1,\eta_2,\vx_2) &= \psi^\free(\eta_1,\vx_1,\eta_2,\vx_2) + a^{-3/2}(\eta_1) a^{-3/2}(\eta_2) \int_0^\infty d \eta_1' \\
  &\times \int d^3 \vx_1' \int_0^\infty d \eta_2' \int d^3 \vx_2'\nonumber\\\nonumber
  &\times a^{5/2-\alpha}(\eta_1') a^{5/2-\alpha}(\eta_2') \, S_1^\ret(\eta_1-\eta_1', \vx_1-\vx_1')\\
  &\times  S_2^\ret(\eta_2-\eta_2',\vx_2-\vx_2') \, (\widetilde{K} \psi)(\eta_1',\vx_1',\eta_2',\vx_2').
\label{eq:inteqcurvedexplicit}
\end{align}
Apart from the scale factors which produce a certain singularity of 
$\psi$ for $\eta_1,\eta_2 \rightarrow 0$, this integral equation has 
the form of \eqref{eq:inteq} on $\frac{1}{2}\M$. Indeed, we can use 
the transformation
\begin{equation}
	\chi(\eta_1,\vx_1,\eta_2,\vx_2) = a^{3/2}(\eta_1) a^{3/2}(\eta_2)\, \psi(\eta_1,\vx_1,\eta_2,\vx_2)
	\label{eq:psichi}
\end{equation}
to transform the two equations into each other. We arrive at the 
following result.

%%%
\begin{Thm}[Existence and uniqueness of dynamics on a flat 
  FLRW universe] \label{thm:flrw}
	Let, $0 \leq \alpha \leq 1$ and let $a : [0,\infty) \rightarrow [0,\infty)$ be a differentiable function with $a(0)=0$ and $a(\eta) >0$ for $\eta>0$. Moreover, assume that $\widetilde{K} \in C^2 \left( ([0,\infty)\times \R^3)^2,\CC\right)$ with
  \begin{equation}
		\| a^{1-\alpha}(\eta_1) a^{1-\alpha}(\eta_2) \, \widetilde{K} \| <1.
	\label{eq:ktildecondition}
	\end{equation}
 Then for every $\psi^\free$ with 
 $a^{3/2}(\eta_1) a^{3/2}(\eta_2) \psi^\free \in \Banach_g$, 
 \eqref{eq:inteqcurvedexplicit} has a unique solution $\psi$ with 
 $a^{3/2}(\eta_1) a^{3/2}(\eta_2)\psi \in \Banach_g$ (and with \(g\) 
 as in Thm. \ref{thm:minkhalfspace}).
\end{Thm}

\begin{proof}
	Multiplying \eqref{eq:inteqcurvedexplicit} with $a^{3/2}(\eta_1) a^{3/2}(\eta_2)$ and using the relation yields
	\begin{align}\nonumber
  \chi(\eta_1,\vx_1,\eta_2,\vx_2) =& \chi^\free(\eta_1,\vx_1,\eta_2,\vx_2) +\int_0^\infty d \eta_1' \int d^3 \vx_1 \int_0^\infty d \eta_2'\\
  &\times~a^{1-\alpha}(\eta_1') a^{1-\alpha}(\eta_2') \nonumber\\\nonumber
  &\times  \, S_1^\ret(\eta_1-\eta_1', \vx_1-\vx_1') S_2^\ret(\eta_2-\eta_2',\vx_2-\vx_2') \\
  &\times (\widetilde{K} \chi)(\eta_1',\vx_1',\eta_2',\vx_2').
\label{eq:inteqcurvedexplicit2}
\end{align}
This equation has the form of \eqref{eq:inteq} on $\frac{1}{2}\M$ with $K$ replaced by $a^{1-\alpha}(\eta_1') a^{1-\alpha}(\eta_2') \widetilde{K}$. Thus, using the same distributional understanding of the Green's functions as before, Thm.\ \ref{thm:minkhalfspace} yields the claim. \qed 
\end{proof}

%%%
\begin{Remark}
\begin{enumerate}
	\item Both $\psi^\free$ and $\psi$ have a singularity proportional to $a^{-3/2}(\eta_1)a^{-3/2}(\eta_2)$ for $\eta_1, \eta_2 \rightarrow 0$.
%
	\item For $\alpha < 1$, $\widetilde{K}$ has to compensate the singularities caused by $a^{-3/2}(\eta_1) a^{-3/2}(\eta_2)$ in order for \eqref{eq:ktildecondition} to hold. In the most natural case $\alpha = 1$, however, $\widetilde{K}$ only needs to satisfy $\| \widetilde{K} \| < 1$, i.e.\, the same condition as for $K$ in Thm.\ \ref{thm:minkhalfspace}. 
%
	\item Let $\chi^\free = a^{3/2}(\eta_1) a^{3/2}(\eta_2) \psi^\free$ be differentiable and let $\chi$ be the unique solution of \eqref{eq:inteqcurvedexplicit2}. Then, by \eqref{eq:inteqcurvedexplicit2}, we have:
  \begin{equation}
		\chi^\free(0,\vx_1,0,\vx_2) = \chi(0,\vx_1,0,\vx_2),
	\end{equation}
	i.e.\, $\chi$ satisfies a Cauchy problem "at the Big Bang".
%
  \item Remarkably, Thm.\ \ref{thm:flrw} covers a class of manifestly 
  covariant, interacting integral equations in 1+3 dimensions. Then 
  the interaction kernel $\widetilde{K}$ has to be covariant as 
  well. A class of examples (see also \cite{lienertcurved}) is 
  given by $\alpha = 1$ and 
  \begin{equation}
	\widetilde{K}(x_1,x_2) = \left\{ \begin{array}{cl} f(d(x_1,x_2))& \text{if } x_1, x_2 \text{ are time-like related}\\ 0 &  \text{else}, \end{array} \right.
\end{equation}
where $d(x_1,x_2) = (|\eta_1-\eta_2| -|\vx_1-\vx_2|) \int_0^1 d 
\tau \, a(\tau \eta_1 + (1-\tau) \eta_2)$ denotes the time-like 
distance of the spacetime points $x_1 = (\eta_1,\vx_1)$ and 
$x_2 = (\eta_2,\vx_2)$, and $f$ is an arbitrary smooth function 
which leads to $\| \widetilde{K} \| < 1$.

\end{enumerate}
\end{Remark}


%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
\subsection{Proofs} \label{sec:proofs}

%%%%%
\subsubsection{Proof of lemma \ref{thm:freesolutionsinbanach}}

\label{sec:freesolutionsinbanach}

Consider a solution $\psi$ of $D_i \psi^\free = 0,~i=1,2$ for 
compactly supported initial data at $x_1^0 = 0 = x_2^0$. As the 
Dirac equation has finite propagation speed, $\psi^\free$  is 
spatially compactly supported for all times. Without loss of 
generality we may assume 
\(\|\psi^\free (t_1,\cdot,t_2,\cdot)\|_{L^2(\mathbb{R}^6)}=1\) for 
all times \(t_1,t_2\), so it follows that also 
\([\psi^\free](t_1,t_2)=1\). In the following we will construct a 
sequence of test functions \((\psi_m)_{m\in\mathbb{N}}\) 
satisfying \(\psi_m \xrightarrow{\|\cdot\|_g ,~m\rightarrow \infty} 
\psi^\free\). Let \(\eta : \mathbb{R}\rightarrow \mathbb{R}\) be 
zero for arguments less than \(0\) and greater than \(1\) and in 
between given by (see also Fig. \ref{fig:eta})
\begin{equation}
	\eta(t)=  \exp\left(-\frac{1}{t} \exp\left({\frac{1}{t-1}}\right)\right).
\end{equation}
%
\begin{figure}
	\centering
	\begin{tikzpicture}
  \begin{axis}[
      xlabel={$t$}, 
      ylabel={$\eta$},
      samples=500,
      xmin=-0,
      xmax=1,
      ymin=-0.1,
      ymax=1.1,
      enlarge y limits=false,
      axis x line=middle,
      axis y line=middle,
      axis line style={-},
      width=10. cm,
      height=6. cm,
      yticklabel style={xshift=7.5mm},
      ylabel style={xshift=-3mm,yshift=5mm},
      xlabel style={xshift=5mm,yshift=-2.4mm},
      ticklabel style={font=\small}
    ]
    \addplot[black,thick,domain=-0.1:0.001](x,0);
    \addplot [black,thick,domain=0.001:0.999] (x, {pow(e,-1/x * pow(e,1/(x-1)))});
    \addplot[black,thick,domain=0.999:1.1] (x,1);
    \end{axis}
\end{tikzpicture}
	\caption{The function \( \eta(t)\).}
	\label{fig:eta}
\end{figure}
%
 Note that $\eta$ is smooth and monotonically increasing. Next, we 
 define for every \(m\in\mathbb{N}\)
 \begin{equation}
 \psi^\free_m(t_1,\vx_1, t_1, \vx_2) ~:=~ e^{-\eta(t_1-m) (t_1-m)} e^{-\eta(t_2-1)(t_2-m)} \psi^\free(t_1,\vx_1, t_2, \vx_2).
 \end{equation}
 This function is smooth and decreases rapidly in all variables and 
 thus lies in \(\mathscr{D}\). Now we estimate
  \(\|\psi^\free-\psi_m\|_g\). Pick \(m\in\mathbb{N}\). First 
  consider \(\|\psi^\free -\psi_m\|_{L^2(\mathbb{R}^6)}(t_1,t_2)\). 
  This function is identically zero for all \(t_1<n\) and 
  \(t_2<n\), so we obtain the estimate
 \begin{align}
    & \sup_{t_1,t_2>0} \frac{1}{g(t_1)^2g(t_2)^2} \|\psi^\free- \psi_n\|^2_{L^2(\mathbb{R}^6)}\nonumber\\
     =&\sup_{t_1,t_2>0} \frac{1}{g(t_1)^2g(t_2)^2} \left|1-e^{-\eta(t_1-n)(t_1-n)}e^{-\eta(t_2-n)(t_2-n)}\right|\\
     & ~\le~\frac{1}{g(n)^2}.
\end{align}
For the other terms we use that \(\psi^\free\) solves the free Dirac 
equation in each variable and that 
\(\sup_{t>0}\partial_{t} e^{-\eta(t)t}=:\alpha<\infty\) is realized 
for some positive value of \(t\). So we find for \(i\in \{0,1\}\):
 \begin{align}
& \sup_{t_1,t_2>0}\frac{1}{g(t_1)^2g(t_2)^2}\|D_i (\psi^\free-\psi_n)\|^2_{L^2(\mathbb{R}^6)}(t_1,t_2)\nonumber\\
= &\sup_{t_1,t_2>0}\frac{1}{g(t_1)^2g(t_2)^2}
 \\
 &\times\|\gamma_{i}^0 \psi^\free(t_1,\cdot,t_2,\cdot) e^{-\eta(t_{3-i}-n)(t_{3-i}-n)}\partial_{t_i}e^{-\eta(t_i-n)(t_i-n)} \|^2_{L^2(\mathbb{R}^6)}\\
 &\le~ \frac{\alpha}{g(n)^2}.
 \end{align}
For the first inequality it has been used that the factor with a 
derivative vanishes for \(t_i<n\). 

An analogous estimate repeated for the \(D_1D_2\)-term yields
\begin{equation*}
     \sup_{t_1,t_2>0} \frac{1}{g(t_1)^2g(t_2)^2}\|D_1D_2(\psi^\free - \psi_n)\|^2_{L^2(\mathbb{R}^6)}(t_1,t_2)~\le~ \frac{\alpha^2}{g(n)^4} ~\le~ \frac{\alpha^2}{g(n)^2}.
 \end{equation*}
All in all, adding the estimates and taking the square root we 
find \(\|\psi^\free - \psi_n\|_g \le \frac{1+\alpha}{g(n)}\), which 
together with the asymptotic behavior of \(g\) implies convergence. 
It follows that the free solution \(\psi^\free\) can be approximated 
by Cauchy sequences in \(\mathscr{D}\) and hence is contained in 
\(\mathscr{B}_g\) which, we recall, has been defined as the 
completion of $\mathscr{D}$ with respect to $\| \cdot \|_g$. \qed


%%%%%
\subsubsection{Proof of Lemma \ref{thm:boundsa}} \label{sec:proofboundsa}

Throughout the following subsections, let $\psi \in \mathscr{D}$ and 
$K : \R^8 \rightarrow \CC$ be a smooth function. Furthermore define 
\(\delta:= 1-\|K\|^2>0, \mu=\text{max}\{m_1,m_2\}\) and let \(g\) 
be as in the statement of Thm. \ref{thm:minkhalfspace}.

We begin with some lemmas which are useful for estimating \\
$[A\psi]^2(x_1^0,x_2^0)$.

\begin{Lemma} \label{thm:estimatetensoroperators}
	Let the following operators be defined on $C([0,\infty))$:
	\begin{align}
		\big(\mathcal{A}^{(1)}(m) f\big)(t) ~&=~ t \int_0^{t} d \rho ~ (t-\rho)^2\,  f(\rho),\nonumber\\
		\big(\mathcal{A}^{(2)}(m) f \big)(t) ~&=~ \frac{m^4 t^4}{2^4 \,3^2} \int_0^{t} d\rho ~(t-\rho)^3 \, f(\rho),\nonumber\\
		\big(\mathcal{A}^{(3)}(m) f\big)(t) ~&=~ t^2 \, f(0),\nonumber\\
		\big(\mathcal{A}^{(4)}(m) f \big)(t) ~&=~ \frac{m^4 t^6}{2^2\, 3^2} \, f(0).
	\label{eq:defcurlyoperators}
	\end{align}
  Then, for $j=1,2$ and $k=1,2,3,4$, we define the operator 
  $\mathcal{A}_j^{(k)}(m)$ acting on functions 
  $\phi \in C([0,\infty)^2)$ by letting $\mathcal{A}^{(k)}(m)$ act 
  on the $j$-th variable of $\phi(t_1,t_2)$.
Then we have for all $\psi \in \mathscr{D}$, all $m_1,m_2\geq 0$ and 
all $k,l=1,2,3,4$:
\begin{equation}
		\left\| A_1^{(k)}(m_1) A_2^{(l)}(m_2)  \psi(t_1,\cdot,t_2,\cdot) \right\|^2_{L^2} \!\!\leq\! \mathcal{A}_j^{(k)}(m_1) \mathcal{A}_j^{(l)}(m_2)\, \|\psi(t_1,\cdot,t_2,\cdot)\|^2_{L^2}.\label{eq:aestimate}
	\end{equation}
Here, it is understood that the operators $\mathcal{A}_j^{(k)}$ 
are applied to the functions defined by the norms which follow them, 
e.g.\, \\
$\mathcal{A}_1^{(4)}(m_1)\, \|\psi(t_1,\cdot,t_2,\cdot)\|^2_{L^2} = \frac{m^4_1 t_1^6}{2^2\, 3^2} \, \|\psi(0,\cdot,t_2,\cdot)\|^2_{L^2}$.
\end{Lemma}

\begin{proof}
  We prove \eqref{eq:aestimate} for $k=1$, $ l=2$ and $k=3$, $l=4$. 
  The remaining cases can be treated in the same way. We begin with 
  $k=1$, $l=2$, using $|J_1(x)/x| \leq \frac{1}{2}$:
\begin{align*}
	&\| A_1^{(1)}(m_1) A_2^{(2)}(m_2)\, \psi(x_1^0,\cdot,x_2^0,\cdot) \|^2_{L^2} ~= \frac{m_2^2}{(4\pi)^4} \int_{\R^3 \times \R^3} \!\!\! d^3 \vx_1 \, d^3 \vx_2 \nonumber\\
&\hspace{-1cm}\times \left| \int_{B_{x_1^0}(0)} \!\!\! d^3\vy_1 \int_{-x_2^0}^0 dy_2^0 \int_{B_{|y_2^0|}(0)} \!\!\! d^3 \vy_2 ~\frac{1}{|\vy_1|}  \frac{J_1(m_2\sqrt{y_2^2})}{\sqrt{y_2^2}}\psi(x_1+y_1,x_2+y_2)|_{y^0_1=-|\vy_1|} \right|^2\nonumber\\
&\leq \frac{m_2^2}{(4\pi)^4} \int_{\R^3 \times \R^3}  \!\!\! d^3 \vx_1 \, d^3 \vx_2 \\
&\times \left( \int_{B_{x_1^0}(0)} \!\!\! d^3\vy_1 \int_{-x_2^0}^0 dy_2^0 \int_{B_{|y_2^0|}(0)} \!\!\! d^3 \vy_2~ \frac{1}{|\vy_1|^2} \left| \frac{J_1(m_2\sqrt{y_2^2})}{\sqrt{y_2^2}} \right|^2 \right)\nonumber\\
&~\times \left( \int_{B_{x_1^0}(0)} \!\!\! d^3\vy_1 \int_{-x_2^0}^0 dy_2^0 \int_{B_{|y_2^0|}(0)} \!\!\! d^3 \vy_2 ~|\psi|^2(x_1+y_1,x_2+y_2)|_{y^0_1=-|\vy_1|} \right)\nonumber\\
&\leq~  \frac{m_2^2}{(4\pi)^4} \int_{\R^3 \times \R^3}  \!\!\! d^3 \vx_1 \, d^3 \vx_2 ~4\pi x_1^0 \left( \frac{\pi m_2^2 (x_2^0)^4}{12}\right) \nonumber\\
&~\times \left( \int_{B_{x_1^0}(0)} \!\!\! d^3\vy_1 \int_{-x_2^0}^0 dy_2^0 \int_{B_{|y_2^0|}(0)} \!\!\! d^3 \vy_2 ~|\psi|^2(x_1+y_1,x_2+y_2)|_{y^0_1=-|\vy_1|} \right)\nonumber\\
&\leq~  \frac{m_2^4 \, x_1^0 \,  (x_2^0)^4}{3 \pi^2\, 2^8 } \int_{\R^3 \times \R^3}  \!\!\! d^3 \vx_1 \, d^3 \vx_2  \int_{B_{x_1^0}(0)} \!\!\! d^3\vy_1 \int_{-x_2^0}^0 dy_2^0 \int_{B_{|y_2^0|}(0)} \!\!\! d^3 \vy_2\nonumber\\
&~\times |\psi|^2(x_1^0 - |\vy_1|,\vx_1+\vy_1,x_2^0 + y_2^0,\vx_2+\vy_2).
\label{eq:a1a2estimate1}
\end{align*}
Exchanging the $x$ and $y$ integrals yields:
\begin{align}\nonumber
&\eqref{eq:a1a2estimate1} \leq  \frac{m_2^4 \, x_1^0\,  (x_2^0)^4}{3 \pi^2\, 2^8 } \int_{B_{x_1^0}(0)} \!\!\! d^3\vy_1 \int_{-x_2^0}^0 dy_2^0 \int_{B_{|y_2^0|}(0)} \!\!\! d^3 \vy_2 \\
&\hspace{3cm}\times\| \psi(x_1^0 - |\vy_1|,\cdot,x_2^0 + y_2^0,\cdot)\|_{L^2} \nonumber\\\nonumber
&\leq~  \frac{m_2^4 \, x_1^0\,  (x_2^0)^4}{3 \pi^2\, 2^8 } \, 4\pi \int_{0}^{x_1^0} \!\!\! d r_1 ~r_1^2 \int_{-x_2^0}^0 dy_2^0 ~\frac{4\pi}{3} |y_2^0|^3 \\\nonumber
&\hspace{3cm}\times \| \psi(x_1^0 - |\vy_1|,\cdot,x_2^0 + y_2^0,\cdot)\|_{L^2} \nonumber\\
&\leq~  \frac{m_2^4 \, x_1^0 \,  (x_2^0)^4}{2^4 \, 3^2} \int_{0}^{x_1^0} \!\!\! d \rho_1 ~(x_1^0-\rho_1)^2 \int_{0}^{x_2^0} d\rho_2 ~(x_2^0-\rho_2)^3 \, \| \psi(\rho_1,\cdot,\rho_2,\cdot)\|_{L^2} \nonumber\\
&=~ \mathcal{A}_1^{(1)}(m_1) \mathcal{A}_2^{(2)}(m_2)\, \|\psi(x_1^0,\cdot,x_2^0,\cdot)\|^2_{L^2}.
\end{align}

Next, we turn to the case $k=3, l=4$. Using that the modulus of the 
largest eigenvalue of $\gamma^0$ is 1, we obtain:
\begin{align}
	&\| A_1^{(3)}(m_1) A_2^{(4)}(m_2)\, \psi(x^0_1,\cdot,x_2^0,\cdot) \|^2_{L^2} ~\leq~ \frac{m_2^2}{(4\pi)^4 (x_1^0)^2} \int_{\R^3\times \R^3} \!\!\! d^3 \vx_1 \, d^3 \vx_1\nonumber\\
 &\hspace{-0.5cm}\times \left|\int_{\partial B_{x_1^0}(0)} \!\!\! d \sigma(\vy_1) \int_{B_{x_2^0}(0)} \!\!\! d^3 \vy_2 ~\frac{J_1\left( m_2\sqrt{(x_2^0)^2-\vy_2^2}\right)}{\sqrt{(x_2^0)^2-\vy_2^2}} |\psi|(0,\vx_1+\vy_2,0,\vx_2+\vy_2) \right|^2\nonumber\\
&\leq \frac{m_2^4}{(4\pi)^4 (x_1^0)^2} \int_{\R^3\times \R^3} \!\!\! d^3 \vx_1 \, d^3 \vx_1\nonumber\\
& \times\left(\int_{\partial B_{x_1^0}(0)} \!\!\! d \sigma(\vy_1) \int_{B_{x_2^0}(0)} \!\!\! d^3 \vy_2 \left|\frac{J_1\left( m_2\sqrt{(x_2^0)^2-\vy_2^2}\right)}{m_2\sqrt{(x_2^0)^2-\vy_2^2}}\right|^2 \right)\nonumber\\
& \times \left( \int_{\partial B_{x_1^0}(0)} \!\!\! d \sigma(\vy_1) \int_{\partial B_{x_2^0}(0)} \!\!\! d \sigma(\vy_2) ~|\psi|^2(0,\vx_1+\vy_2,0,\vx_2+\vy_2)\right)\nonumber\\
&=~ \frac{m_2^4}{(4\pi)^4 (x_1^0)^2} \, 4\pi (x_1^0)^2 \, \frac{\pi (x_2^0)^3}{3} \int_{\R^3\times \R^3} \!\!\! d^3 \vx_1 \, d^3 \vx_1 \int_{\partial B_{x_1^0}(0)} \!\!\! d \sigma(\vy_1)  \nonumber\\
&~~~~~\times\int_{B_{x_2^0}(0)} \!\!\! d^3 \vy_2 ~|\psi|^2(0,\vx_1+\vy_2,0,\vx_2+\vy_2).
\label{eq:a3a4estimate1}
\end{align}
Exchanging the order of the $x$ and $y$ integrals yields:
\begin{align}
	\eqref{eq:a3a4estimate1} ~&=~ \frac{m_2^4}{(4\pi)^4} \pi (x_2^0)^2\int_{\partial B_{x_1^0}(0)} \!\!\! d \sigma(\vy_1) \int_{B_{x_2^0}(0)} \!\!\! d^3 \vy_2 ~\|\psi(0,\cdot,0,\cdot)\|^2_{L^2}\nonumber\\
&=~ \frac{m_2^4 \, (x_1^0)^2 \, (x_2^0)^6}{2^2 \, 3^2} \, \|\psi(0,\cdot,0,\cdot)\|^2_{L^2}\nonumber\\
&= ~ \mathcal{A}_1^{(3)}(m_1) \mathcal{A}_2^{(4)}(m_2) \,  \|\psi(x_1^0,\cdot,x_2^0,\cdot)\|^2_{L^2}.
\end{align} 
\end{proof}


%%%
\begin{Lemma} \label{thm:spatialnormestimates}
	For $j=1,2$ let $\mathcal{A}_j(m) = \sum_{k=1}^4 \mathcal{A}_j^{(k)}(m)$. Then the following estimates hold:
\begin{align}
		& \!\| (A \psi)(x_1^0,\cdot,x_2^0,\cdot)\|^2_{L^2} ~\leq ~64 \, \|K\|^2 \mathcal{A}_1(m_1) \mathcal{A}_2(m_2) \, [\psi]^2(x_1^0,x_2^0),
	\label{eq:apsiestimate}\\
		&\! \| (D_1(A \psi))(x_1^0,\cdot,x_2^0,\cdot) \|^2_{L^2}~\leq~8 \, \| K \|^2 \, \mathcal{A}_2(m_2) \, [\psi](x_1^0,x_2^0),
	\label{eq:d1apsiestimate}\\
		&\! \| (D_2(A \psi))(x_1^0,\cdot,x_2^0,\cdot) \|^2_{L^2} ~\leq~8 \, \| K \|^2 \, \mathcal{A}_1(m_1) \, [\psi](x_1^0,x_2^0),
	\label{eq:d2apsiestimate}\\
		&\! \| (D_1 D_2(A \psi))(x_1^0,\cdot,x_2^0,\cdot) \|^2_{L^2} ~\leq~ \| K \|^2 \, [\psi]^2(x_1^0,x_2^0),
	\label{eq:d1d2apsiestimate}
	\end{align}
	where $[\psi]^2(x_1^0,x_2^0)$ is regarded as a function of $x_1^0,x_2^0$ to which the operators in front of it are applied.
\end{Lemma}

\begin{proof}
	We start with \eqref{eq:apsiestimate}. Recalling \eqref{eq:defa}, the expression $A\psi$ contains terms such as $D_1 D_2 (K \psi)$ and $D_i(K\psi)$, $i=1,2$. Recalling also the definition of $\mathcal{D}_k$ (Eq.\ \eqref{eq:defdk}), we have:
\begin{equation}
	D_1 D_2 (K\psi) = \sum_{k=0}^3 (\nabla_{3-k}K)(\mathcal{D}_k \psi)
\end{equation}
with
\begin{equation}
\nabla_k := \left\{\begin{array}{cl} 1, & k=0\\ i\slashed{\partial}_1, & k=1 \\ i \slashed{\partial}_2, & k=2 \\ -\slashed{\partial}_1\slashed{\partial}_2, & k=3. \end{array}\right.
\end{equation}
Hence, noting \eqref{eq:normk}:
\begin{equation}
	|D_1D_2 \psi| \leq \| K \| \sum_{k=0}^3 |\mathcal{D}_k \psi|.
\end{equation}
Similarly, we find:
\begin{equation}
	D_i (K\psi) \leq \| K\| \sum_{k=0}^3 |\mathcal{D}_k \psi|,~~i=1,2.
\end{equation}
Considering the definition of $A_j^{(k)}(m),$ $j=1,2$, $k=1,2,3,4$ 
it follows that
\begin{align}\nonumber
  |A\psi| ~\leq ~\|K\| \sum_{k=0}^3   \prod_{j=1,2} \big[ &A_j(m_j)^{(1)} + A_j^{(2)}(m_j) + A_j^{(3)}(m_j)\\
  & + A_j^{(4)}(m_j)\big]  |\mathcal{D}_k \psi|.
\end{align}
In slight abuse of notation, we here use the same symbols for the 
operators $A_j^{(k)}(m)$ acting on functions with and without spin 
components.

The idea now is to make use of lemma 
\ref{thm:estimatetensoroperators}. In order to be able to apply the 
lemma, we first note that by Young's inequality for 
$a_1,...,a_N \in \R$, we have 
$\left(\sum_{i=1}^N a_i\right)^2 \leq N \sum_{i=1}^N a_i^2$ and thus:
\begin{equation}
	|A\psi (x_1,x_2)|^2 ~\leq~ 64\, \|K\|^2 \sum_{i,j=1}^4 \sum_{k=0}^3 \big| A_1^{(i)}(m_1) A_2^{(j)}(m_2) \, |\mathcal{D}_k \psi|\big|^2.
\end{equation}
Integrating over this expression and using lemma 
\ref{thm:estimatetensoroperators}, we obtain:
\begin{align}\nonumber
    \| (A \psi)(x_1^0,\cdot,x_2^0,\cdot)\|^2_{L^2} ~\leq ~64 \, &\|K\|^2\sum_{i,j=1}^4 \sum_{k=0}^3 \mathcal{A}_1^{(i)}(m_1) \mathcal{A}_2^{(j)}(m_2) \,\\
    &\times \| (\mathcal{D}_k \psi)(x_1^0,\cdot,x_2^0,\cdot) \|^2_{L^2}.
\end{align}
Recalling the definition of $[\psi]^2(x_1^0,x_2^0)$, Eq.\ 
\eqref{eq:spatialnorm} yields \eqref{eq:apsiestimate}.

Next, we turn to \eqref{eq:d1apsiestimate}. We start from the initial 
form of the integral equation \eqref{eq:inteq} and use that as a 
distributional identity on test functions $\psi \in \mathscr{D}_T$, 
we have $D_1 S^\ret(x_1-x_1') = \delta^{(4)}(x_1-x_1')$. Thus, we 
obtain:
\begin{equation}
	(D_1 A \psi)(x_1,x_2) = \int_{\tfrac{1}{2} \M} d^4 x_2' ~S_2^\ret(x_2-x_2') (K \psi)(x_1,x_2').
\end{equation}
Proceeding similarly as for \eqref{eq:defa} we rewrite this as:
\begin{equation}
	D_1 (A \psi)\! =\! \left( A_2^{(1)}(m_2)\, D_2\! +\! A_2^{(2)}(m_2) D_2 \!+\! A_2^{(3)}(m_2) \!+\! A_2^{(4)}(m_2)\right)\! (K\psi).
\end{equation}
Considering the form of $A_j^{(k)}(m_j)$ this implies:
\begin{equation}
	|D_1 (A \psi)| ~\leq~  \| K \| \sum_{i=1}^4 \sum_{k\in \{ 0,2\}} A_2^{(i)}(m_2)\, |\mathcal{D}_k \psi|.
\end{equation}
We now square and use Young's inequality, finding:
\begin{equation}
	|D_1 (A \psi)|^2 ~\leq~  8 \, \| K \|^2 \sum_{i=1}^4 \sum_{k\in \{ 0,2\}} A_2^{(i)}(m_2) \, | \mathcal{D}_k \psi|^2.
\end{equation}
Integrating and using lemma \ref{thm:estimatetensoroperators} yields:
\begin{align}\nonumber
  \| D_1 (A \psi)(x_1^0,\cdot,x_2^0,\cdot)\|^2_{L^2} ~\leq~ 8 \, &\| K \|^2 \sum_{i=1}^4 \sum_{k\in \{ 0,2\}} \mathcal{A}_2^{(i)}(m_2)\,\\
  & \| (\mathcal{D}_k \psi)(x_1^0,\cdot,x_2^0,\cdot) \|^2_{L^2}.
\end{align}
Adding the terms with $k=1,3$ and using the definition of 
$[\psi]^2(x_1^0,x_2^0)$ gives us \eqref{eq:d1apsiestimate}.

The estimate \eqref{eq:d2apsiestimate} follows in an analogous way.

Finally, for \eqref{eq:d1d2apsiestimate} we also start from the 
initial integral equation \eqref{eq:inteq} and use 
$D_i S_i^\ret(x_i-x_i') = \delta^{(4)}(x_i-x_i')$. This results in:
\begin{equation}
	D_1 D_2 (A \psi) = K \psi.
\end{equation}
Squaring and integrating gives us:
\begin{align}\nonumber
  \| D_1 D_2 (A \psi)(x_1^0,\cdot,x_2^0,\cdot) \|^2 ~ &\leq ~\| K \|^2 \, \| \psi(x_1^0,\cdot,x_2^0,\cdot) \|^2_{L^2} \\
  \leq~ \| K \|^2 \, [\psi]^2(x_1^0,x_2^0),
\end{align}
which yields \eqref{eq:d1d2apsiestimate}. 
\end{proof}

These estimates are the core of:

%%%
\begin{proof}[Proof of Lemma\ \ref{thm:boundsa}:]
	
	We use lemma \ref{thm:spatialnormestimates} together with the definition of $[\psi]^2(x_1^0,x_2^0)$ to obtain:
  \begin{equation}
		[A \psi]^2(x_1^0,x_2^0) ~\leq~ \eqref{eq:apsiestimate} + \eqref{eq:d1apsiestimate}+ \eqref{eq:d2apsiestimate}+ \eqref{eq:d1d2apsiestimate}.
  \end{equation}
	Summarizing the operators into a product yields \eqref{eq:spatialnormapsi}.

\end{proof}

%%%%%%%
\subsubsection{Proof of Theorem \ref{thm:minkhalfspace}} \label{sec:proofminkhalfspace}

In order to prove Thm. \ref{thm:minkhalfspace}, we combine the 
previous estimates to show that \(\|A\|<1\), first on test 
functions $\psi \in \mathscr{D}$ and by linear extension also on the 
whole of $\Banach_g$ . We start with Eq. \eqref{eq:spatialnormapsi} 
of Thm. \ref{sec:minkhalfspace} using the definition of 
\(\mathcal{A}_j\) for \(j=1,2\), as well as the following 
estimate, valid for all \(\psi \in \mathscr{D}, t_1,t_2>0\):
\begin{equation}
	[\psi](t_1,t_2) ~=~ [\psi](t_1,t_2) \, \frac{g(t_1)g(t_2)}{g(t_1)g(t_2)} ~\le~ \|\psi\|_g\, g(t_1)g(t_2).
\end{equation}
Using this in \eqref{eq:spatialnormapsi} yields:
\begin{align}
\|A\psi\|^2_g ~&\le~ \sup_{x_1^0,x_2^0>0}\frac{1}{(g(x_1^0)g(x_2^0))^2} \,  \|K\|^2 \prod_{j=1,2} \left( \id + 8\mathcal{A}_j(m_j) \right) [\psi]^2(x_1^0,x_2^0),\\\nonumber
&\le~ \sup_{x_1^0,x_2^0>0} \frac{\|\psi\|^2}{(g(x_1^0)g(x_2^0))^2} \,  \|K\|^2\\
& \hspace{2cm}\times\prod_{j=1,2} \left( \id + 8\mathcal{A}_j(m_j) \right) \, (g^2\otimes g^2)(x_1^0,x_2^0),\\
&\le~ \|K\|^2\, \|\psi\|^2_g \left( \sup_{t>0}\frac{1}{g(t)^2} \left( \id + 8\mathcal{A}(\mu) \right)g^2 (t) \right)^2,
\label{est:NormA1}
\end{align}
where $\mu = \max \{ m_1,m_2\}$ and 
$\mathcal{A}(\mu) = \sum_{k=1}^4 \mathcal{A}^{(k)}(\mu)$ with 
$ \mathcal{A}^{(k)}(\mu)$ as in \eqref{eq:defcurlyoperators}.

Next, we shall estimate the term in the big round bracket. To this 
end, we first note some special properties of \(g^2\), which 
motivated choosing $g$ as in \eqref{eq:defg}.
%%%
\begin{Lemma}\label{lem:int}
 For all \(t>0\), we have
\begin{equation}
	\int_0^t d\tau \, g^2(\tau) = \frac{t}{1+b t^8} \, g^2(t).
\end{equation}
\end{Lemma}
\begin{proof}[Proof:] Differentiating the right side of the equation 
  and using the concrete function \(g^2\) as in \eqref{eq:defg} shows 
  that it is, indeed, the anti-derivative of \(g^2\). Since this 
  function vanishes at \(t=0\), the claim follows. \qed
\end{proof}

%%%
\begin{Lemma}\label{lem:someidentities}
For $c<8$ we have
\begin{equation}
	\sup_{t>0} \frac{t^c}{1+b t^8}  ~=~\frac{c}{8}\,  b^{-c/8} \left( \frac{8}{c}-1\right)^{-c/8},
	\label{eq:firstequality}
\end{equation}
and furthermore for $c=8$:
\begin{equation}
	\sup_{t>0} \frac{t^8}{1+b t^8} ~=~\frac{1}{b}.
	\label{eq:secondequality}
\end{equation}
\end{Lemma}

\begin{proof}
To prove \eqref{eq:firstequality}, considering the shape of the 
function $h(t)= t^c/(1+b t^8)$ we find that the supremum is in 
fact a maximum which is located at 
\(t = b^{-1/8} \left(8/c-1\right)^{-1/8}\). Inserting this back into 
the function $h(t)$ yields \eqref{eq:firstequality}.  
Equation \eqref{eq:secondequality} follows from 
$\frac{t^8}{1+b t^8} = \frac{1}{b} \frac{1}{1/(b t^8) +1} \leq 
\frac{1}{b}$. \qed
\end{proof}

\paragraph{Proof of Thm. \ref{thm:minkhalfspace}:} Applying 
Lemma \ref{lem:int} to \(\mathcal{A}(\mu) \,g^2\) yields:

\begin{align}
    \big(\mathcal{A}^{(1)}(\mu) \, g^2\big)(t)&~=~ t \int_0^t d\rho \, (t-\rho)^2\, g^2(\rho)~\le~ t^3 \int_0^t d\rho \,g^2(\rho)\nonumber\\
    &\hspace{2cm} = \frac{t^4}{1+b t^8} \,g^2(t),\nonumber\\
     \big(\mathcal{A}^{(2)}(\mu) \, g^2\big)(t)&~=~ \frac{\mu^4\, t^4}{2^4\, 3^2} \int_0^t d\rho\, (t-\rho)^3\, g^2(\rho)~\le~ \frac{\mu^4\, t^8}{2^4 \,3^2} \frac{g^2(t)}{1+b t^8},\nonumber\\
    \big( \mathcal{A}^{(3)}(\mu) \, g^2\big) (t) &~=~ t^2,\nonumber\\
    \big( \mathcal{A}^{(4)}(\mu) \, g^2 \big)(t) &~=~ \frac{\mu^4\, t^6}{2^2\, 3^2}.
\end{align}
Multiplying with \(1/g^2(t)\) and using Lemma 
\ref{lem:someidentities} as well as $1/g(t)^2 \le (1+b t^8)^{-1}$, 
we find:
\begin{align}
    g^{-2}(t)\, \big(\mathcal{A}^{(1)}(\mu) \,g^2\big)(t) & ~\le~ \frac{1}{2} \, b^{-\frac{1}{2}},\nonumber\\
    g^{-2}(t)\, \big(\mathcal{A}^{(2)}(\mu) \,g^2\big)(t) & ~\le~ \frac{\mu^4}{2^4 \,3^2 \,b},\nonumber\\
    g^{-2}(t)\, \big( \mathcal{A}^{(3)}(\mu) \, g^2\big)(t) & ~\le~ \frac{1}{2^2\, (3b)^{1/4}},\nonumber\\
   g^{-2}(t)\, \big( \mathcal{A}^{(4)}(\mu) \, g^2\big) (t) & ~\le~ \frac{\mu^4}{2^4 \, 3^{1/4}} \, b^{-3/4}.
\label{eq:gtotheminus2estimates}
\end{align}
Using \eqref{est:NormA1}, we can employ these inequalities (whose 
right hand sides are inversely proportional to powers of $b$) to 
estimate the norm of $A$. According to \eqref{est:NormA1}, we have, 
first on $\mathscr{D}$ and by linear extension also on the whole of 
$\Banach_g$: 
\begin{equation}
	\|A\| ~\leq~ \|K\| \, \sup_{t>0}\, g^{-2}(t) \big((\id + 8\mathcal{A}(\mu)) \, g^2 \big)(t) .
\end{equation}
Now we use \eqref{eq:gtotheminus2estimates} for the various 
contributions 
$A^{(k)}(\mu) $ to $\mathcal{A}(\mu) = \sum_{k=1}^4 A^{(k)}(\mu)$, 
finding:
\begin{align}
	\|A\| & ~\le~ \|K\| + \frac{4 \|K\|}{b^{1/2}} + \frac{\mu^4 \|K\|}{18 b} + \frac{2 \|K\|}{(3b)^{1/4}} + \frac{\mu^4 \|K\|}{2 (3b^3)^{1/4}}\nonumber\\
&\overset{b\ge 1}{\le}~ \|K\| + \frac{\|K\|}{b^{1/4}} \left(4+ \mu^4/18 + 2/3^{1/4} + \mu^4/(2 \cdot 3^{1/4})\right)\\
&<~ \|K\| + \frac{\|K\|}{b^{1/4}} (6 + \mu^4 ).
\end{align}
Recalling that $b=\frac{\|K\|^4}{(1-\|K\|)^4} (6+\mu^4)^4$ (see 
\eqref{eq:defb}), we finally obtain that:
\begin{equation}
	\|A\| ~<~ \|K\| + \frac{\|K\|}{b^{1/4}} (6 + \mu^4 ) ~= ~ \|K\| + 1-\|K\| =1.
\end{equation}
We have thus shown that $A$ defines (by linear extension) a 
contraction on $\Banach_g$. Thus, the Neumann series 
$\psi = \sum_{k=0}^\infty A^k \psi^\free$ yields the unique 
(global-in-time) solution of the equation 
\(\psi=\psi^\free+A\psi\). \qed


%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
\subsection{Conclusion and outlook} \label{sec:discussion}

Extending previous work for Klein-Gordon particles 
\cite{mtve,lienertcurved} to the Dirac case, we have established the 
existence of dynamics for a class of integral equations which express 
direct interactions with time delay at the quantum level. To obtain 
this result, we have assumed a cutoff of the spacetime before $t=0$. 
It has been demonstrated that the Big Bang singularity can naturally 
provide such a cutoff. Remarkably, this yields a class of rigorous 
interacting models in 1+3 spacetime dimensions.

Compared to the previous works \cite{mtve,int_eq_curved}, our 
techniques have been modified and improved. Instead of demonstrating 
explicitly the convergence of the Neumann series by iterating the 
estimate \eqref{eq:spatialnormapsi} -- which is lengthy -- we have 
here succeeded in directly showing that $A$ is a contraction on the 
weighted space $\Banach_g$ for a suitable $g$. This also has the 
advantage that no arbitrary final time $T$ as in 
\cite{mtve,int_eq_curved} had to be introduced which could only 
later be taken to infinity by an additional argument (involving a 
change of Banach space).

The main challenge in our work has been the non-Markovian nature of 
the dynamics. This has made it necessary to directly prove global 
existence in time instead of concatenating short-time solutions 
on small time intervals (which would have been easier to obtain). 
Apart from this, the distributional derivatives in the Green's 
functions of the Dirac equation have made the analysis substantially 
more difficult than in the Klein-Gordon case. Compared to the 
latter, we have also treated the massive case (which was not 
considered in \cite{mtve} for 1+3 dimensions).

Our results are furthermore characterized as follows. We have shown 
that the wave function is determined by Cauchy data at the initial 
time (corresponding to the Big Bang singularity); however, no 
Cauchy problem is available at different times. The main requirement 
of our theorems is a smallness condition on the interaction kernel 
$K$, demanding that both $K$ and certain first and second order 
derivatives of $K$ must be bounded and not too large. This still 
admits a wide class of interaction kernels, and we emphasize that 
in no way the interaction needs to be small compared to the size of 
the domain of the wave function. The latter is a common requirement 
for Fredholm integral equations but it would make the result 
worthless for infinite spatio-temporal domains.

Besides, we have assumed that $K$ is complex-valued while it could 
be matrix-valued in the most general case. The reason for this 
assumption is that our proof requires the integral operator $A$ to 
be a map from a certain Sobolev space onto itself in which weak 
derivatives with respect to the Dirac operators of the two particles 
can be taken. If $K$ were matrix-valued, it would not commute with 
these Dirac operators in general. Then $A \psi$ would contain new 
types of weak derivatives which cannot be taken in the initial 
Sobolev space. As illustrated in Sec.\ \ref{sec:choiceofB}, this 
creates a situation where more and more derivatives have to be 
controlled, possibly up to infinite order where the success of an 
iteration scheme seems unlikely. At present, we do not know how to 
deal with this issue. 
Improving on this point, however, defines an important task for 
future research, as e.g.\ electromagnetic interactions involve 
interaction kernels proportional to $\gamma_1^\mu \gamma_{2\mu}$ 
(see \cite{direct_interaction_quantum}).

In addition, it would be desirable to generalize our work in the 
following regards.
\begin{itemize}
  \item \textit{$N$ particle integral equations.} Our hope is that 
  our work could contribute to the formulation of a rigorous 
  relativistic many-body theory that can be applied for finite 
  times, not only for scattering processes. An important step in 
  this direction is to treat an arbitrary fixed number $N \in \N$ 
  of particles (setting aside particle creation and annihilation).  
  A class of possible $N$-particle integral equations has been 
  suggested in \cite{direct_interaction_quantum}. It has the 
  schematic form
	\begin{align}\nonumber
    \psi(x_1,...,x_N) = \psi^\free + &\sum_{i < j} \int d^4 x_i' \, d^4 x_j' S_i(x_i-x_i')\, S_j(x_j-x_j')\\
    &\times K_{ij}(x_i',x_j')\, \psi(...x_i', ...,x_j',...).
  \end{align}
  It might well be possible to prove the existence and uniqueness of 
  solutions for that equation using the methods developed in the 
  present paper.
	%
	\item \textit{Singular interaction kernels.} The physically most natural interaction kernel is given by a delta function along the light cone, $K(x_1,x_2) \propto \delta((x_1-x_2)_\mu (x_1-x_2)^\mu)$. Getting closer to this case is one of our central goals. Apart from approaching the problem head-on by suitably interpreting the distributional expressions and trying to prove the existence of solutions of the resulting singular integral equation, which seems difficult, one could also try to make smaller steps first. For example, one could decompose $\delta((x_1-x_2)_\mu (x_1-x_2)^\mu))$ into $\frac{1}{2 |\vx_1-\vx_2|} [ \delta(x_1^0-x_2^0 - |\vx_1-\vx_2|) + \delta(x_1^0-x_2^0 + |\vx_1-\vx_2|) ]$ and only then replace the delta functions with a peaked but smooth function, keeping the singular factor $1/|\vx_1-\vx_2|$. This has been done in \cite{mtve} for the Klein-Gordon case. In the Dirac case, the distributional derivatives make a generalization of that result difficult, and we have not attempted it here. However, it is conceivable that a suitable modification of our techniques could make it possible to treat this case.\\
Another interesting question is whether the smallness condition on 
$K$ can be alleviated such that arbitrarily peaked functions are 
admitted. This could allow taking a limit where $K$ approaches the 
delta function along the light cone.
\end{itemize}








\section{Singular light cone interactions of scalar particles in 1+3 dimensions}\label{sec:KG lightcones}
Here we consider an integral equation describing a fixed number of scalar particles which interact not through boson exchange but directly along light cones, similarly as in bound state equations such as the Bethe-Salpeter equation. The equation involves a multi-time wave function $\psi(x_1,...,x_N)$ with $x_i=(t_i,\vx_i) \in \R^4$ as a crucial concept. Assuming a cutoff in time, we prove that it has a unique solution for all data at the initial time. The cutoff is justified by considering the integral equation for a particular curved spacetime with a Big Bang singularity where an initial time occurs naturally without violating any spacetime symmetries. The main feature of our work is that we treat the highly singular case that interactions occur exactly at zero Minkowski distance, reflected by a delta distribution along the light cone. We also extend the existence and uniqueness result to an arbitrary number $N \geq 2$ of particles. Overall, we provide a rigorous example for a certain type of interacting relativistic quantum dynamics in 1+3 spacetime dimensions.

\subsection{Introduction}
\label{sec:intro}

%%%%%
\subsubsection{Motivation}
The goal of this paper is to prove the existence and uniqueness of solutions of the equation
\begin{equation}
	\psi(x,y) = \psi^\free(x,y) + \int d^4 x' \,d^4 y'~G_1^\ret(x-x') G_2^\ret(y-y') K(x',y')\psi(x',y')
\label{eq:inteq} 
\end{equation}
and its $N$-particle generalization for the singular case of light cone interactions, i.e., for
\begin{equation}
	K(x,y) ~=~ \frac{\lambda}{4\pi} \delta((x-y)^2).
\end{equation}
Here, $\lambda > 0$ is a coupling constant and $(x-y)^2 = (x^0-y^0)^2 - |\vx-\vy|^2$ stands for the Minkowski distance of the spacetime points $x=(x^0,\vx)$ and $y=(y^0,\vy)$. Moreover, $\psi^\free$ is a solution of the free Klein-Gordon equation in each variable, i.e., $(\Box_k+m_k^2)\psi^\free(x_1,x_2) = 0$, $k=1,2$. We shall later see that $\psi^\free$ plays the role of initial data for Eq.\@ \eqref{eq:inteq}. $G_k^\ret$ stands for the retarded Green's function of the respective Klein-Gordon equation. $\psi$ is a \textit{multi-time wave function}, i.e., for $N=2$ particle, a map
\begin{equation}
	\psi : \text{spacetime} \times \text{spacetime} \rightarrow \CC,~~~(x,y) \mapsto \psi(x,y).
\label{eq:multitimewf}
\end{equation}
The crucial point about Eq.\@ \eqref{eq:inteq} is that it describes a fixed number of (here $N=2$) interacting particles in a manifestly Lorentz invariant way. Interactions happen directly along the light cones instead of through particle exchange as in quantum field theory. Such a relativistically invariant interacting dynamics for a fixed number of particles in 1+3 spacetime dimensions is difficult to achieve; in fact, for Hamiltonian theories this is generally believed to be impossible, and there have long been no-go theorems in that direction (see e.g. \cite{nointeraction,nointeraction2}). It is therefore not surprising that Eq.\@ \eqref{eq:inteq} has a distinctly non-Hamiltonian character. This is evident from the fact that the interaction term involves values of $\psi$ in the past, not only on a Cauchy surface which defines the present. In fact, the time delay of the interaction is an important resource of the kind of dynamics defined by Eq.\@ \eqref{eq:inteq}, and it is only made possible by the more general notion of wave function, the multi-time wave function $\psi$. 

The concept of multi-time wave functions enjoys a long history, going back to 
well-known physicists such as Eddington \cite{eddington}; Dirac \cite{dirac_32}; 
Dirac, Fock, Podolsky \cite{dfp}; Bloch \cite{bloch}; Tomonaga \cite{tomonaga} 
and Schwinger \cite{schwinger}. While it was intermittently picked up during 
the years (see e.g. 
\cite{guenther_1952,marx_1974,schweber,drozvincent_1981,sazdjian_2bd,2bdem}, it 
has recently received renewed attention and undergone significant developments 
\cite{nogo_potentials,qftmultitime,multitime_pair_creation,1d_model,nt_model,2bd_current_cons,deckert_nickel_2016,lpt_2017b,generalized_born,ibc_model,phd_nickel,deckert_nickel_2019,lnt_2020}; 
an overview from 2016 can be found in \cite{dice_paper}. The idea is 
straightforward: to seek a Lorentz covariant generalization of the usual 
Schr\"odinger picture wave function (here for $N=2$)
\begin{equation}
	\varphi : \R \times \R^3 \times \R^3 \rightarrow \CC,~~~~~ (t,\vx,\vy)\mapsto \varphi(t,\vx,\vy).
\end{equation}
In fact, the relation of $\psi$ to $\varphi$ is just given 
by evaluation of $\psi$ at equal times in a given Lorentz 
frame:
\begin{equation}
	\varphi(t,\vx,\vy)~=~\psi(t,\vx,t,\vy).
\end{equation}
For the present purposes, the point of interest is that the 
concept of a multi-time wave function allows us to see the 
integral equation \eqref{eq:inteq} as the natural 
generalization of the Schr\"odinger equation 
$(i \partial_t - H_1^\free - H_2^\free - 
V(t,\vx,\vy))\varphi(t,\vx,\vy) = 0$ when formulated as an 
integral equation (see \cite{direct_interaction_quantum} 
for a more detailed discussion). The latter can namely be 
written as
\begin{align}
  \varphi(t,\vx,\vy) =& \varphi^\free(t,\vx_1,\vx_2) + \int_0^\infty \!\! dt' \int d^3 \vx' \, d^3 \vy' G_1^\ret(t-t',\vx-\vx')\nonumber\\
&\times  G_2^\ret(t-t',\vy-\vy') V(t',\vx',\vy')\varphi(t',\vx',\vy'),
	\label{eq:schroedint}
\end{align}
where $\varphi^\free$ is a solution of 
$(i \partial_t - H_1^\free - H_2^\free)\varphi(t,\vx,\vy) 
= 0$ and $G_k^\ret$ stands for the retarded Green's 
function of the operator $(i\partial_t - H_k^\free)$.

Now, our previous integral equation \eqref{eq:inteq} 
reduces to a very similar equation when we neglect the 
time delay $|\vx-\vy|$ (we here work with units where 
$\hbar = 1 = c$) by replacing
\begin{equation}
	\delta((x'-y')^2) = \frac{\delta({x'}^0-{y'}^0-|\vx'-\vy'|) + \delta({x'}^0-{y'}^0+|\vx'-\vy'|)}{2|\vx'-\vy'|}
\label{eq:lcdeltasplit}
\end{equation}
with $\delta({x'}^0-{y'}^0)/|\vx'-\vy'|$. After performing 
the time integration over ${y'}^0$, renaming the remaining 
time variable ${x'}^0$ as $t'$ and considering on the left 
hand side of \eqref{eq:inteq} at equal times $x^0= t = y^0$ 
in the given Lorentz frame, we arrive at 
\eqref{eq:schroedint} with Klein-Gordon Green's functions 
and $V(t,\vx,\vy) \propto 1/|\vx-\vy|$, the Coulomb 
potential.

This train of thought suggests that Eq.\@ \eqref{eq:inteq} 
constitutes a natural generalization of the Schr\"odinger 
equation \eqref{eq:schroedint} for relativistic quantum 
phenomena (here for two scalar particles with 
electromagnetic interactions) -- at least for processes 
where particle creation and annihilation are not relevant, 
such as relativistic bound states. Interestingly, this is 
also the domain where the well-known Bethe-Salpeter 
equation \cite{bs_equation,nakanishi} of quantum field 
theory is usually applied. The equation also involves a 
multi-time wave function \eqref{eq:multitimewf} and has a 
similar form as \eqref{eq:inteq}. The main differences, 
however, are that for the Bethe-Salpeter equation (a) the 
interaction kernel $K$ is not given by a clear-cut 
expression such as $\delta((x-y)^2)$ but by a (potentially 
divergent) infinite series of Feynman diagrams, and (b) 
that it involves Feynman Green's functions instead of 
retarded Green's functions. Contrary to retarded Green's 
functions, Feynman Green's functions have support not only 
along and inside backward light cones. Nevertheless, the 
similarity of \eqref{eq:inteq} with the Bethe-Salpeter 
equation constitutes further motivation for its study.

In this context, it is interesting to note that previous 
works about the existence and solutions of simplified 
models for the Bethe-Salpeter equation (as in the 
so-called Wick-Cutkosky model, see 
\cite{wick_54,cutkosky54,green57,consenza65,tiktopoulos65,obrien75}) 
have (to the best of our knowledge) not answered the 
question of the existence and uniqueness of solutions. 
Rather, they omit the free solution $\psi^\free$ from 
the equation (which, as we will see, leads only to the 
trivial solution in our case), perform a Fourier 
transform in all eight variables, assume a plane wave in 
the center-of-momentum coordinate, use a Wick rotation 
and then study the eigenvalue problem of a resulting 
equation of the qualitative form $\widetilde{\psi} = 
\lambda \widehat{K}(E) \widetilde{\psi}$ in $\lambda$ 
(instead of the energy $E$ in the center-of-momentum 
frame). A transformation back to the original problem is 
not attempted (and may not always be possible). While 
these results are nevertheless interesting as they reveal 
features of possible stationary states of the actual 
problem (i.e., for the physical value of $\lambda$), they 
are far-removed from the physical problem of time 
evolution of the quantum-mechanical wave function which we 
attempt to address here.


%%%%%
\subsubsection{Previous Works}

The physical ideas underlying the multi-time integral equation 
\eqref{eq:inteq} were first introduced in 
\cite{direct_interaction_quantum}. That paper includes a 
more detailed derivation of \eqref{eq:inteq} as a relativistic 
generalization of the integral version of the Schr\"odinger 
equation, the treatment of the non-retarded limit as well as a 
comparison with differential multi-time equations. Furthermore, 
it discusses the parallels of \eqref{eq:inteq} with classical 
action-at-a-distance electrodynamics (where interactions also 
occur directly along the light cone). In addition, different 
$N$-particle generalizations of \eqref{eq:inteq} are compared 
and analyzed.

The first rigorous results about the existence and uniqueness of 
solutions of multi-time integral equations of the form 
\eqref{eq:inteq} were obtained in \cite{mtve}. This article 
also focuses on the (simpler) case of scalar particles; in 
addition, it makes two important assumptions: (i) Only bounded 
or weakly singular interactions kernels $K$ instead of 
$\delta((x-y)^2)$ are considered which makes the problem much 
easier to treat. (ii) A cutoff in time is assumed, meaning that 
the domain of integration is only $(\frac{1}{2}\M)\times 
(\frac{1}{2}\M)$ where $\tfrac{1}{2}\M = [0,\infty)\times \R^3$ 
denotes a Minkowski half-space. This assumption together with 
the fact that the retarded Green's functions are only supported 
on and inside of the backward light cone has the important effect 
of rendering the domain of integration in \eqref{eq:inteq} finite. 
In addition, one obtains a Volterra-structure in the time 
variables, meaning that the time integrations in ${x'}^0$ and 
${y'}^0$ only run from 0 to $x^0$ and from 0 to $y^0$, 
respectively. This, in turn, made it possible to utilize an 
efficient iteration scheme for the proof of existence and 
uniqueness of solutions. The result was that for every free 
solution $\psi^\free$ in a suitable Banach space, the integral 
equation posseses a unique solution $\psi$ in that space which, 
furthermore, agrees with $\psi^\free$ at the initial time, i.e., 
for $x^0 = 0 = y^0$.

The assumption of a cutoff in time was made in \cite{mtve} with 
reference to a potential Big Bang singularity without, however, 
considering \eqref{eq:inteq} on curved spacetimes. To carry out 
this task for a class of spacetimes where the Green's functions 
of the (conformal) wave equation are explicitly known was the 
topic of \cite{lienertcurved}. There, it was shown that Eq.\@ 
\eqref{eq:inteq} has a straightforward generalization to curved 
spacetimes. A number of explicit examples (flat, open and closed 
Friedman-Lema\^itre-Robertson-Walker (FLRW) spacetimes) was 
formulated, and it was shown that for most of these cases, 
conformal transformations could be used to reduce the proof of 
existence and uniqueness of solutions to the one on 
$(\frac{1}{2}\M)^2$. Thereby, the point was made that a cutoff 
in time can arise naturally in a cosmological context, without 
violating any spacetime symmetries.

The most recent work about multi-time integral equation is 
\cite{selfDirac}. It is concerned with extending the previous 
results to the case of Dirac particles. This has been achieved 
for a class of sufficiently regular interaction kernels $K(x,y)$. 
The main difficulty in the Dirac case compared to the Klein-Gordon 
case is that the Green's functions involve distributional 
derivatives which complicates the analysis. In particular, it 
becomes necessary to achieve a delicate balance of the regularity 
of solutions with the form of the integral equation. Apart from 
this, the work \cite{selfDirac} also led to some technical 
developments where the method of proof was refined by directly 
using a contraction argument on a weighted Sobolev space instead 
of the Volterra iteration scheme of \cite{mtve,lienertcurved}.


%%%%%
\subsubsection{Overview of the paper}

The goal of the present paper is to extend the previous results for 
scalar particles to the physically most interesting case $K(x,y) 
\propto \delta((x-y)^2)$. This is, at the same time, a highly 
singular and therefore challenging case. It becomes necessary 
to define the particular combination of the three distributions 
$G^\ret_1(x-x')$, $G_2^\ret(y-y')$ and $\delta((x-y)^2)$ which 
occurs in \eqref{eq:inteq} and then prove the existence and 
uniqueness of the resulting singular integral equation. This 
equation significantly differs from that considered in 
\cite{mtve} where through admitting only less singular interaction 
kernels $K$ only two singular distributions acting on different 
variables needed to be considered.

The paper is structured as follows. In Sec.\@ \ref{sec:formulation} 
it is shown how to define the integral equation in a rigorous way 
(by using the delta distributions to eliminate certain integration 
variables). To this end, we again consider the equation on the 
Minkowski half-space (assuming a cutoff in time). Section 
\ref{sec:results} contains our main results: Thm.\@ \ref{thm:bounds} 
contains explicit bounds for the integral operator in terms of a 
general weight function of a weighted $L^\infty$ space. Thm.\@ 
\ref{thm:exponentialg} shows that in the case of massless particles 
already an exponential weight function leads to the existence and 
uniqueness of solutions of the integral equation. Our main result 
is Thm.\@ \ref{thm:existence}, an existence and uniqueness theorem 
for the full (massive) case. In that case, a different weight 
function growing like the exponential of a polynomial is used. 

Section \ref{sec:Npart} deals with generalizing this existence and 
uniqueness theorem to $N$ scalar particles; the corresponding 
theorem, Thm.\@ \ref{thm:Npart}, is a direct consequence of Thm.\@ 
\ref{thm:existence}. To the best of our knowledge, this is the 
first rigorous result about a multi-time integral equation for 
$N$-particles. 

In Section \ref{sec:curvedspacetime} we show by considering a 
specific example (an open FLRW spacetime) that the cutoff in time 
can be achieved naturally for a cosmological spacetime with a Big 
Bang singularity, without breaking any spacetime symmetries. That 
is, we show the equivalent result of \cite{lienertcurved} for 
singular light cone interactions. The respective existence and 
uniqueness theorem is Thm.\@ \ref{thm:existencecurved}.

Section \ref{sec:proofs} contains the proofs. In Sec.\@ 
\ref{sec:conclusions}, we conclude.


%%%%%
\subsection{Precise formulation of the problem}
\label{sec:formulation}

In the following, we show how to precisely define the integral 
equation \eqref{eq:inteq} for the case of two scalar particles with 
masses $m_1$ and $m_2$ on the Minkowski half space $\frac{1}{2}\M = 
[0,\infty) \times \R^3$. Strictly speaking, to introduce a cutoff 
in time in this way breaks the Poincar\'e invariance of 
\eqref{eq:inteq}; however, we will give an argument for its use 
in Sec.\@ \ref{sec:curvedspacetime}.

It is necessary to take special care of the definition of the 
integral equation as it contains certain combinations (convolutions 
and products) of distributions (the Green's functions). Our strategy 
is to consider the integral operator acting on test functions first 
where its action can be defined straightforwardly. Later it will be 
shown that it is bounded on test functions with respect to a suitably 
chosen weighted norm. This will make it possible to linearly extend 
the integral operator to the completion of test functions with 
respect to that norm.

The retarded Green's function of the Klein-Gordon equation is given 
by:
\begin{equation}
	G^\ret(x) ~=~ \frac{1}{4\pi|\vx|}\delta(x^0-|\vx|) - \frac{m}{4\pi \sqrt{x^2}} H(x^0-|\vx|) \frac{J_1(m\sqrt{x^2})}{\sqrt{x^2}} 
	\label{eq:gretkg}
\end{equation}
where $H$ denotes the Heaviside function and $J_1$ stands for a 
Bessel function of the first kind.
Then, with $K(x,y) = \frac{\lambda}{4\pi} \delta((x-y)^2)$, our 
integral equation \eqref{eq:inteq} on $(\tfrac{1}{2}\M)^2$ becomes:
\begin{equation}
	\psi ~=~ \psi^\free + A \psi
\label{eq:abstractinteq}
\end{equation}
where $A = A_0 + A_1 + A_2 + A_{12}$ and
\begin{align}
(A_0 \psi)(x,y) ~=& \frac{\lambda}{(4\pi)^3} \int_0^{x^0} d {x'}^0 \int_{\mathbb{R}^3} d^3 \vx' \int_0^{y^0} d{y'}^0 \int_{\mathbb{R}^3} \nonumber\\ 
  &\times \frac{\delta(x^0-{x'}^0-|\vx-\vx'|)}{|\vx-\vx'|}\frac{\delta(y^0-{y'}^0-|\vy-\vy'|)}{|\vy-\vy'|}\notag\\
  &\times\delta((x'-y')^2) \psi(x',y'),\label{eq:a0informal}\\
(A_1 \psi)(x,y) ~=&  -\frac{\lambda \, m_1}{(4 \pi)^3} \int_0^\infty d{x'}^0 \int d^3 \vx' \int_0^\infty d{y'}^0 \int d^3 \vy'  \nonumber\\
  &\times H(x^0-{x'}^0-|\vx-\vx'|)\frac{J_1(m_1\sqrt{(x-x')^2})}{\sqrt{(x-x')^2}} \notag \\
  &\times  \frac{\delta(y^0-{y'}^0-|\vy-\vy'|)}{|\vy-\vy'|}\delta((x'-y')^2) \psi(x',y')\label{eq:a1informal}\\
(A_2 \psi)(x,y) ~=& -\frac{\lambda \, m_2}{(4 \pi)^3} \int_0^\infty d{x'}^0 \int d^3 \vx' \int_0^\infty d{y'}^0 \int d^3 \vy' \nonumber\\
& \times \frac{\delta(x^0-{x'}^0-|\vx-\vx'|)}{|\vx-\vx'|}H(y^0-{y'}^0-|\vy-\vy'|) \notag\\
& \times \frac{J_1(m_2\sqrt{(y-y')^2})}{\sqrt{(y-y')^2}} \delta((x'-y')^2) \psi(x',y') \label{eq:a2informal}\\
(A_{12} \psi)(x,y) ~=& \frac{\lambda \, m_1 m_2}{(4\pi)^3}  \int_0^\infty d{x'}^0 \int d^3 \vx' \int_0^\infty d{y'}^0 \int d^3 \vy'  \nonumber\\
&\times H(x^0-{x'}^0-|\vx-\vx'|) \frac{J_1(m_1\sqrt{(x-x')^2})}{\sqrt{(x-x')^2}}  \nonumber\\
&\times H(y^0-{y'}^0-|\vy-\vy'|) \frac{J_1(m_2\sqrt{(y-y')^2})}{\sqrt{(y-y')^2}}\notag\\
&\quad \times\delta((x'-y')^2) \psi(x',y')\label{eq:a12informal}.
\end{align}
We now formally manipulate these informal expressions in such a way 
that the end results can be given a precise meaning on test 
functions. Let $\mathcal{S} = \mathcal{S}((\tfrac{1}{2}\M)^2)$ 
denote the space of Schwartz functions on $(\tfrac{1}{2}\M)^2$, 
and let $\psi \in \mathcal{S}$. 

%%%
\subsubsection{Definition of \(A_0\).}
We consider the massless term $A_0$ first which is also the most 
singular term. Using the $\delta$-functions to eliminate the 
integration over ${x'}^0$ and ${y'}^0$ results in:
\begin{align}
	(A_0 \psi)(x,y) =& \frac{\lambda}{(4\pi)^3} \int_{B_{x^0}(\vx)} \hspace{-0.5cm}d^3\vx' \int_{B_{y^0}(\vy)} \hspace{-0,5cm}d^3 \vy' \,   \nonumber \\
  &\times \frac{\delta((x^0-y^0-|\vx'|+|\vy'|)^2-|\vx-\vy +\vx'-\vy'|^2)}{|\vx'||\vy'|}\notag\\
  &\times \psi(x+x',y+y')|_{{x'}^0 = -|\vx'|, \, {y'}^0 = -|\vy'|},
	\label{eq:a0informal2}
\end{align}
Note that the domain of integration has been reduced to a compact 
region whose size depends on $x^0$ and $y^0$. There is still one 
more $\delta$-distribution left. We choose to use it to eliminate 
$|\vx'| =: r$. It is convenient to introduce the vector
\begin{equation}
	b = x-y-(-|\vy'|, \vy').
\label{eq:b}
\end{equation}
Then the argument of the delta function can be written as:
\begin{equation}
	(b^0-|\vx'|)^2 - |\vb + \vx'|^2.
\end{equation}
This expression has a root in $r$ for
\begin{equation}
	r =  r^* := \frac{1}{2} \frac{b^2}{b^0 + |\vb| \cos \vartheta}
\label{eq:r}
\end{equation}
where $\vartheta$ is the angle between $\vb$ and $\vx'$. Of course, 
$r^*$ inherits the restrictions of the range of $r$, thus is only a 
valid root for
\begin{equation}
	0 < r^* < x^0.
\end{equation}
The requirement $0 < r^*$ can be satisfied in two cases, either 
\(b^2>0\) and \(b^0>0\), or \(b^2<0\) and \(\cos\vartheta< - 
\frac{b^0}{|\vb|}\). Using these restrictions, the condition 
$r^*< x^0$ can be converted into a restriction of the domain of 
integration in \(\vartheta\):
\begin{align}
   & \frac{1}{2}\frac{b^2}{b^0+|\vb|\cos\vartheta}~ <~x^0\nonumber\\
   \iff~~~ &\sgn(b^2) b^2 ~< ~ 2x^0 \sgn(b^2) ( b^0+|\vb|\cos \vartheta)\nonumber\\
    \iff~~~ &\frac{|b^2|}{2x^0 |\vb|} -\frac{\sgn(b^2)b^0}{|\vb|} ~<~ \sgn(b^2) \cos \vartheta\nonumber\\
    \iff ~~~&\left\{\begin{matrix}\cos\vartheta > \frac{b^2}{2x^0|\vb|}- \frac{b^0}{|\vb|}, \quad \text{for} ~ b^2 >0 \\ \cos\vartheta < \frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|}, \quad \,  \text{for} ~ b^2<0. \end{matrix} \right.
\end{align}
In case of $b^2<0$, the new restriction on \(\cos\vartheta\) is 
stricter than \(\cos\vartheta<-\frac{b^0}{|\vb|}\); we thus use it 
to replace the latter. We evaluate the $\delta$-function using 
spherical coordinates in $\vy'$ and the usual rule
\begin{equation}
    \delta(f(z))=\sum_{z^* : f(z^*)=0} \frac{\delta(z-z^*)}{|f'(z^*)|},
\end{equation}
where \(f(r)=(b^0-r)^2-(\vb+x')^2= -(r-r^*)2(b^0+|\vb|\cos\vartheta) \). 
The result is an expression for \(A_0\psi\) which does not contain 
distributions anymore:
\begin{align}\nonumber
    &(A_0\psi)(x,y)=\frac{\lambda}{(4\pi)^3}\int_{B_{y^0}(\vy)}d^3\vy'  \int_0^{2\pi}d\varphi \int_{-1}^{1} d\!\cos\vartheta ~\frac{|b^2|}{4(b^0+|\vb|\cos\vartheta)^2 |\vy'|} \\
    &\left(\!1_{b^2>0}1_{b^0>0} 1_{\cos\vartheta > \frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|}}\!\!+1_{b^2<0}1_{\cos\vartheta<\frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|}}\!\right)\!\psi(x+\!x',y+\!y'),
\label{eq:defa0}
\end{align}
still subject to \(x'^0=-r^*=-|\vx'| , {y'}^0=-|\vy'|\). The 
different cases for $b$ have been implemented through the various 
indicator functions. Eq.\@ \eqref{eq:defa0} will serve as our 
\textit{definition} of $A_0$ on test functions $\psi \in \mathcal{S}$.

%%%
\subsubsection{Definition of $A_1$.}
Next, we turn to the definition of $A_1$, starting from the informal 
expression \eqref{eq:a1informal}. We first split up the 
$\delta$-function of the interaction kernel according to 
\eqref{eq:lcdeltasplit}.
Then we use $\delta(y^0-{y'}^0-|\vy-\vy'|)$ to eliminate 
${y'}^0~(=y^0-|\vy-\vy'|)$. Note that the order of these two steps 
does not matter. This yields:
\begin{align}
	&(A_1 \psi)(x,y) =  -\frac{\lambda \, m_1}{2(4 \pi)^3} \int_0^\infty d{x'}^0 \int d^3 \vx' \int d^3 \vy'~H(x^0-{x'}^0-|\vx-\vx'|)  \nonumber\\
& ~~~\times \frac{J_1(m_1\sqrt{(x-x')^2})}{\sqrt{(x-x')^2}} \frac{H(y^0-|\vy-\vy'|)}{|\vy-\vy'|} \frac{1}{|\vx'-\vy'|} \nonumber\\
&~~~\left[\delta({x'}^0 - y^0 \!+ \!|\vy-\vy'| \!-\! |\vx'\!-\!\vy'|)+ \! \delta({x'}^0 \!- y^0\! +\!|\vy-\vy'| \!+\! |\vx'-\vy'|) \right] \notag\\
&\hspace{1cm}\times\psi(x',y^0-|\vy-\vy'|,\vy').\label{eq:a1informal2}
\end{align}
Finally, we use the remaining $\delta$-functions to eliminate 
${x'}^0$. We obtain:
\begin{align}
	(A_1 \psi)(x,y) &=  -\frac{\lambda \, m_1}{2(4 \pi)^3} \int d^3 \vx' \int d^3 \vy'~\frac{H(y^0-|\vy-\vy'|)}{|\vy-\vy'|} \frac{1}{|\vx'-\vy'|} \nonumber\\
&\Bigg[ H({x'}^0)H(x^0-{x'}^0-|\vx-\vx'|)  \notag\\
& \quad \times\frac{J_1(m_1\sqrt{(x-x')^2})}{\sqrt{(x-x')^2}} \psi(x',y')\bigg|_{\substack{{y'}^0 = y^0-|\vy-\vy'|,\\{x'}^0 = y^0\!\!- |\vy-\vy'| \!+\! |\vx'\!-\!\vy'| }}\nonumber\\
&+ \, H({x'}^0)H(x^0-{x'}^0-|\vx-\vx'|)  \notag\\
&\times \frac{J_1(m_1\sqrt{(x-x')^2})}{\sqrt{(x-x')^2}} \psi(x',y')\bigg|_{\substack{{y'}^0 = y^0-|\vy-\vy'|,\\{x'}^0 = y^0\!\!- |\vy-\vy'| \!-\! |\vx'\!-\!\vy'| }} \Bigg] .\label{eq:defa1}
\end{align}
This expression is free of distributions, so it will serve as our 
definition of $A_1$ on test functions $\psi \in \mathcal{S}$. Note 
that the domain of integration is effectively finite due to the 
Heaviside functions.

%%%
\subsubsection{Definition of $A_2$.} Starting from 
\eqref{eq:a2informal}, the analogous steps as for $A_1$ yield:
\begin{align}
	(A_2 \psi&)(x,y) \!=\!  -\frac{\lambda  m_2}{2(4 \pi)^3}\! \int \!d^3 \vx' \!\!\int\! d^3 \vy'\frac{H(x^0-|\vx-\vx'|)}{|\vx-\vx'|} \frac{1}{|\vx'-\vy'|} \nonumber\\
&\Bigg[ H({y'}^0)H(y^0-{y'}^0-|\vy-\vy'|)  \notag\\
&\times \frac{J_1(m_2\sqrt{(y-y')^2})}{\sqrt{(y-y')^2}} \psi(x',y')\bigg|_{\substack{{x'}^0 = x^0-|\vx-\vx'|,\\{y'}^0 = x^0 - |\vx-\vx'| \!+\! |\vx'\!-\!\vy'| }}\nonumber\\
&+ \, H({y'}^0)H(y^0-{y'}^0-|\vy-\vy'|)   \notag\\
&\times \frac{J_1(m_2\sqrt{(y-y')^2})}{\sqrt{(y-y')^2}} \psi(x',y')\bigg|_{\substack{{x'}^0 = x^0-|\vx-\vx'|,\\{y'}^0 = x^0 - |\vx-\vx'| \!-\! |\vx'\!-\!\vy'| }} \Bigg] .\label{eq:defa2}
\end{align}
This serves as our definition of $A_2$ on test functions $\psi \in \mathcal{S}$.

%%%
\subsubsection{Definition of $A_{12}$.} Here, we start with 
\eqref{eq:a12informal}. We change variables $(\vx',\vy') 
\rightarrow (\vx',\vz = \vx'-\vy')$ (Jacobi determinant $=1$), with 
the goal of using the remaining $\delta$-function to eliminate 
$|\vz| = |\vx'-\vy'|$ in mind. We find:
\begin{align}
(A_{12} \psi)(x,y) =&\! \frac{\lambda  m_1 m_2}{(4\pi)^3}  \!\int_0^\infty \!\!d{x'}^0 \!\int \!d^3 \vx'\! \int_0^\infty \!\!d{y'}^0 \!\int \!d^3 \vz H(x^0\!-\!{x'}^0\!\!-\!|\vx-\vx'|)  \nonumber\\
&\times \frac{J_1(m_1\sqrt{(x-x')^2})}{\sqrt{(x-x')^2}} H(y^0-{y'}^0-|\vy-\vx'+\vz|)\nonumber\\
&\times  \frac{J_1(m_2\sqrt{(y-y')^2})}{\sqrt{(y-y')^2}}\delta(({x'}^0\!-{y'}^0)^2 -|\vz|^2) \psi(x',y')\Big|_{\vy' = \vx'-\vz}\label{eq:a12informal2}.
\end{align}
Now we use spherical coordinates for $\vz$ and eliminate $|\vz|$ 
through the $\delta$-function, using
\begin{equation}
	 \delta(({x'}^0-{y'}^0)^2 -|\vz|^2) =  \frac{1}{2 |\vz|} \delta(|{x^0}'-{y^0}'|-|\vz|).
\end{equation}
This yields:
\begin{align}
&(A_{12} \psi)(x,y) = \frac{\lambda \, m_1 m_2}{2(4\pi)^3}  \int_0^\infty d{x'}^0 \int d^3 \vx' \int_0^\infty d{y'}^0 \int_0^{2\pi} d\varphi \int_{0}^{\pi} d \vartheta  \nonumber\\
&\times \cos(\vartheta) |{x'}^0-{y'}^0| H(x^0-{x'}^0-|\vx-\vx'|) \frac{J_1(m_1\sqrt{(x-x')^2})}{\sqrt{(x-x')^2}}\nonumber\\
&\times \!H(y^0\!\!-\!{y'}^0\!\!-|\vy\!-\!\vx'\!+\!\vz|) \frac{J_1(m_2\sqrt{(y\!-\!y')^2})}{\sqrt{(y\!-\!y')^2}} \psi(x'\!,y')\Big|_{\vy' = \vx'-\vz, \, |\vz| = |{x^0}'-{y^0}'|}
\label{eq:defa12}.
\end{align}
The resulting expression does not contain distributions anymore and 
will serve as our definition of $A_{12}$ on test functions $\psi 
\in \mathcal{S}$. Note that the domain of integration is again 
effectively finite.

%%%
\subsubsection{Lifting of the integral operator from test 
functions to a suitable Banach space.}
In order to prove the existence and uniqueness of solutions 
of the integral equation $\psi = \psi^\free + A\psi$, we need 
to define the operator $A$ not only on test functions but on a 
suitable Banach space which includes (at least) sufficiently many 
solutions $\psi^\free$ of the free multi-time Klein-Gordon equations, 
$(\Box_k + m_k^2)\psi^\free(x_1,x_2) = 0,~k=1,2$. 
We shall define this Banach space as the completion of 
$\mathcal{S} =\mathcal{S}((\tfrac{1}{2}\M)^2)$ with respect to a 
suitable norm. A good choice which works well for the upcoming 
existence and uniqueness proofs is the class of weighted 
$L^\infty$-norms
\begin{equation}
	\| \psi \|_g := \esssup_{x,y \in \tfrac{1}{2}\M} \frac{|\psi(x,y)|}{g(x^0)g(y^0)},
\end{equation}
where $g : \R^+_0 \rightarrow \R^+$ is assumed to be a monotonically 
increasing function such that $1/g$ is bounded. Then our Banach space 
is given by the completion
\begin{equation}
	\Banach_g = \overline{\mathcal{S}}^{\| \cdot \|_g}.
\end{equation}
Our next goal is to find a weight function $g$ such that the operator 
$A$ is not only bounded but even defines a contraction on $\Banach_g$. 
By linear extension, it is sufficient to estimate $\| A \psi \|_g$ 
on test functions $\psi \in \mathcal{S}$.

%%%%
\begin{Remarks}
	\begin{enumerate}
    \item We have attempted to use an $L^\infty_t L^2_\vx$-based norm 
    ($L^\infty$ in the times and $L^2$ in the space variables). 
    However, we did not succeed with obtaining suitable estimates 
  for that case. This might not be a problem in principle, but its 
  treatment would require further technical innovation.
More precisely, one would need to understand integral operators such 
as \eqref{eq:defa0} whose kernel is in $L^1$ but not in $L^2$.
		%
    \item Nevertheless, our definition of $\Banach_g$ contains a 
    large class of free solutions of the Klein-Gordon equation. As 
    the Klein-Gordon equation preserves boundedness, all bounded 
    initial data for $\psi^\free$ lead to a free solution $\psi^\free 
    \in \Banach_g$ which can be used as an input to our integral 
    equation.
	\end{enumerate}
\end{Remarks}


%%%%%
\subsection{Results}
\label{sec:results}

This section is structured as follows. Sec.\@ \ref{sec:2part} (which 
is about the two-particle case) contains the main results: the 
estimates of the integral operators as well as the theorems about 
existence and uniqueness of solutions. Sec.\@ \ref{sec:Npart} we 
extend these results to the $N$-particle case and in Sec.\@ 
\ref{sec:curvedspacetime} we show that a curved spacetime with a 
Big Bang singularity can provide a natural reason for a cutoff in 
time.


%%%%%%
\subsubsection{The two-particle case} \label{sec:2part}

For $t\geq0$, we define the functions:
\begin{align}
	g_0(t) &= g(t),\nonumber\\
\text{and for }n\in\N:~~~	g_n(t) &= \int_0^t dt' \, g_{n-1}(t').	
\end{align}
Note that due to the properties of $g$, the functions $g_n$ are 
monotonically increasing for all $n\in \N$; furthermore, by 
definition, they satisfy $g_n(0)=0$.

Our first theorem gives explicit bounds for the operators 
\(A_0\), \(A_1\), \(A_2\), \(A_{12}\) in terms of the functions $g_n$. 
The proof 
can be found in Sec.\@ \ref{sec:proofbounds}.

%%%%%
\begin{Thm}[Bounds of the integral operators on $\mathcal{S}$.]
	\label{thm:bounds}
	For all $\psi \in \mathcal{S}((\tfrac{1}{2}\M)^2)$, the integral operators $A_0, A_1, A_2, A_{12}$ satisfy the following bounds:
	\begin{align}
		\sup_{\psi \in \mathcal{S}((\frac{1}{2}\M)^2)} \frac{\| A_0 \psi \|_g}{\| \psi \|_g} ~\leq~& \frac{\lambda}{8\pi} \left(\sup_{t\geq 0} \frac{g_1(t)}{g(t)}\right)^2\label{eq:estimatea0},\\
%
  \sup_{\psi \in \mathcal{S}((\frac{1}{2}\M)^2)} \frac{\| A_1 \psi \|_g}{\| \psi \|_g} ~\leq~& \frac{\lambda \, m_1^2}{16\pi} \bigg[ 3\left(\sup_{t\geq 0}\frac{t g_1(t)}{g(t)}\right)\left(\sup_{t\geq 0}\frac{g_2(t)}{g(t)}\right) \notag\\
  &\quad+ 3\left( \sup_{t\geq 0} \frac{g_1(t)}{g(t)} \right)\left( \sup_{t\geq 0} \frac{tg_2(t)}{g(t)} \right) \nonumber\\
	&\quad\left. +\, 2 \left( \sup_{t\geq 0} \frac{g_1(t)}{g(t)} \right)\left( \sup_{t\geq 0} \frac{g_3(t)}{g(t)} \right) \right], \label{eq:estimatea1}\\
%
  \sup_{\psi \in \mathcal{S}((\frac{1}{2}\M)^2)} \frac{\| A_2 \psi \|_g}{\| \psi \|_g} ~\leq~& \frac{\lambda \, m_2^2}{16\pi} \bigg[ 3\left(\sup_{t\geq 0}\frac{t g_1(t)}{g(t)}\right)\left(\sup_{t\geq 0}\frac{g_2(t)}{g(t)}\right) \notag\\
  &\quad+ 3\left( \sup_{t\geq 0} \frac{g_1(t)}{g(t)} \right)\left( \sup_{t\geq 0} \frac{tg_2(t)}{g(t)} \right) \nonumber\\
	&\quad\left. + \, 2 \left( \sup_{t\geq 0} \frac{g_1(t)}{g(t)} \right)\left( \sup_{t\geq 0} \frac{g_3(t)}{g(t)} \right) \right], \label{eq:estimatea2}\\
	\sup_{\psi \in \mathcal{S}((\frac{1}{2}\M)^2)} \frac{\| A_{12} \psi \|_g}{\| \psi \|_g} ~\leq~& \frac{\lambda \, m_1^2 m_2^2}{96\pi} \left[ \left( \sup_{t\geq 0} \frac{t^2 g_2(t)}{g(t)}\right)\left( \sup_{t\geq 0} \frac{t g_1(t)}{g(t)}\right) \right.\nonumber\\
&~~~~~\left.+ \, \frac{1}{2}  \left( \sup_{t\geq 0} \frac{t^2 g_3(t)}{g(t)}\right)\left( \sup_{t\geq 0} \frac{g_1(t)}{g(t)}\right) \right].\label{eq:estimatea12}
	\end{align}
\end{Thm}
In case these expressions are finite, $A_0, A_1, A_2, A_{12}$ 
extend to linear operators on $\Banach_g$ with the same norms. Our 
next task is to find suitable weight functions $g$ such that this 
is actually the case. We begin with the massless case where already 
an exponential weight function leads to an estimate which remains 
finite after taking the supremum. The massive case is treated 
subsequently; it is a little more difficult as all the estimates 
for the operators $A_0, A_1, A_2, A_{12}$ have to be finite at 
the same time. This  requires a different choice of weight 
function (see Thm.\@ \ref{thm:existence}).


%%%%%
\begin{Thm}[Bounds for $A_0$ and $g(t) = e^{\gamma t}$; existence 
  of massless dynamics.]
	\label{thm:exponentialg}
~\\ For any $\gamma> 0$, let $g(t) = e^{\gamma t}$. Then $A_0$ 
can be linearly extended to a bounded operator on $\Banach_g$ 
with norm
	\begin{equation}
		\| A_0 \| ~\leq~ \frac{\lambda}{8\pi \gamma^2}.
	\label{eq:norma0exponential}
	\end{equation}
Consequently, for all $\gamma > \sqrt{\frac{\lambda}{8\pi}}$, the 
integral equation $\psi = \psi^\free + A_0\psi$ has a unique 
solution $\psi \in \Banach_g$ for every $\psi^\free \in \Banach_g$.
\end{Thm}

Now we come to our main result.

%%%%%
\begin{Thm}[Existence of dynamics in the massive case.]
	\label{thm:existence}~\\
	For any $\alpha > 0$, let
	\begin{equation}
		g(t) = (1+\alpha t^2)e^{\alpha t^2/2}.
	\label{eq:weightfactor}
	\end{equation}
	 Then $A_0, A_1, A_2$ and $A_{12}$ can be linearly extended to bounded operators on $\Banach_g$ with norms
	\begin{align}
		\| A_0 \| ~&\leq~ \frac{\lambda}{32 \pi} \frac{1}{\alpha}, \label{eq:estimatea0final}\\
		\| A_1 \| ~&\leq~ \frac{5\lambda \, m_1^2}{16\pi} \frac{1}{\alpha^2},\label{eq:estimatea1final}\\
		\| A_2 \| ~&\leq~ \frac{5\lambda \, m_2^2}{16\pi} \frac{1}{\alpha^2}, \label{eq:estimatea2final}\\
		\| A_{12} \| ~&\leq~ \frac{\lambda \, m_1^2 m_2^2}{80\pi} \frac{1}{\alpha^3}. \label{eq:estimatea12final}
	\end{align}
	Consequently, for all $\alpha > 0$ with
\begin{equation}
	\frac{\lambda}{8\pi \alpha} \left( \frac{1}{4} + \frac{5(m_1^2 + m_2^2)}{2} \frac{1}{\alpha} + \frac{m_1^2 \, m_2^2}{10} \frac{1}{\alpha^2} \right) ~<~ 1,
\label{eq:condexistencemassive}
\end{equation}
	the integral equation $\psi = \psi^\free + A\psi$ has a unique solution $\psi \in \Banach_g$ for every $\psi^\free \in \Banach_g$.
\end{Thm}

The proof can be found in Sec.\@ \ref{sec:proofexistence}.

%%%
\begin{Remarks}
	\begin{enumerate}
    \item \textit{Comparison of Thms. 
    \ref{thm:exponentialg} and \ref{thm:existence} in 
    the massless case.} On the first glance, the result 
    of Thm.\@ \ref{thm:exponentialg} looks stronger in 
    the sense that for $g(t)=e^{\gamma t}$, the estimate 
    of $\|A_0\|$ goes with $\gamma^{-2}$ while for 
    $g(t)=(1+\alpha t^2 )e^{\alpha t^2/2}$, the 
    estimate of $\|A_0\|$ goes with $\alpha^{-1}$. 
    However, one should note that $\gamma$ is the 
    constant in front of $t$ while $\alpha$ occurs in 
    combination with $t^2$. Thus, if one wants to draw 
    a comparison between these different cases at all, 
    then it should be between $\gamma$ and 
    $\sqrt{\alpha}$. Of course, the main difference 
    between the two theorems is the admitted growth 
    rate of the solutions. In this regard, Thm.\@ 
    \ref{thm:exponentialg} contains the stronger 
    statement.
	%
    \item A \textit{physically realistic value of 
    $\lambda$} is $\frac{1}{137}$, the value of the 
    fine structure constant. In that case, $\alpha$ 
    need not even be particularly large in order for 
    condition \eqref{eq:condexistencemassive} to be 
    satisfied.
	%
    \item \textit{Initial value problem.} By the 
    integral equation \eqref{eq:inteq}, we obtain that 
    the solution $\psi$ satisfies $\psi(0,\vx,0,\vy) = 
    \psi^\free(0,\vx,0,\vy)$. If $\psi^\free$ is a 
    solution of the free multi-time Klein-Gordon 
    equations, then it is itself determined by initial 
    data at $x_1^0,x_2^0=0$. (As the Klein-Gordon 
    equation is of second order in time, these initial 
    data include data for $\partial_{x^0}\psi$, 
    $\partial_{y^0}\psi$ and $\partial_{x^0} 
    \partial_{y^0} \psi$, see 
    \cite[chap. 5]{phd_nickel}.) Thus, we find that 
    $\psi$ is determined by these data at 
    $x_1^0,x_2^0=0$ as well. Note that for later 
    times, $\psi$ and $\psi^\free$ do not, in general, 
    coincide and consequently a similar statement does 
    not hold.
	%
  \item \textit{Finite propagation speed.} The theorem 
  implies that \linebreak 
  $\psi = \sum_{k=0}^\infty A^k\psi^\free$. 
  As $(A \psi^\free)(x,y)$ involves only values of 
  $\psi^\free$ in $\past(x) \times \past(y)$ where 
  $\past(x)$ denotes the causal past of $x \in 
  \frac{1}{2}\M$ (see Eqs.\@ \eqref{eq:defa0}, 
  \eqref{eq:defa1}, \eqref{eq:defa2}, 
  \eqref{eq:defa12}), so do $A^k \psi^\free$ for 
  all $k \in \N$ and $\psi$. Therefore, we obtain: 
  if the initial data for $\psi^\free$ at $x^0 = 0 
  = y^0$ are compactly supported in a region 
  $R \subset \left( \{ 0\} \times \R^3\right)^2$, 
  then for all Cauchy surfaces $\Sigma \subset 
  \frac{1}{2}\M$, $\psi|_{\Sigma \times \Sigma}$ is 
  supported in the causally grown set  
  $\text{Gr}(R,\Sigma) = \left(\bigcup_{(x,y)\in R} 
  \future(x) \times \future(y) \right) \cap 
  (\Sigma \times \Sigma)$ where $\future(x)$ stands 
  for the causal future of $x \in \frac{1}{2}\M$.
%
  \item \textit{Square integrable solutions.} As a 
  consequence of the previous item, compactly supported 
  and bounded initial data for $\psi^\free$ lead to a 
  compactly supported and bounded solution $\psi$. In 
  particular, this implies that $\psi(x^0,\cdot,y^0)$ 
  lies in $L^2(\R^6)$ for all times $x^0,y^0\geq 0$.
	\end{enumerate}
\end{Remarks}


%%%%%
\subsubsection{The $N$-particle case} \label{sec:Npart}

Here we extend Thm.\@ \ref{thm:existence} from two to 
$N\geq 3$ scalar particles. While there are different 
possibilities to generalize the two-particle integral 
equation \eqref{eq:inteq}, we focus on the one 
advocated in \cite{direct_interaction_quantum} as the 
most promising. For
\begin{equation}
	\psi : \big(\tfrac{1}{2}\M\big)^N \rightarrow \CC,~~~~~(x_1,...,x_N) \mapsto \psi(x_1,...,x_N)
\end{equation}
we consider the integral equation
\begin{align}\label{eq:npartint}
  \psi(x_1,...,x_N) &~=~ \psi^\free(x_1,...,x_N) +\frac{ \lambda}{4\pi} \sum_{i,j =1,...,N; \, i<j}\\
&\times \int_{\tfrac{1}{2}\M} d^4 x_i \int_{\tfrac{1}{2}\M} d^4 x_j~G^\ret_i(x_i-x_i')G^\ret(x_j-x_j') \nonumber\\
&\times \delta((x_i'-x_j')^2) \psi(x_1,...,x_i, ...,x_j,..., x_N).\notag
\end{align}
Here, $\psi^\free$ is again a solution of the free 
Klein-Gordon equations $(\Box_k + m_k^2)\phi(x_k)$ in 
each spacetime variable and $G^\ret_k$ stands for the 
retarded Green's function of the operator 
$(\Box_k + m_k^2)$, $k=1,2,...,N$.

Eq.\@ \eqref{eq:npartint} is written down in an 
informal way. To define a rigorous version, let 
$\psi \in \mathcal{S}\big( (\tfrac{1}{2}\M)^N\big)$ 
be a test function. Moreover, let $A^{(ij)}$ be the 
integral operator of the two-particle problem acting 
on the variables $x_i$ and $x_j$ instead of $x = x_1$ 
and $y=x_2$. We define
\begin{equation}
	\,^{(N)}\! A ~=~ \sum_{i,j =1,...,N; \, i<j} A^{(ij)}.
\end{equation}
As will be shown below, $\! \,^{(N)}\! A$ can be 
linearly extended to a bounded operator on the Banach 
space $\! \,^{(N)}\!\Banach_g$. That space is defined 
as the completion of $\mathcal{S}\big( 
  (\tfrac{1}{2}\M)^N\big)$ with respect to the norm
\begin{equation}
	\|\psi \|_g ~=~ \esssup_{x_1,...,x_N \in \frac{1}{2}\M} \frac{|\psi|(x_1,...,x_N)}{g(x_1^0)\cdots g(x_N^0)},
\end{equation}
where the function $g$ is defined as before.

Then we take the equation
\begin{equation}
	\,^{(N)}\! A ~=~\psi^\free + \! \,^{(N)}\! A \psi.
\label{eq:npartintabstract}
\end{equation}
to be the rigorous version of \eqref{eq:npartint} on 
$\! \,^{(N)}\!\Banach_g$.

With these preparations, we are ready to formulate 
the $N$-particle existence and uniqueness theorem.

%%%%%
\begin{Thm}[Existence of dynamics for $N$ particles.]
	\label{thm:Npart}~\\
		For any $\alpha > 0$, let $g(t) = (1+\alpha t^2)e^{\alpha t^2/2}$. Then the operator $\! \,^{(N)}\! A$ can be linearly extended to a bounded operator on $\! \,^{(N)}\!\Banach_g$ with norm
	\begin{equation}
    \|\! \,^{(N)}\! A\| \leq\frac{\lambda}{8\pi \alpha}\! \sum_{i,j =1,...,N; \, i<j} \! 
    \left( \frac{1}{4} \!+\! \frac{5(m_i^2 + m_j^2)}{2} \frac{1}{\alpha} 
    \!+ \!\frac{m_i^2 \, m_j^2}{10} \frac{1}{\alpha^2} \right).
	\end{equation}
  If $\alpha > 0$ is such that this expression is 
  strictly smaller than one, the integral equation 
  \eqref{eq:npartintabstract} has a unique solution 
  $\psi \in \! \,^{(N)}\!\Banach_g$ for every 
  $\psi^\free \in \! \,^{(N)}\!\Banach_g$.
\end{Thm}

The proof follows straightforwardly from that of 
Thm.\@ \ref{thm:existence} using
\begin{equation}
	\|\! \,^{(N)}\! A\| \leq \sum_{i,j =1,...,N; \, i<j} \| A^{(ij)}\|_g.
\end{equation}
For the norms of the operators $A^{(ij)}$, one can 
use the previous expressions as these operators act 
only as the identity on variables $x_k$ with 
$k \notin \{i,j\}$.

%%%
\begin{Remark}
  To the best of our knowledge, Thm.\@ \ref{thm:Npart} 
  is the first result about the existence and 
  uniqueness of solutions of multi-time integral 
  equations for $N$ particles. While for the present 
  contraction argument the generalization to $N$ 
  particles has been straightforward, this is not 
  the case for other works. For example, the Volterra 
  iterations used in \cite{mtve} become increasingly 
  complicated with increasing particle number $N$. For 
  Dirac particles,  a similar technique as ours was 
  used in \cite{selfDirac}. However, as the Dirac 
  Green's functions contain distributional derivatives, 
  one has to control weak derivatives of the solutions, 
  and the number of such derivatives depends on $N$. 
  That situation also does not allow for such a 
  straightforward generalization to $N$ particles as 
  has been possible here.
\end{Remark}


%%%%%
\subsubsection{On the possible origin of a cutoff in 
time}
\label{sec:curvedspacetime}

So far, we have assumed a cutoff in time. In the way 
this has been treated so far, this cutoff breaks the 
manifest Poincar\'e invariance of our integral 
equation. In this section, we demonstrate at a 
particular (simple and tractable) example that such a 
cutoff can arise naturally if the considered spacetime 
has a Big Bang singularity. Then the Big Bang defines 
the initial time. To consider a simple example is 
necessary as otherwise the Green's functions may not 
be known in detail, and in that case it would not be 
possible to explicitly define the integral operator, 
let alone to carry out an analysis of that operator 
comparable to the one before.

Our example consists of two massless scalar particles 
which, in absence of interactions, obey the conformally 
invariant wave equation on a curved spacetime 
$\mathcal{M}$ with metric g,
\begin{equation}
	\left( \Box_g - \xi R \right) \chi = 0,
	\label{eq:conformalwaveeq}
\end{equation}
where $R$ denotes the Ricci scalar and in 1+3 
dimensions $\xi = \frac{1}{6}$.

We consider these particles on a flat 
Friedman-Lema\^itre-Robertson-Walker (FLRW) spacetime 
which is described by the metric
\begin{equation}
	ds^2 = a^2(\eta) \left( d\eta^2 - dr^2 - r^2 d \Omega^2 \right),
\end{equation}
where $\eta$ denotes conformal time, $d \Omega$ 
denotes the surface measure on $\mathbb{S}^2$ and 
$a(\eta)$ is the so-called \textit{scale function}, a 
continuous function with $a(0) = 0$ and $a(\eta) > 0$ 
for $\eta >0$. This form makes it obvious that the 
spacetime is conformally equivalent to a Minkowski 
half space $\tfrac{1}{2}\M$, with conformal factor 
$a(\eta)$.

In this case, it is well-known that the Green's 
functions of \eqref{eq:conformalwaveeq} on the flat 
FLRW spacetime $\mathcal{M}$ can be obtained from 
those of the usual wave equation on $\tfrac{1}{2}\M$ 
as follows (using coordinates $x=(\eta,\vx)$ and 
$x'=(\eta',\vx')$ with $\eta,\eta' \in [0,\infty)$ 
and $\vx,\vx' \in \R^3$; see \cite{lienertcurved} for 
a more detailed explanation):
\begin{equation}
	G_{\mathcal{M}}(x,x') ~=~ \frac{1}{a(\eta)} \frac{1}{a(\eta')} G_{\frac{1}{2}\M}(x,x').
\end{equation}
Inserting the well-known expression for the retarded 
and symmetric Green's functions on $\tfrac{1}{2}\M$ 
(see \eqref{eq:gretkg}) yields:
\begin{align}
	G_{\mathcal{M}}^\ret(x,x') ~&=~ \frac{1}{4\pi} \frac{1}{a(\eta) a(\eta')} \frac{\delta(\eta - \eta' -|\vx-\vx'|)}{|\vx-\vx'|}\nonumber\\
G_{\mathcal{M}}^\sym(x,x') ~&=~ \frac{1}{4\pi} \frac{1}{a(\eta) a(\eta')} \delta((\eta-\eta')^2-|\vx-\vx'|^2).
\end{align}

With this information, we are ready to write down 
the integral equation on $\mathcal{M}$. The 
generalization of \eqref{eq:inteq} to curved 
spacetimes is straightforward: $\psi$ becomes a 
scalar function on $\mathcal{M}\times\mathcal{M}$, 
one exchanges the Minkowski spacetime volume elements 
with the invariant 4-volume elements on $\mathcal{M}$, 
and the Green's functions on $\tfrac{1}{2}\M$ get 
replaced with those on $\mathcal{M}$ as well. As in 
the Minkowski case, the interaction kernel is given 
by the symmetric Green's function. With this, the 
relevant integral equation becomes:
\begin{align}\notag
\psi(x,y) = \psi^\free(x,y) 
+ \lambda \int_{\mathcal{M}\times \mathcal{M}} \!\!
&dV(x) dV(y)~G_1^\ret(x,x')G_2^\ret(y,y')\\
&\times  G^\sym(x',y') \psi(x',y'),
\label{eq:inteqcurved}
\end{align}
For regular and only weakly singular interaction 
kernels $K(x,y)$ instead of $G^\sym(x',y')$, the 
problem of existence and uniqueness of solutions of 
this equation has been treated in \cite{lienertcurved} 
for flat, open and closed FLRW universes; the case of 
Dirac particles and smooth interaction kernels has 
been addressed in \cite{selfDirac}. For flat FLRW 
universes and scalar particles, we here extend 
\cite{lienertcurved} to the physically most interesting 
and mathematically challenging case 
$K(x,y)=G^\sym(x,y)$.

We now formulate \eqref{eq:inteqcurved} explicitly. 
The spacetime volume element is given by:
\begin{equation}
	dV(x) = a^4(\eta) \, d\eta \, d^3 \vx.
\end{equation}
With this information, \eqref{eq:inteqcurved} becomes:
\begin{align}
  \psi(\eta_1,&\vx_1,\eta_2,\vx_2) = \psi^\free(\eta_1,\vx_1,\eta_2,\vx_2) + \frac{\lambda}{(4\pi)^3} \frac{1}{a(\eta_1) a(\eta_2)} \notag\\
&\int_0^{\eta_1} d \eta_1' \int d^3 \vx_1' \int_0^{\eta_2} d \eta_2' \int d^3  \vx_2' a^2(\eta_1') a^2(\eta_2')\nonumber\\
&\times ~\frac{\delta(\eta_1-\eta_1' - |\vx_1-\vx_1'|)}{|\vx_1-\vx_1'|}\frac{\delta(\eta_2-\eta_2' - |\vx_2-\vx_2'|)}{|\vx_2-\vx_2'|}\nonumber\\
&\times ~\delta((\eta_1'-\eta_2')^2-|\vx_1'-\vx_2'|^2) \psi(\eta_1',\vx_1',\eta_2',\vx_2').
\label{eq:inteqcurvedexplicit}
\end{align}
Now let
\begin{equation}
	\chi(\eta_1,\vx_1,\eta_2) = a(\eta_1) a(\eta_2) \psi(\eta_1,\vx_1,\eta_2).
\end{equation}
and $\chi^\free(\eta_1,\vx_1,\eta_2) = a(\eta_1) a(\eta_2) \psi^\free(\eta_1,\vx_1,\eta_2)$.
Then \eqref{eq:inteqcurvedexplicit} is equivalent to:
\begin{align}\label{eq:inteqcurvedexplicitchi}
	\chi(\eta_1,\vx_1&,\eta_2,\vx_2)\! =\! \chi^\free(\eta_1,\vx_1,\eta_2,\vx_2) + \frac{\lambda}{(4\pi)^3} \int_0^{\eta_1} d \eta_1' \int d^3 \vx_1' \\
&\times \int_0^{\eta_2} \!\!d \eta_2' \int d^3\!  \vx_2' \frac{\delta(\eta_1\!-\!\eta_1'\! -\! |\vx_1\!-\!\vx_1'|)}{|\vx_1\!-\!\vx_1'|}\frac{\delta(\eta_2\!-\!\eta_2' \!-\! |\vx_2\!-\!\vx_2'|)}{|\vx_2\!-\!\vx_2'|}\nonumber\\
&\times  a(\eta_1') a(\eta_2') \delta((\eta_1'\!-\!\eta_2')^2-|\vx_1'-\vx_2'|^2) \chi(\eta_1',\vx_1',\eta_2',\vx_2').\notag
\end{align}
We can see that this equation has almost exactly the 
same form as the massless version of \eqref{eq:inteq} 
on $\tfrac{1}{2}\M$ (see \eqref{eq:a0informal}). The 
only difference is the additional appearance of the 
factor $a(\eta_1') a(\eta_2')$ inside the integrals.

Going through the same steps as for \eqref{eq:defa0} 
before, \eqref{eq:inteqcurvedexplicitchi} can be 
defined on test functions $\chi \in \mathcal{S}$ by
\begin{equation}
	\chi ~=~ \chi^\free + \widetilde{A}_0 \chi,
\label{eq:inteqcurvedabstract}
\end{equation}
where $\widetilde{A}_0$ is defined by (using 
coordinates $x=(\eta_1,\vx)$, $y=(\eta_2,\vy)$):
\begin{align}\nonumber
    &(\widetilde{A}_0\psi)(x,y)\!=\!\frac{\lambda}{(4\pi)^3}\int_{B_{y^0}(\vy)}\!\!\!d^3\vy'  \!\int_0^{2\pi}\!\!d\varphi \int_{-1}^{1} \!\!d\!\cos\vartheta ~\frac{|b^2|}{4(b^0+|\vb|\cos\vartheta)^2 |\vy'|}  \\
    &\times a(\eta_1+\eta_1') a(\eta_2+\eta_2') \psi(x+x',y+y')\notag\\ 
    &\times \left(1_{b^2>0}1_{b^0>0} 1_{\cos\vartheta > \frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|}}+1_{b^2<0}1_{\cos\vartheta<\frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|}}\right),
\label{eq:defa0tilde}
\end{align}
with \(\eta_1'=-r^*=-|\vx'| , \eta_2'=-|\vy'|\). 
(Here, $b$ and $r^*$ are defined as in \eqref{eq:b} 
and \eqref{eq:r}, respectively).

Knowing precisely how our integral equation on the 
flat FLRW spacetime is to be understood, we can 
formulate the respective existence and uniqueness 
theorem:

%%%%%
\begin{Thm}[Existence of dynamics for an open FLRW 
  universe.]
	\label{thm:existencecurved}~\\
  Let $a: [0,\infty) \rightarrow [0,\infty)$ be a 
  continuous function with $a(0)=0$ and $a(\eta)>0$ 
  for $\eta>0$. Moreover, let
	\begin{equation}
		g(t) = \exp \left( \gamma \int_0^t d\tau \, a(\tau) \right).
	\end{equation}
  Then, the operator $\widetilde{A}_0$ satisfies the 
  following estimate:
	\begin{equation}
		\sup_{\chi \in \mathcal{S}\big( ([0,\infty)\times \R^3)^2\big)} \frac{\| \widetilde{A}_0 \chi \|_g}{\| \chi \|_g} ~\leq~ \frac{\lambda}{8\pi \gamma^2}.
	\label{eq:a0tildebound}
	\end{equation}
  $\widetilde{A}_0$ can be extended to a linear 
  operator on $\Banach_g$ which satisfies the same 
  bound. Moreover, for $\gamma < 
  \sqrt{\frac{\lambda}{8\pi}}$, the equation 
  $\chi = \chi^\free + \widetilde{A}_0\chi$ has a 
  unique solution $\chi \in \Banach_g$ for every 
  $\psi^\free \in \Banach_g$.
\end{Thm}

The proof can be found in Sec.\@ 
\ref{sec:proofexistencecurved}.

\begin{Remarks}
	\begin{enumerate}
    \item \textit{Manifest covariance.} The theorem 
    shows the existence and uniqueness of solutions 
    of the manifestly covariant integral equation 
    \eqref{eq:inteqcurved}. Our example of a particular 
    FLRW spacetime thus achieves its goal of 
    demonstrating that a cutoff in time can arise 
    naturally in a cosmological context.
		%
    \item \textit{Initial value problem.} As in the 
    case of $\tfrac{1}{2}\M$, the solution $\chi$ 
    satisfies $\chi(0,\vx,0,\vy) = 
    \chi^\free(0,\vx,0,\vy)$ where $\chi^\free$ is 
    determined by the solution $\psi^\free$ of the 
    free conformal wave equation 
    \eqref{eq:conformalwaveeq} in both spacetime 
    variables. Since $\psi^\free$ is determined by 
    initial data at $\eta_1 = 0 =\eta_2$, so are 
    $\chi^\free$ and $\chi$.
		%
    \item \textit{Behavior of $\psi$ towards the Big 
    Bang singularity}. While the transformed wave 
    function $\chi$ remains bounded for $\eta_1, 
    \eta_2 \rightarrow 0$, the physical wave function 
    $\psi(\eta_1,\vx,\eta_2,\vy) = 
    \frac{1}{a(\eta_1)a(\eta_2)} 
    \chi(\eta_1,\vx,\eta_2,\vy)$ diverges like 
    $\frac{1}{a(\eta_1)a(\eta_2)}$. This is to be 
    expected, as the Klein-Gordon equation has a 
    preserved "energy" (given by a certain spatial 
    integral) and as the volume in $\vx, \vy$ 
    contracts to zero towards the Big Bang.
		%
    \item \textit{$N$-particle generalization.} As 
    shown in Sec.\@ \ref{sec:Npart} for the Minkowski 
    half-space, it would also be easy to extend 
    Thm.\@ \ref{thm:existencecurved} to $N$ particles. 
    To avoid duplication, we do not carry this out 
    explicitly for the curved spacetime example here.
	\end{enumerate}
\end{Remarks}


%%%%%
\subsection{Proofs}
\label{sec:proofs}

%%%%%
\subsubsection{Proof of Theorem \ref{thm:bounds}} 
\label{sec:proofbounds}
The proof is divided into the proofs of the estimates 
\eqref{eq:estimatea0}, \eqref{eq:estimatea1}, 
\eqref{eq:estimatea2} and \eqref{eq:estimatea12}, 
respectively. Here, \eqref{eq:estimatea0} is the most 
singular and difficult term which deserves the greatest 
attention.

Throughout this subsection, let $\psi \in 
\mathcal{S}((\tfrac{1}{2}\M)^2)$.

%%%
\paragraph{Estimate of the massless term 
\eqref{eq:estimatea0}.} \label{sec:estimatemassless}

We start with Eq.\@ \eqref{eq:defa0} and take the 
absolute value. Using, in addition, that
\begin{equation}
	|\psi(x,y)| ~\leq~ \| \psi \|_g \, g(x^0) g(y^0)
\end{equation}
leads us to:
\begin{align}
    &|A_0\psi|(x,y) \!\le\! \frac{\lambda \|\psi\|_g}{4(4\pi)^3} \!\int_{B_{y^0}(\vy)}\!\!\!d^3 \vy' \int_0^{2\pi} \!\!d\varphi \int_{-1}^1 \!\!\!d\cos\vartheta \, \frac{|b^2|}{(b^0\!+\!|\vb|\cos\vartheta)^2 |\vy'|} \nonumber\\
    &\times g(y^0\!-\!|\vy'|) g\!\left(x^0-\frac{1}{2}\frac{b^2}{b^2+|\vb|\cos\vartheta}\right)\notag\\
    &\times \left(1_{b^2>0}1_{b^0>0} 1_{\cos\vartheta > \frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|}}+1_{b^2<0}1_{\cos\vartheta<\frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|}}\right).
\label{eq:a0calc01}
\end{align}
Next, we observe that the fraction 
$\frac{|b^2|}{(b^0+|\vb|\cos\vartheta)^2 |\vy'|}$ is 
the derivative of the fraction which occurs in the 
argument of the second $g$-function. Introducing 
$u = \cos \vartheta$ allows us to rewrite 
\eqref{eq:a0calc01} as
\begin{align}
     &\eqref{eq:a0calc01} =\frac{\lambda \|\psi\|_g}{8(4\pi)^2} \int_{B_{y^0}(\vy)}\!\!\!d^3\vy' \int_{-1}^1\!du  2 \sgn(b^2)\,  \notag\\
    &\quad \times \partial_u g_1\!\left(x^0-\frac{1}{2}\frac{b^2}{b^0+|\vb|u}\right) g(y^0-|\vy'|) \frac{1}{|\vb||\vy'|}\\\nonumber
    &\quad \times \left(1_{b^2>0}1_{b^0>0} 1_{u > \frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|}}+1_{b^2<0}1_{u<\frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|}}\right) \\
    &=\frac{\lambda \|\psi\|_g}{4(4\pi)^2} \int_{B_{y^0}(\vy)}d^3\vy' \int_{-1}^1 du  ~ \partial_u g_1\left(x^0-\frac{1}{2}\frac{b^2}{b^0+|\vb|u}\right) g(y^0-|\vy'|)\nonumber\\
    &\quad \times   \left(1_{b^2>0}1_{b^0>0} 1_{u > \frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|}}-1_{b^2<0}1_{u<\frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|}}\right) \frac{1}{|\vb||\vy'|}.
\label{eq:a0calc02}
\end{align}
This form allows for a direct integration with respect 
to $u$.
Before we integrate, we check whether the conditions 
implicit in the characteristic functions can always be 
satisfied. (Otherwise, the respective term would not 
contribute any further and we could drop it.) Recall 
that $	b = x-y-(-|\vy'|, \vy')$. First we check 
whether in the case \(b^2>0, b^0>0\) it is true that 
\( 1> \frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|}\) 
holds. (The comparison with 1 is due to the upper 
range for $u$.) We compute
\begin{align}
    1> \frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|} &\iff  2 x^0 |\vb|+2 x^0 b^0 > b^2 \nonumber\\
    &\iff 2 x^0(b^0 + |\vb|) > (b^0+|\vb|)(b^0-|\vb|)\nonumber\\
   &\!\!\!\! \overset{b^2>0,b^0>0}{\iff} 2x^0>b^0-|\vb| \nonumber \\
   &\iff  x^0+y^0\!\!- |\vy'| > -|\vb|.
\end{align}
Now because of \(|\vy'|<y^0\) we see that this 
inequality always holds true. Hence the respective 
term in \eqref{eq:a0calc02} contributes without 
further restrictions.

Next, we turn to the case \(b^2<0\). Here we check 
whether (or when) $-1<\frac{b^2}{2x^0|\vb|} - 
\frac{b^0}{|\vb|}$ holds. (The comparison with $-1$ 
is due to the lower bound for $u$.) A similar 
calculation yields 
\begin{align}
    -1<\frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|} &\iff -2x^0|\vb| + 2x^0 |\vb| < b^2\\
    &\iff 2x^0 (b^0-|\vb|) < (b^0-|\vb|)(b^0+|\vb|) \\
   & \overset{b^2<0}{\iff} 2x^0 > b^0 + |\vb|.
\end{align}
This inequality need not always hold, as we can 
increase \(|\vb|\) with respect to \(b^0\) as much as 
we like, e.g., by picking \(|\vx-\vy|\) large. 
Therefore, in this case, the respective term is only 
sometimes nonzero. We make this clear by including 
the characteristic function $1_{2x^0>b^0+|\vb|}$.

Taking these considerations into account, we now carry 
out the $u$-integration in \eqref{eq:a0calc02}:
\begin{align}
    &|A_0\psi|(x,y) ~\le~ \frac{\lambda \|\psi\|_g}{4(4\pi)^2} \int_{B_{y^0}(\vy)}d^3\vy'~ \frac{g(y^0-|\vy'|)}{|\vb||\vy'|} \notag\\
    &\times \Bigg(1_{b^2>0,b^0>0} \Bigg[g_1\left(x^0-\frac{1}{2} \frac{b^2}{b^0+|\vb|}\right) \notag\\
    &\quad \quad - g_1\left( x^0 - \frac{1}{2}\frac{b^2}{b^0+|\vb| \max (-1, \frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|})}\right)\Bigg]\notag\\
    & -1_{b^2<0}1_{2x^0>b^0+|\vb|}\Bigg[
    g_1\left(x^0-\frac{1}{2} \frac{b^2}{b^0+|\vb| \min(1, \frac{b^2}{2x^0|\vb|}-\frac{b^0}{|\vb|})}\right)\notag\\
    &\quad \quad - g_1\left(x^0-\frac{1}{2} \frac{b^2}{b^0-|\vb|} \right)\Bigg]\Bigg).
\label{eq:a0calc03}
\end{align}
The minima and maxima in this expression result from 
the indicator functions $1_{u > \frac{b^2}{2x^0|\vb|} 
- \frac{b^0}{|\vb|}}$ and $1_{u<\frac{b^2}{2x^0|\vb|} 
- \frac{b^0}{|\vb|}}$, respectively.

Our next step is to simplify the complicated fractions 
in \eqref{eq:a0calc03} involving $\min$ and $\max$. 
For the first one we use that 
\(1/\max(a,b)=\min(1/a,1/b)\) whenever \(a,b>0\) or 
\(a,b<0\) holds. Therefore, we have:
\begin{align}
    &\frac{1}{2}\frac{b^2}{b^0+|\vb|\max\left( -1,\frac{b^2}{2x^0|\vb|} - \frac{b^0}{|\vb|}\right)}
	= \frac{1}{2} \frac{b^2}{\max\left(b^0-|\vb|,\frac{b^2}{2x^0}\right)}\nonumber\\
    &= \frac{1}{2} \min \left(\frac{b^2}{b^0-|\vb|}, 2x^0 \right)\nonumber
    = \min \left( \frac{b^0+|\vb|}{2}, x^0 \right).
\end{align}
The fraction in \eqref{eq:a0calc03} which contains a 
minimum can be simplified by observing that
\begin{align}
    b^0+|\vb|\min\left(1,\frac{b^2}{2x^0|\vb|}-\frac{b^0}{|\vb|}\right) = \min \left( b^0+|\vb|, \frac{b^2}{2x^0} \right)= \frac{b^2}{2x^0}
\end{align}
as the term contributes only for \(b^2<0\) and 
$2x^0>b^0+|\vb|$ (note that then $\frac{b^2}{2x^0} 
< \frac{(b^0)^2-|\vb|^2}{b^0 + |\vb|} = b^0 - |\vb| 
< b^0 + |\vb|$). Thus,
\begin{equation}
	\frac{1}{2} \frac{b^2}{b^0+|\vb| \min(1, \frac{b^2}{2x^0|\vb|}-\frac{b^0}{|\vb|})} = x^0.
\end{equation}
With these simplifications, we obtain (using 
$g_1(0)=0$):
\begin{align}
    &|A_0\psi|(x,y) \le  \frac{\lambda \|\psi\|_g}{4(4\pi)^2} \int_{B_{y^0}(\vy)}d^3\vy'~ \frac{g(y^0-|\vy'|)}{|\vb||\vy'|} \nonumber\\
    &\times \!\left(\!1_{b^2>0,b^0>0} \left[g_1\!\left(\!x^0\!-\frac{b^0-|\vb|}{2} \!\right)\! -\! g_1\!\left(\! x^0 \!- \!\min\!\left(\frac{b^0+|\vb|}{2},x^0\!\right)\!\right)\!\right]\right.\nonumber\\
    &\left. -1_{b^2<0}~1_{2x^0>b^0+|\vb|}\left[
    g_1\left(x^0-x^0\right)- g_1\left(x^0-\frac{b^0+|\vb|}{2} \right)\right]\right)\nonumber\\\label{y'Integral1}
    &=\!\frac{\lambda \|\psi\|_g}{4(4\pi)^2} \!\int_{B_{y^0}(\vy)}\!\!\!d^3\vy' \frac{g(y^0\!-\!|\vy'|)}{|\vb||\vy'|}
    1_{b^2>0,b^0>0} \notag\\
    &\hspace{3cm} \times g_1\!\left(\!\frac{x^0+y^0-|\vy'|+|\vb|}{2} \right) \\\label{y'Integral2}
    &-\frac{\lambda \|\psi\|_g}{4(4\pi)^2}\! \int_{B_{y^0}(\vy)}\!\!\!d^3\vy' \, \frac{g(y^0-|\vy'|)}{|\vb||\vy'|}
    1_{b^2>0,b^0>0}\notag\\
    &\hspace{3cm} \times   g_1\left( \max\left(\frac{x^0+y^0-|\vy'|-|\vb|}{2},0\right)\right)\\\label{y'Integral3}
    & +\frac{\lambda \|\psi\|_g}{4(4\pi)^2} \int_{B_{y^0}(\vy)}d^3\vy' \, \frac{g(y^0-|\vy'|)}{|\vb||\vy'|} 1_{b^2<0}~1_{x^0+y^0-|\vy'|>|\vb|}\notag\\
    &\hspace{3cm} \times g_1\left(\frac{x^0+y^0-|\vy'|-|\vb|}{2} \right).
\end{align}

We now want to carry out as many of the remaining 
$\vy'$-integrations as possible. In order to do so, 
we orient the coordinates such that \(\vx-\vy\) is 
parallel to the \((\vy')_3\) axis. Then the integrands 
in \eqref{y'Integral1}-\eqref{y'Integral3} are 
independent of the azimuthal angle $\varphi$ of the 
respective spherical coordinate system 
$(\rho,\theta,\varphi)$ with standard conventions.

In order to perform the remaining angular and then the 
radial integral, we need to find out which boundaries 
for $\theta$ and $r$ result from the characteristic 
functions. First we analyze for which arguments the 
maximum in \eqref{y'Integral2} is greater than zero 
and therefore contributes to the integral (as 
$g_1(0) = 0$). We have:
\begin{align}\notag
  &\frac{x^0+y^0-|\vy'|-|\vb|}{2}~>~0 \\
  &\iff  (x^0\!+\!y^0\!-\!|\vy'|)^2>|\vx-\vy|^2\!+\!|\vy'|^2\!+\!2|\vy'||\vx\!-\!\vy|\!\cos\theta\nonumber\\
  &\iff \cos\theta\!<\!\frac{(x^0\!+\!y^0)^2}{2|\vy'||\vx\!-\!\vy|} \!-\!\frac{|\vx\!-\!\vy|}{2|\vy'|} \!-\! \frac{x^0\!+\!y^0}{|\vx\!-\!\vy|}\!=:P_{x,y}(|\vy'|).
\label{eq:costhetapxy}
\end{align}
This calculation also helps to reformulate the second 
indicator function $1_{b^2<0}~1_{x^0+y^0-|\vy'|>
|\vb|}$ in \eqref{y'Integral3} (for which we have 
\(b^2<0\)).
The condition \(b^0>0\) in \eqref{y'Integral1} and 
\eqref{y'Integral2} is readily seen to be equivalent 
to
\begin{equation}
	|\vy'|>y^0-x^0.
	\label{eq:b0cond}
\end{equation}
In order to perform the \(\theta\)-integral we have 
to translate \(b^2\gtrless 0\) into conditions on 
\(\theta\). We have:
\begin{align}
    b^2>0 &\iff (x^0-y^0+|\vy'|)^2~>~ |\vx-\vy|^2+|\vy'|^2+2|\vy'||\vx-\vy|\cos\theta \nonumber\\
    &\iff \cos\theta~<~ \frac{(x-y)^2}{2|\vy'||\vx-\vy|} + \frac{x^0-y^0}{|\vx-\vy|}:=K_{x-y}(|\vy'|).
\label{eq:costhetakxy}
\end{align}
With these considerations, we have extracted 
relatively simple conditions on the boundaries of the 
integrals in spherical coordinates. However, if 
different restrictions of the boundaries conflict 
with each other, it may happen that for some 
parameter values the domain of integration is the 
empty set. 
We check whether this is so term by term, focusing 
on the \(\theta\)-integration first. For term 
\eqref{y'Integral1}, \(\theta\) needs to satisfy 
\(-1<\cos\theta<\min(1,K_{x-y}(|\vy'|))\), so we 
need to check whether \(-1<K_{x-y}(|\vy'|)\) holds. 
We have:
\begin{align}\notag
    &-1 < K_{x-y}(|\vy'|) \\
    &\iff -2|\vy'||\vx-\vy|<(x-y)^2+2|\vy'|(x^0-y^0)\nonumber\\
    &\iff 0~<~(x-y)^2+2|\vy'|(x^0-y^0+|\vx-\vy|)\nonumber\\
    &\iff \left\{\begin{matrix}
    \frac{y^0-x^0+|\vx-\vy|}{2}<|\vy'|  \quad \text{for } |\vx-\vy|>y^0-x^0\\
    \frac{y^0-x^0+|\vx-\vy|}{2}>|\vy'| \quad \, \text{for } |\vx-\vy|<y^0-x^0.
    \end{matrix}\right.
\end{align}
Together with \eqref{eq:b0cond}, we obtain the 
condition \(y^0-x^0<|\vy'|<
\frac{y^0-x^0+|\vx-\vy|}{2}<y^0-x^0\) in the second 
case which means that there is no contribution to 
the integral. So we focus on the first case,
\begin{equation}
	\frac{y^0-x^0+|\vx-\vy|}{2}<|\vy'|  \quad \text{and } |\vx-\vy|>y^0-x^0,
\label{eq:xycond1}
\end{equation}
by including the characteristic function 
$1_{|\vx-\vy|>y^0-x^0}$ in the integral. Next, we turn 
to the radial integral. By comparing its upper 
limit $|\vy'|<y^0$ and lower limit $(y^0-x^0+
|\vx-\vy|)/2$, we find that the integral can only be 
nonzero for
\begin{equation}
	y^0+x^0>|\vx-\vy|.
	\label{eq:xycond2}
\end{equation}
We make this clear by including the respective 
characteristic function.

%%%
\subparagraph{Simplification of term 
\eqref{y'Integral1}.}
These considerations allow us to continue computing 
\eqref{y'Integral1}:
\begin{align}\label{casesFirstTermBeginning}
    &\eqref{y'Integral1}
    =\frac{\lambda \|\psi\|_g}{4(4\pi)^2}1_{y^0+x^0>|\vx-\vy|}  \int_{\max(0,y^0-x^0)}^{y^0} d\rho \int_{0}^{2\pi}d\varphi~ 
    1_{\frac{y^0-x^0+|\vx-\vy|}{2}<\rho}\nonumber\\ 
  &\times 1_{|\vx-\vy|>y^0-x^0} \int_{-1}^{\min(1,K_{x-y}(\rho))} d\cos\theta  ~\frac{\rho \, g(y^0-\rho)}{\sqrt{|\vx-\vy|^2+\rho^2 + 2 |\vx-\vy|\rho \cos\theta}}\nonumber\\ 
	&\times g_1\left(\frac{x^0+y^0-\rho + \sqrt{|\vx-\vy|^2+\rho^2+2\rho|\vx-\vy|\cos\theta}}{2}\right).
    \end{align}
Now we carry out the $\varphi$-integration and use the 
same trick for the $\theta$-integral as for the 
$\vartheta$-integral in the \(\vx'\)-integration 
earlier.  Moreover, we absorb some of the restrictions 
of \(\rho\) into the limits of the integrals. This 
yields:
    \begin{align}
    \eqref{y'Integral1} &=\frac{\lambda \|\psi\|_g}{8(4\pi)}  1_{y^0+x^0>|\vx-\vy|>y^0-x^0} \int_{\max\left(0,y^0-x^0,\frac{y^0-x^0+|\vx-\vy|}{2}\right)}^{y^0} d\rho \notag\\
    &\int_{-1}^{\min(1,K_{x-y}(\rho))} \!\!\! dw \, \frac{2 g(y^0-\rho)}{|\vx-\vy|}\nonumber\\
 &\times \partial_w g_2\left(\frac{x^0+y^0-\rho + \sqrt{|\vx-\vy|^2+\rho^2+2\rho|\vx-\vy|w}}{2}\right)\nonumber\\
    &= \frac{\lambda \|\psi\|_g}{4(4\pi)} 1_{x^0+y^0>|\vx-\vy|>y^0-x^0} \int_{\max\left(0,y^0-x^0,\frac{y^0-x^0+|\vx-\vy|}{2}\right)}^{y^0} \!\!\!\!d\rho~   \frac{ g(y^0-\rho)}{|\vx-\vy|}\nonumber\\ 
    &\hspace{-1cm}\times \left[ g_2\!\left(\!\frac{x^0\!+\!y^0\!-\!\rho \!+\! \sqrt{|\vx\!-\!\vy|^2\!+\!\rho^2\!+\!2\rho|\vx\!-\!\vy|\min(1,K_{x-y}(\rho))}}{2}\right)\right.\nonumber\\
    &\left.-g_2\left(\frac{x^0+y^0-\rho + ||\vx-\vy|-\rho|}{2}\right)\right]
\end{align}
The square root can be simplified using the following 
identity:
\begin{align}\notag
   &\sqrt{|\vx-\vy|^2+\rho^2+2\rho|\vx-\vy|K_{x-y}(\rho)}\\\label{PlugKIn}
   &=\sqrt{\rho^2 +(x^0-y^0)^2+2\rho (x^0-y^0)}
   =|x^0-y^0+\rho|.
\end{align}
Using this, we can effectively pull the minimum out of 
the square root. We obtain:
\begin{align}\nonumber
    &\eqref{y'Integral1} \!=\!\frac{\lambda \|\psi\|_g}{16\pi} 1_{x^0+y^0>|\vx-\vy|>y^0-x^0}\! \int_{\max\left(0,y^0-x^0,\frac{y^0-x^0+|\vx-\vy|}{2}\right)}^{y^0} \!\!\!d\rho ~ \frac{ g(y^0-\rho)}{|\vx-\vy|}\\ 
    &\times\Bigg[ g_2\!\left(\frac{x^0+y^0-\rho + \min(|\vx-\vy|+\rho,|x^0-y^0+\rho|)}{2}\right)\notag\\
    &\hspace{3cm}-g_2\!\left(\frac{x^0+y^0-\rho + ||\vx-\vy|-\rho|}{2}\right)\Bigg].
\label{CasesFirstTermEnd}
\end{align}
Next, we subdivide the conditions in the first 
indicator function into two cases, (a) $(x-y)^2>0$ and 
(b) $(x-y)^2<0$. In case (a), the condition 
$|\vx-\vy|>y^0-x^0$ implies $x^0>y^0$. This, in turn, 
yields $\max\left(0,y^0-x^0,\frac{y^0-x^0+|\vx-\vy|}{2}
\right) = 0$.  Moreover, the condition 
$x^0+y^0>|\vx-\vy|$ is automatically satisfied (note 
that $x^0,y^0>0$). In case (b), the condition 
$|\vx-\vy|>0$ is automatically satisfied. We find:
\begin{align}
 \nonumber
     &\eqref{y'Integral1} = \frac{\lambda \|\psi\|_g}{16\pi} 1_{(x-y)^2>0, x^0>y^0} \int_{0}^{y^0} d\rho  \, \frac{ g(y^0-\rho)}{|\vx-\vy|}\\\nonumber
    &\times\left[ g_2\left(\frac{x^0+y^0 + |\vx-\vy|}{2}\right)
    -g_2\left(\frac{x^0+y^0-\rho + ||\vx-\vy|-\rho|}{2}\right)\right]\\\nonumber
    &+ \frac{\lambda \|\psi\|_g}{16\pi} 1_{(x-y)^2<0}~1_{ x^0+y^0>|\vx-\vy|} \int_{\frac{y^0-x^0+|\vx-\vy|}{2}}^{y^0} d\rho \, \frac{ g(y^0-\rho)}{|\vx-\vy|}\\\nonumber
    &\times \Bigg[ g_2\left(\frac{x^0+y^0-\rho + |x^0-y^0+\rho|}{2}\right)\\\notag
    &\quad \quad -g_2\left(\frac{x^0+y^0-\rho + ||\vx-\vy|-\rho|}{2}\right)\Bigg]\\\nonumber
    &= \frac{\lambda \|\psi\|_g}{16\pi} 1_{(x-y)^2>0, x^0>y^0} \int_{0}^{y^0} d\rho \, \frac{ g(y^0-\rho)}{|\vx-\vy|}\\\nonumber
    &\times \Bigg[ g_2\left(\frac{x^0+y^0 + |\vx-\vy|}{2}\right)\\\notag
    &\quad \quad -g_2\!\max\left(\frac{x^0+y^0- |\vx-\vy|}{2},\frac{x^0+y^0+ |\vx-\vy|}{2}-\rho\right)\Bigg]\\\nonumber
    &+ \frac{\lambda \|\psi\|_g}{16\pi} 1_{(x-y)^2<0}~1_{ x^0+y^0>|\vx-\vy|} \int_{\frac{y^0-x^0+|\vx-\vy|}{2}}^{y^0} d\rho \, \frac{ g(y^0-\rho)}{|\vx-\vy|}\\
    &\times\Bigg[ g_2\!\max\left(x^0,y^0-\rho\right)\notag\\
    &\quad \quad -g_2\!\max\left(\frac{x^0+y^0-|\vx-\vy|}{2},\frac{x^0+y^0+|\vx-\vy|}{2}-\rho\right)\Bigg].
\label{eq:resulty'Integral1}
\end{align}
Here and in the following we abbreviate 
$g_2(\max(\cdots))$ as $g_2 \max (\cdots)$, and 
similarly for the minimum. This ends the calculation 
of \eqref{y'Integral1}: we have arrived at an 
expression where no more exact calculations can be 
done and further estimates are needed.

%%%
\subparagraph{Simplification of term \eqref{y'Integral2}.}
Next, we proceed with \eqref{y'Integral2} in a similar fashion. In case the reader is not interested in the details of the calculation, the result can be found in \eqref{eq:resulty'Integral2}.

The restrictions of the integration variables for 
\eqref{y'Integral2} are the same as for 
\eqref{y'Integral1}, namely:
\begin{align}
\cos\theta<K_{x-y}(|\vy'|)\quad\quad   &\text{from } \eqref{eq:costhetakxy},\\
\frac{y^0-x^0+|\vx-\vy|}{2} < |\vy'| \quad\quad  &\text{from } \eqref{eq:xycond1}\label{eq:condfromkxy}\\
y^0-x^0<|\vx-\vy|<y^0+x^0 \quad\quad  &\text{from \eqref{eq:xycond1} and } \eqref{eq:xycond2}.
\end{align}
The only difference is that from the maximum in 
\eqref{y'Integral2}, we obtain the additional 
restriction \eqref{eq:costhetapxy}, i.e.
\begin{equation}
	\cos\theta< P_{x,y}(|\vy'|).
\end{equation}
We need to check if there are new restrictions imposed 
by \(P_{x,y}(|\vy'|)>-1\). We compute
\begin{align}
    P_{x,y}(|\vy'|) ~&>~ -1~~ \iff\nonumber\\
     \frac{(x^0+y^0)^2}{2|\vy'||\vx-\vy|} - \frac{|\vx-\vy|}{2|\vy'|} - \frac{x^0+y^0}{|\vx-\vy|}~&>~-1    ~~\iff\nonumber \\
   |\vy'| ~&<~ \frac{x^0+y^0+|\vx-\vy|}{2};
\end{align}
however, the last inequality is already ensured by 
\eqref{eq:condfromkxy} and $x^0>0$. 
In order to be able to evaluate \eqref{y'Integral2} 
further, we next plug the condition 
$\cos \theta < P_{x,y}(|\vy'|)$ into the expression 
for $|\vb|$. This yields (recall that we use spherical 
variables for $|\vy'|$):
\begin{align}\label{PlugPIn}
   |\vb| &= \sqrt{|\vx-\vy|^2+\rho^2 + 2 \rho |\vx-\vy| \cos \theta} \notag\\
   &< \sqrt{|\vx-\vy|^2+\rho^2 + 2 \rho |\vx-\vy| P_{x,y}(\rho)}\nonumber\\
    &= \sqrt{\rho^2 - 2\rho(x^0+y^0) +(x^0+y^0)^2}=x^0+y^0-\rho.
\end{align}
With this, we perform for \eqref{y'Integral2} the 
analogous calculation to 
\eqref{casesFirstTermBeginning}--
\eqref{CasesFirstTermEnd}. This yields:
\begin{align}
    &\eqref{y'Integral2}=\frac{\lambda\|\psi\|_g}{16\pi} 1_{y^0-x^0<|\vx-\vy|<x^0+y^0} \int_{\max \left(0,y^0-x^0, \frac{y^0-x^0+|\vx-\vy|}{2} \right)}^{y^0} \!\!\!d\rho \frac{g(y^0-\rho)}{|\vx-\vy|}\nonumber\\
    &\times \left[g_2\!\left(\frac{x^0\!+\!y^0\!-\!\rho\!-\!\min(|\vx\!-\!\vy|\!+\!\rho,|x^0\!-\!y^0\!+\!\rho|,x^0\!+\!y^0\!-\!\rho)}{2} \right)\right.\nonumber\\
    &\left.\quad\quad  -g_2\!\left( \frac{x^0+y^0-\rho-||\vx-\vy|-\rho|}{2} \right) \right] \nonumber\\
    &= \frac{\lambda \|\psi\|_g}{16\pi} 1_{(x-y)^2>0, x^0>y^0} \int_{0}^{y^0} d\rho  ~\frac{ g(y^0-\rho)}{|\vx-\vy|}\nonumber\\
    &\times\left[ g_2\!\left(\frac{x^0\!+\!y^0 \!-\! |\vx\!-\!\vy|}{2}\!-\!\rho\right)
    \!-\!g_2\!\left(\!\frac{x^0\!+\!y^0\!-\!\rho \!-\! ||\vx\!-\!\vy|\!-\!\rho|}{2}\right)\right]\nonumber\\
    &+ \frac{\lambda \|\psi\|_g}{16\pi} 1_{(x-y)^2<0}~1_{ x^0+y^0>|\vx-\vy|} \int_{\frac{y^0-x^0+|\vx-\vy|}{2}}^{y^0} d\rho  ~\frac{ g(y^0-\rho)}{|\vx-\vy|}\nonumber\\
    &\times \left[ g_2\!\left(\!\frac{x^0\!+\!y^0\!-\!\rho \!-\! |x^0\!-\!y^0\!+\!\rho|}{2}\right)
    \!-\!g_2\!\left(\!\frac{x^0\!+\!y^0\!-\!\rho \!-\! ||\vx\!-\!\vy|\!-\!\rho|}{2}\right)\right]\nonumber\\
    &= \frac{\lambda \|\psi\|_g}{16\pi} 1_{(x-y)^2>0, x^0>y^0} \int_{0}^{y^0} d\rho  ~\frac{ g(y^0-\rho)}{|\vx-\vy|}\nonumber\\
    &\times \Bigg[ g_2\!\left(\!\frac{x^0\!+\!y^0 \!-\! |\vx\!-\!\vy|}{2}\!-\!\rho\right)\\
    &\quad \quad \!-\!g_2\!\min\!\left(\!\frac{x^0\!+\!y^0 \!-\! |\vx\!-\!\vy|}{2},\frac{x^0\!+\!y^0 \!+\! |\vx\!-\!\vy|}{2}\!-\!\rho\!\right)\Bigg]\nonumber\\
    &+ \frac{\lambda \|\psi\|_g}{16\pi} 1_{(x-y)^2<0}~1_{ x^0+y^0>|\vx-\vy|} \int_{\frac{y^0-x^0+|\vx-\vy|}{2}}^{y^0} d\rho ~ \frac{ g(y^0-\rho)}{|\vx-\vy|}\nonumber\\
    &\times \Bigg[ g_2\!\min\left(x^0,y^0-\rho\right)\notag\\
    &\quad \quad -g_2\!\min\left(\!\frac{x^0\!+\!y^0\!-\! |\vx\!-\!\vy|}{2},\frac{x^0\!+\!y^0\!+\! |\vx\!-\!\vy|}{2}\!-\!\rho\right)\Bigg].
\label{eq:resulty'Integral2}
\end{align}
This ends the calculation of \eqref{y'Integral2}. 

%%%
\subparagraph{Simplification of term 
\eqref{y'Integral3}.}
We next turn to \eqref{y'Integral3}. In case the reader 
is not interested in the details of the computation, 
the result can be found in \eqref{eq:resulty'Integral3}.
First we note that the restriction imposed by the first 
indicator function here is \(\cos\theta>K_{x-y}(|\vy'|)\) 
and the condition of the second indicator function is 
\(\cos\theta<P_{x,y}(|\vy'|)\). In order to to satisfy 
these conditions (and the restrictions of the regular 
range of integration) it is required that
\begin{equation}
    \max(-1,K_{x-y}(|\vy'|)<\cos\theta<\min(1,P_{x,y}(|\vy'|)).
\end{equation}
This leads us to ask which restrictions on \(|\vy'|\) 
are imposed by the conditions
\begin{align}
    K_{x-y}(|\vy'|) ~&<~1,\\
    P_{x,y}(|\vy'|) ~&>~-1,\\
    K_{x-y}(|\vy'|) ~&<~ P_{x,y}(|\vy'|).
\end{align}
These restrictions shall be computed next. With 
\(|\vy'|=\rho\), we find:
\begin{align}\notag
     &K_{x-y}(|\vy'|)<1\\
    &\iff \frac{(x-y)^2}{2\rho|\vx-\vy|}+\frac{x^0-y^0}{|\vx-\vy|}~<~1\nonumber\\
 &\iff    (x-y)^2~<~2\rho(y^0-x^0+|\vx-\vy|)\nonumber\\
   &\iff  \left\{\begin{matrix}\rho>\frac{y^0-x^0-|\vx-\vy|}{2}\quad \text{for } |\vx-\vy|>x^0-y^0,\\
\rho<\frac{y^0-x^0-|\vx-\vy|}{2}\quad \, \text{for } |\vx-\vy|<x^0-y^0.
     \end{matrix} \right.
\end{align}
The second case in the last line is in conflict with 
\(\rho>0\), so we have to impose the first condition on 
\eqref{y'Integral3}. We continue with 
\(P_{x,y}(\rho)>-1\).
\begin{align}\notag
    &P_{x,y}(\rho)>-1\\
    &\iff~ \frac{(x^0+y^0)^2}{2\rho|\vx-\vy|} - \frac{|\vx-\vy|}{2\rho}-\frac{x^0+y^0}{|\vx-\vy|} ~>~-1 \nonumber \\
   &\iff~ (x^0+y^0)^2-|\vx-\vy|^2~>~2\rho(x^0+y^0-|\vx-\vy|)\nonumber \\
    &\iff~\left\{\begin{matrix}\rho< \frac{x^0+y^0+|\vx-\vy|}{2} \quad \text{for } x^0+y^0>|\vx-\vy|,\\
    \rho> \frac{x^0+y^0+|\vx-\vy|}{2} \quad \text{for } x^0+y^0<|\vx-\vy|.
    \end{matrix} \right.
\end{align}
The second case is in conflict with \(\rho<y^0\), so 
we implement indicator functions corresponding only to 
the first case in \eqref{y'Integral3}. The third 
condition \(K_{x-y}(\rho)<P_{x,y}(\rho)\) in fact does 
not impose any additional conditions. This can be seen 
as follows:
\begin{align}\notag
  &K_{x-y}(\rho)~<~P_{x,y}(\rho)\\
  &\iff~~~ \frac{(x-y)^2}{2\rho|\vx-\vy|}+\frac{x^0-y^0}{|\vx-\vy|}~<~\frac{(x^0+y^0)^2}{2\rho|\vx-\vy|} - \frac{|\vx-\vy|}{2\rho}-\frac{x^0+y^0}{|\vx-\vy|}\nonumber\\
  &\iff~~~ -2x^0y^0 + 4\rho x^0~<~2x^0y^0\nonumber\\
  &\iff~~~   \rho<y^0,
\end{align}
which always holds true.

Taking into account the computed restrictions, we 
arrive at:
\begin{align}
    &\eqref{y'Integral3} \stackrel{\cos\theta=w}{=}\frac{\lambda \|\psi\|_g}{4(4\pi)^2} \int_0^{2\pi} d\varphi \int_0^{y^0} d\rho \int_{-1}^1 dw \,1_{K_{x-y}(\rho)<w<P_{x,y}(\rho)}  \nonumber\\
     &\quad \times 1_{\frac{y^0-x^0-|\vx-\vy|}{2}<\rho<\frac{x^0+y^0+|\vx-\vy|}{2}}\frac{g(y^0-\rho)\rho}{\sqrt{\rho^2+|\vx-\vy|^2+2\rho|\vx-\vy|w}} \nonumber\\
     &\quad \times 1_{x^0-y^0<|\vx-\vy|<x^0+y^0} g_1\!\!\left( \!\frac{x^0\!\!+\!y^0\!\!-\!\sqrt{\rho^2\!\!+\!|\vx\!-\!\vy|^2\!\!+\!2\rho|\vx\!-\!\vy|w}}{2}\right)\nonumber\\
     &= \frac{\lambda \|\psi\|_g 2\pi}{4(4\pi)^2}1_{x^0-y^0<|\vx-\vy|<x^0+y^0} \int_{\max\big(0,\frac{y^0-x^0-|\vx-\vy|}{2}\big)}^{\min\big(y^0,\frac{x^0+y^0+|\vx-\vy|}{2} \big)}d\rho\notag\\
     &\quad \times \int_{\max(-1,K_{x-y}(\rho))}^{\min(1,P_{x,y}(\rho))}dw \, \frac{-2 g(y^0-\rho)}{|\vx-\vy|}\nonumber\\
     &\times \partial_w 
     g_2\!\left(\frac{x^0+y^0-\sqrt{\rho^2+|\vx-\vy|^2 +2\rho|\vx-\vy|w}}{2} \right)\nonumber\\
     &=\frac{\lambda \|\psi\|_g}{16\pi} 1_{x^0-y^0<|\vx-\vy|<x^0+y^0} 
     \int_{\max\big(0,\frac{y^0-x^0-|\vx-\vy|}{2}\big)}^{\min\big(y^0,\frac{x^0+y^0+|\vx-\vy|}{2} \big)}d\rho \, \frac{g(y^0-\rho)}{|\vx-\vy|}\nonumber\\ 
     &\times \left[g_2\!\!\left(\!\frac{x^0\!+\!y^0\!-\!\rho\!-\!\sqrt{\rho^2\!+\!|\vx\!-\!\vy|^2\!+\!2\rho|\vx\!-\!\vy|\max(-1,K_{x-y}(\rho))}}{2}\right)\right.\nonumber\\
     &\left.-g_2\!\!\left(\!\frac{x^0\!\!+\!y^0\!\!-\!\rho\!-\!\sqrt{\rho^2\!+\!|\vx\!-\!\vy|^2\!+\!2\rho|\vx\!-\!\vy|\min(1,P_{x,y}(\rho))}}{2}\right)
     \!\right]\!.
\end{align}
At this point, the expressions look quite formidable. 
We can, however, achieve significant simplifications 
by inserting the functional form of $K_{x,y}(\rho)$ 
and $P_{x,y}(\rho)$ as in \eqref{PlugPIn} and 
\eqref{PlugKIn}. This yields:
\begin{align}
  &\eqref{y'Integral3} =\notag\\
  &\frac{\lambda\|\psi\|_g}{16\pi} 1_{x^0-y^0<|\vx-\vy|<x^0+y^0}
  \int_{\max\left(0,\frac{y^0-x^0-|\vx-\vy|}{2}\right)}^{\min\left(y^0,\frac{x^0+y^0+|\vx-\vy|}{2} \right)}d\rho \, \frac{g(y^0-\rho)}{|\vx-\vy|}\nonumber\\ 
  &~~~\times\left[ g_2\left( \frac{x^0+y^0-\rho-\max(||\vx-\vy|-\rho|,|x^0-y^0+\rho|)}{2} \right)\right.\nonumber\\
  &~~~\left.-g_2\left( \frac{x^0+y^0-\rho-\min(|\vx-\vy|+\rho,x^0+y^0-\rho)}{2} \right)\right]
	\label{eq:y'Integral3calc}
\end{align}
Now we simplify the arguments of the $g_2$-functions. 
For the first one, we have:
\begin{align}
&x^0\!\!+\!y^0\!\!-\!\rho\!-\!\max(||\vx\!-\!\vy|\!-\!\rho|,|x^0\!\!-\!y^0\!\!+\!\rho|)\nonumber\\
&= x^0\!\!+\!y^0\!\!-\!\rho\! -\!\max ( |\vx\!-\!\vy|\!-\!\rho,\rho\!-\!|\vx\!-\!\vy|,x^0\!\!-\!y^0\!\!+\!\rho,y^0\!\!-\!\rho\!-\!x^0)\nonumber\\
&= \min\!\left(x^0\!\!+\!y^0\!\!-\!\rho\!-\!|\vx\!-\!\vy|,x^0\!\!+\!y^0\!\!+\!|\vx\!-\!\vy|\!-\!2\rho,2(y^0\!\!-\!\rho),2x^0 \right).
\end{align}
For the second one we get
\begin{equation}
	x^0\!\!+\!y^0\!\!-\!\min(|\vx\!-\!\vy|\!+\!\rho,x^0\!\!+\!y^0\!\!-\!\rho) = \max(x^0\!\!+\!y^0\!\!-\!|\vx\!-\!\vy|\!-\!2\rho,0).
\end{equation}
Using this in \eqref{eq:y'Integral3calc}, we find:
\begin{align}
    &\eqref{y'Integral3} =\frac{\lambda\|\psi\|_g}{16\pi} 
    1_{x^0-y^0<|\vx-\vy|<x^0+y^0}
    \int_{\max\big(0,\frac{y^0-x^0-|\vx-\vy|}{2}\big)}^{\min\big(y^0,\frac{x^0+y^0+|\vx-\vy|}{2} \big)}\!d\rho \, \frac{g(y^0-\rho)}{|\vx-\vy|}\nonumber\\
    &\quad\times \left[g_2 \min\!\left(\frac{x^0\!\!+\!y^0\!\!-\!|\vx\!-\!\vy|}{2},\frac{x^0\!\!+\!y^0\!\!+\!|\vx\!-\!\vy|}{2}\!-\!\rho,y^0\!\!-\!\rho,x^0 \!\right) \right.\nonumber\\
    &\quad \quad \left. -g_2 \max\!\left( \frac{x^0\!\!+\!y^0\!\!-\!|\vx\!-\!\vy|}{2}\!-\!\rho,0\!\right) \right].
\end{align}
As in the consideration below 
\eqref{CasesFirstTermEnd}, we split the expression 
into separate terms with \((x-y)^2 \gtrless 0\). 
Using \(y^0\gtrless x^0+|\vx-\vy|\), we can simplify 
the expressions involving the minimum. This results 
in:
\begin{align}
    &\eqref{y'Integral3} =
    \frac{\lambda\|\psi\|_g}{16\pi} 1_{(x-y)^2>0,y^0>x^0}
    \int_{\frac{y^0-x^0-|\vx-\vy|}{2}}^{\frac{x^0+y^0+|\vx-\vy|}{2}}d\rho \, \frac{g(y^0-\rho)}{|\vx-\vy|}\nonumber\\
    &\times \left[g_2 \min\!\left(\!\frac{x^0\!\!+\!y^0\!\!+\!|\vx\!-\!\vy|}{2}\!-\!\rho,x^0\!\right) \!-\!g_2\!\max\!\left(\! \frac{x^0\!\!+\!y^0\!\!-\!|\vx\!-\!\vy|}{2}\!-\!\rho,0\!\right) \right]\nonumber\\
    &+\frac{\lambda\|\psi\|_g}{16\pi} 
    1_{(x-y)^2<0,|\vx-\vy|<x^0+y^0}
    \int_{0}^{y^0}d\rho \, \frac{g(y^0-\rho)}{|\vx-\vy|}\label{eq:resulty'Integral3}\\
    &\times \left[g_2\!\min\!\left(\!\frac{x^0\!\!+\!y^0\!\!-\!|\vx\!-\!\vy|}{2},y^0\!\!-\!\rho\!\right) \!-\!g_2\!\max\!\left( \!\frac{x^0\!\!+\!y^0\!\!-\!|\vx\!-\!\vy|}{2}\!-\!\rho,0\!\right) \right].\notag
\end{align}
This concludes the calculation of \eqref{y'Integral3}.

%%%
\subparagraph{Summary of the first estimate.} We have 
obtained the following bound for $| A_0 \psi |(x,y)$:
\begin{align}
    &\frac{16\pi}{\lambda\|\psi\|_g}|A_0\psi|(x,y) \le 
    1_{(x-y)^2>0, x^0>y^0} \int_{0}^{y^0} d\rho \, \frac{ g(y^0-\rho)}{|\vx\!-\!\vy|}\nonumber\\
    &\times \Bigg[ g_2\!\left(\!\frac{x^0\!\!+\!y^0\!\! +\! |\vx\!-\!\vy|}{2}\right)\notag\\
    &\quad\quad \!-\!g_2\!\max\!\left(\!\frac{x^0\!\!+\!y^0\!\!-\! |\vx\!-\!\vy|}{2},\frac{x^0\!\!+\!y^0\!\!+ \!|\vx\!-\!\vy|}{2}\!-\!\rho\right)\Bigg]\nonumber\\
    &+ 1_{(x-y)^2<0}~1_{ x^0+y^0>|\vx\!-\!\vy|} \int_{\frac{y^0-x^0+|\vx\!-\!\vy|}{2}}^{y^0} d\rho \, \frac{ g(y^0-\rho)}{|\vx\!-\!\vy|}\nonumber\\
    &\times \Big[ g_2\!\max\!\left(x^0,y^0-\rho\right)\notag\\
    &\quad\quad -g_2\!\max\!\left(\frac{x^0\!\!+\!y^0\!\!-\!|\vx\!-\!\vy|}{2},\frac{x^0\!\!+\!y^0\!\!+|\vx\!-\!\vy|}{2}-\rho\right)\Bigg]\nonumber\\
    &+ 1_{(x-y)^2>0, x^0>y^0} \int_{0}^{y^0} d\rho \, \frac{ g(y^0-\rho)}{|\vx\!-\!\vy|}\nonumber\\
    &\times \Bigg[ g_2\left(\frac{x^0+y^0 - |\vx\!-\!\vy|}{2}-\rho\right)\notag\\
    &\quad\quad -g_2\min\!\left(\frac{x^0+y^0 - |\vx\!-\!\vy|}{2},\frac{x^0+y^0 + |\vx\!-\!\vy|}{2}\!-\!\rho\right)\Bigg]\nonumber\\\nonumber
    &+ 1_{(x-y)^2<0}~1_{ x^0+y^0>|\vx\!-\!\vy|} \int_{\frac{y^0-x^0+|\vx\!-\!\vy|}{2}}^{y^0} d\rho \, \frac{ g(y^0\!\!-\!\rho)}{|\vx\!-\!\vy|}\nonumber\\
    &\times \Bigg[ g_2\min\!\left(x^0,y^0-\rho\right)\notag\\
    &\quad\quad -g_2\min\!\left(\frac{x^0+y^0- |\vx\!-\!\vy|}{2},\frac{x^0\!\!+\!y^0\!\!+ |\vx\!-\!\vy|}{2}-\rho\right)\Bigg]\nonumber\\
    &+1_{(x-y)^2>0,y^0>x^0} \int_{\frac{y^0-x^0-|\vx\!-\!\vy|}{2}}^{\frac{x^0\!\!+\!y^0\!\!+|\vx\!-\!\vy|}{2}}d\rho \, \frac{g(y^0\!\!-\!\rho)}{|\vx\!-\!\vy|}\nonumber\\
    &\times \Bigg[g_2 \min\!\left(\frac{x^0\!\!+\!y^0\!\!+|\vx\!-\!\vy|}{2}\!-\!\rho,x^0\right)\notag\\
    &\quad\quad \!-\!g_2\!\max\!\left( \frac{x^0\!\!+\!y^0\!\!-\!|\vx\!-\!\vy|}{2}\!-\!\rho,0 \right) \Bigg]\nonumber\\
    &+1_{(x-y)^2<0,|\vx\!-\!\vy|<x^0+y^0}
    \int_{0}^{y^0}d\rho \,\frac{g(y^0-\rho)}{|\vx\!-\!\vy|}\nonumber\\
    &\times \Bigg[g_2 \min\!\left(\!\frac{x^0\!\!+\!y^0\!\!-\!|\vx\!-\!\vy|}{2},y^0\!\!-\!\rho\right)\notag\\
    &\quad\quad \!-\!g_2\!\max\!\left( \!\frac{x^0\!\!+\!y^0\!\!-\!|\vx\!-\!\vy|}{2}\!-\!\rho,0\!\right)  \Bigg].
\end{align}
In order to simplify the result, we introduce the 
variables
\begin{align}
    \xi^+:=\frac{x^0+y^0+|\vx-\vy|}{2},\\
    \xi^-:=\frac{x^0+y^0-|\vx-\vy|}{2}.
\end{align}
Moreover, we collect terms with the same indicator 
functions. This results in:
\begin{align}
    &\frac{16\pi}{\lambda\|\psi\|_g}|A_0\psi|(x,y)\le\notag\\
    &1_{(x-y)^2<0,\xi^->0} \!\!\int_0^{y^0}\!\!\!d\rho  \frac{g(y^0\!\!-\!\rho)}{|\vx\!-\!\vy|}
    \Big[g_2\min(\xi^-,y^0\!\!-\!\rho)\!-\!g_2\!\max(\xi^-\!\!-\!\rho,0)
    \notag\\\label{A psi estimate 1}
    &\quad + 1_{\frac{y^0\!\!-\!x^0+|\vx\!-\!\vy|}{2}<\rho} \big(g_2(x^0)+g_2(y^0\!\!-\!\rho)
    \!-\!g_2(\xi^-)\!-\!g_2(\xi^+\!\!-\!\rho)\big)
    \Big]\\\label{A psi estimate 2}
    &+1_{(x-y)^2>0,x^0>y^0}\int_0^{y^0} d\rho \, \frac{g(y^0\!\!-\!\rho)}{|\vx\!-\!\vy|} \big[g_2(\xi^+)+g_2(\xi^-\!\!-\!\rho)\notag\\
    &\hspace{6cm} \!-\! g_2(\xi^-)\!-\!g_2(\xi^+\!\!-\!\rho)\big]\\\label{A psi estimate 3}
    &+1_{(x-y)^2>0,y^0>x^0}\int_{\frac{y^0\!\!-\!x^0\!\!-\!|\vx\!-\!\vy|}{2}}^{\xi^+}d\rho \, \frac{g(y^0\!\!-\!\rho)}{|\vx\!-\!\vy|}\notag\\
    &\hspace{3cm}\times \big[g_2\min(\xi^+\!\!-\!\rho,x^0) \!-\! g_2\!\max(\xi^-\!\!-\!\rho,0)\big].
\end{align}
This estimate is an important stepping stone in the proof. Except for special weight functions, the resulting expressions are too complicated to be computed explicitly. We therefore continue with further estimates. The main difficulty in these estimates is that the $1/|\vx-\vy|$ singularity in the expressions needs to be compensated by the integrand and that this cancellation needs to be preserved by the respective estimate. Fortunately, the mean value theorem turns out suitable to provide such estimates.


%%%
\subparagraph{Simplification of 
\eqref{A psi estimate 1}-\eqref{A psi estimate 3}.}
First, we note that since $g, g_1$ and $g_2$ are monotonously increasing and 
since $\xi^- \leq \xi^+$, we have in \eqref{A psi estimate 2}:
\begin{equation}
	g_2(\xi^- - \rho) - g_2(\xi^+ - \rho) \leq 0.
\end{equation}
As the remaining terms in \eqref{A psi estimate 2} still vanish in the limit 
$|\vx-\vy| \rightarrow 0$, we may replace this difference by zero to obtain a 
suitable estimate.

Similarly, a brief calculations shows that we have 
$\xi^+ > y^0$ for $(x-y)^2<0$. It follows that:
\begin{equation}
	g_2(y^0-\rho) - g_2(\xi^+ - \rho) < 0.
\end{equation}
We shall use this in \eqref{A psi estimate 1}.

Further simplifications can be obtained using the mean 
value theorem. We begin with the expression in the 
square brackets in \eqref{A psi estimate 3}. The mean 
value theorem then implies that there is a 
\(\chi\in[\max(\xi^-\!\!-\!\rho,0),\min(\xi^+\!\!-\!\rho,x^0)]\) 
such that
\begin{align}\notag
    g_2\!\min(&\xi^+\!\!-\!\rho,x^0)-g_2\!\max(\xi^-\!\!-\!\rho,0)\\
    &= \big[\min(\xi^+\!\!-\!\rho,x^0)-\max(\xi^-\!\!-\!\rho,0)\big]g_1(\chi).
\end{align}
Therefore, we have:
\begin{align}
   & g_2\!\min(\xi^+\!\!-\!\rho,x^0)-g_2\!\max(\xi^-\!\!-\!\rho,0)\nonumber\\
   &~~~\le~ \min(\xi^+\!\!-\xi^-,\xi^+\!\!-\!\rho,x^0-\xi^-+\rho,x^0)\, g_1\!\min(\xi^+\!\!-\!\rho,x^0)\nonumber\\
   & ~~~\le~ |\vx-\vy|\, g_1\!\min(\xi^+\!\!-\!\rho,x^0) ~\le~ |\vx-\vy| \,g_1(x^0).
\end{align}
Note that the factor $|\vx-\vy|$ exactly compensates the $1/|\vx-\vy|$ 
singularity. 
This is the main reason the mean value theorem is so useful here.

Analogously we find for the expression in the square bracket in the first 
line of \eqref{A psi estimate 1}:
\begin{align}
    &g_2\!\min(\xi^-,y^0-\!\rho)-g_2\!\max(\xi^-\!\!-\!\rho,0)\nonumber\\
&~~~\leq ~ \big[\min(\xi^-,y^0-\!\rho)-\max(\xi^-\!\!-\!\rho,0)\big] g_1\!\min(\xi^-,y^0-\!\rho)\nonumber\\
    &~~~=~\min(\rho,\xi^-,y^0-\xi^-,y^0-\!\rho) \, g_1\!\min(\xi^-,y^0-\!\rho)\nonumber\\
    &~~~\le~ (y^0-\xi^-) \,g_1\!\min(\xi^-,y^0-\!\rho)\nonumber\\
    &~~~\le~ |\vx-\vy| \,g_1\!\min(\xi^-,y^0-\!\rho),
\end{align}
where we have used that the further restriction of that term, $(x-y)^2<0$, 
implies $|\vx-\vy|>|x^0-y^0|\geq y^0-x^0$.

With these considerations, we obtain a rougher but simpler estimate than 
\eqref{A psi estimate 1}-\eqref{A psi estimate 3}:
\begin{align}\notag
    &\frac{16\pi}{\lambda\|\psi\|_g}|A_0\psi|(x,y) \\
    &\le~ \label{massless_after estimate1}
    1_{(x-y)^2<0,\xi^->0} \int_0^{y^0}d\rho~ g(y^0-\!\rho)\Big[ g_1\!\min(\xi^-,y^0-\!\rho) \\\label{massless_after estimate2}
    &~~~+ 1_{\frac{y^0-x^0+|\vx-\vy|}{2}<\rho}\frac{g_2(x^0)-g_2(\xi^-)}{|\vx-\vy|}\Big]\\\label{massless_after estimate3}
    &~~~+1_{(x-y)^2>0,x^0>y^0}\frac{g_2(\xi^+)-g_2(\xi^-)}{|\vx-\vy|} \int_{0}^{y^0}d\rho~ g(y^0-\!\rho)\\\label{massless_after estimate4}
    &~~~+1_{(x-y)^2>0,y^0>x^0}\, g_1(x^0)\int_{\frac{y^0-x^0-|\vx-\vy|}{2}}^{\xi^+}d\rho~ g(y^0-\!\rho).
\end{align}
Next, we continue estimating these terms separately so that only expressions 
without integrals remain.


%%%
\subparagraph{Further estimate of \eqref{massless_after estimate1}.}
Using the monotonicity of $g_1$ as well as $\min(\xi^-,y^0-\rho) \leq \xi^-$, 
we find:
\begin{align}
  \eqref{massless_after estimate1} ~&\leq~  1_{(x-y)^2<0,\xi^->0}\, g_1(\xi^-) \int_0^{y^0} ds~ g(s) \notag\\
  &~=~  1_{(x-y)^2<0,\xi^->0} \, g_1(\xi^-) g_1(y^0).
\end{align}
For the constraints given by the indicator function, we have $\xi^- < x^0$. 
Thus:
\begin{equation}
	\eqref{massless_after estimate1} \leq 1_{(x-y)^2<0,\xi^->0} \, g_1(x^0) g_1(y^0).
\label{eq:resultmasslessafterestimate1}
\end{equation}


%%%
\subparagraph{Further estimate of \eqref{massless_after estimate2}.}
We have:
\begin{align}
	 \eqref{massless_after estimate2} ~&=~ 1_{(x-y)^2 < 0, \xi^- >0} \, \frac{ g_2(x^0) - g_2(\xi^-) }{|\vx-\vy|} \int_{\frac{y^0-x^0+|\vx-\vy|}{2}}^{y_0} d \rho \, g(y^0-\rho) \nonumber\\
&= 1_{(x-y)^2 < 0, \xi^- >0} \, \frac{g_2(x^0) - g_2(\xi^-)}{|\vx-\vy|} \int_{0}^{\xi^-} d s \, g(s)\nonumber\\
&= 1_{(x-y)^2 < 0, \xi^- >0} \, \frac{g_2(x^0) - g_2(\xi^-)}{|\vx-\vy|} \big[ g_1(\xi^-) - \underbrace{g_1(0)}_{=0} \big].
\label{eq:secondline159b}
\end{align}
Applying the mean value theorem to $g_2$ in the interval $[\xi^-,x^0]$ 
(note that here $\xi^-<x^0$), we obtain that:
\begin{equation}
	\eqref{massless_after estimate2} \leq 1_{(x-y)^2 < 0, \xi^- >0} \, \frac{x^0-\xi^-}{|\vx-\vy|}\, g_1(x^0) g_1(\xi^-).
\end{equation}
Next, we use that $\frac{x^0-\xi^-}{|\vx-\vy|}  = \frac{x^0-y^0+|\vx-\vy|}{2|\vx-\vy|} \leq 1$ as $|x^0-y^0| < |\vx-\vy|$. Thus:
\begin{equation}
	\eqref{massless_after estimate2} \leq 1_{(x-y)^2 < 0, \xi^- >0} \, g_1(x^0) g_1(\xi^-).
\end{equation}
Using also that for the given constrains $\xi^- < y^0$, we finally obtain:
\begin{equation}
	\eqref{massless_after estimate2} \leq 1_{(x-y)^2 < 0, \xi^- >0} \, g_1(x^0) g_1(y^0).
\label{eq:resultmasslessafterestimate2}
\end{equation}


%%%
\subparagraph{Further estimate of \eqref{massless_after estimate3}.}
Here, we can directly carry out the remaining integral using the definition 
of $g_1$ as the integral of $g$:
\begin{equation}
	\eqref{massless_after estimate3} = 1_{(x-y)^2>0,x^0>y^0}\, \frac{g_2(\xi^+)-g_2(\xi^-)}{|\vx-\vy|} g_1(y^0).
\end{equation}
Next, we apply the mean value theorem to $g_2$ in the interval 
$[\xi^-,\xi^+]$ noting that $\xi^+\!\!-\xi^- = |\vx-\vy|$. This implies:
\begin{equation}
	\eqref{massless_after estimate3} \leq 1_{(x-y)^2>0,x^0>y^0} \,g_1(\xi^+) g_1(y^0).
\end{equation}
Next, we note that $(x-y)^2 > 0 \Leftrightarrow |x^0-y^0| > |\vx-\vy|$. 
Together with $x^0>y^0$, we obtain $x^0>y^0 +|\vx-\vy|$ and therefore:
\begin{equation}
	\xi^+ = \frac{x^0+y^0 +|\vx-\vy|}{2} \leq x^0.
\end{equation}
Thus, we obtain:
\begin{equation}
	\eqref{massless_after estimate3} \leq 1_{(x-y)^2>0,x^0>y^0} \, g_1(x^0) g_1(y^0).
	\label{eq:resultmasslessafterestimate3}
\end{equation}

%%%
\subparagraph{Further estimate of \eqref{massless_after estimate4}.}
Here, we carry out the remaining integral as well.
\begin{align}
	\eqref{A psi estimate 3} &\leq 1_{(x-y)^2>0,y^0>x^0}\, g_1(x^0) [ g_1(\xi^+)\!-\!g_1( (y^0\!\!-\!x^0\!\!-\!|\vx\!-\!\vy|)/2)]\nonumber\\
&\leq 1_{(x-y)^2>0,y^0>x^0}\, g_1(x^0)g_1(y^0).
\label{eq:resultmasslessafterestimate4}
\end{align}
as $\xi^+\leq y^0$.


%%%
\subparagraph{Summary of the result.} Gathering the terms 
\eqref{eq:resultmasslessafterestimate1}, 
\eqref{eq:resultmasslessafterestimate2}, 
\eqref{eq:resultmasslessafterestimate3} and 
\eqref{eq:resultmasslessafterestimate4} yields:
\begin{align}
    &\frac{16\pi}{\lambda\|\psi\|_g}\, |A_0 \psi |(x,y) \\
    &\leq g_1(x^0) g_1(y^0) \left( 2 \!\times\! 1_{(x-y)^2 < 0, \xi^- >0} \!+\! 1_{(x-y)^2>0,x^0>y^0} \!+\! 1_{(x-y)^2>0,y^0>x^0}\right).\notag
\end{align}
Considering that the conditions in different indicator functions are mutually 
exclusive, we finally obtain:
\begin{equation}
	  \frac{16\pi}{\lambda\|\psi\|_g}\, |A_0 \psi |(x,y) \leq 2 g_1(x^0) g_1(y^0).
	\label{eq:resultmasslessestimate}
\end{equation}
Dividing by $g(x^0)g(y^0)$, taking the supremum over 
$x,y \in \tfrac{1}{2}\M$ and factorizing into one-dimensional suprema 
finally yields the claim \eqref{eq:estimatea0}.


%%%
\paragraph{Estimate of the mixed terms \eqref{eq:estimatea1} and 
\eqref{eq:estimatea2}.} \label{sec:estimatemixed}

We focus on $A_2$ first, starting from its definition \eqref{eq:defa2}. 
We take the absolute value and make use of 
$|\psi(x,y)| \leq g(x^0) g(y^0)\, \| \psi \|_g$. Moreover, we use:
\begin{equation}
	\left| J_1(t)/t \right| \leq \frac{1}{2}.
\end{equation}
This yields:
\begin{align}
	&|A_2 \psi|(x,y) \leq  \frac{\lambda \, m_2^2 \, \|\psi\|_g}{4(4 \pi)^3} \int d^3 \vx' \int d^3 \vy'~\frac{H(x^0\!\!-\!|\vx\!-\!\vx'|)}{|\vx\!-\!\vx'|} \frac{g( x^0\!\!-\!|\vx\!-\!\vx'|)}{|\vx'\!-\!\vy'|} \nonumber\\
&\times \left[g(x^0\!\!-\! |\vx\!-\!\vx'| \!+\! |\vx'\!-\!\vy'| ) H(x^0\!\!-\! |\vx\!-\!\vx'| \!+\! |\vx'\!-\!\vy'|) \right. \nonumber\\
&\quad \quad \times H(y^0\!\!-x^0\!\! +\! |\vx\!-\!\vx'| \!-\! |\vx'\!-\!\vy'| \!-\!|\vy\!-\!\vy'|)\notag\\
&\quad + g(x^0\!\!-\! |\vx\!-\!\vx'| \!-\! |\vx'\!-\!\vy'|) H(x^0\!\!-\! |\vx\!-\!\vx'| \!-\! |\vx'\!-\!\vy'|) \nonumber\\
&\quad \quad \left. \times H(y^0\!\!-x^0\!\! +\! |\vx\!-\!\vx'| \!+\! |\vx'\!-\!\vy'|\!-\!|\vy\!-\!\vy'|) \right].
\end{align}
As the remaining singularities are independent of each other for a suitable 
choice of integration variables (see below), we are left with an integrable 
function on a finite domain.

The next task is to bring the expressions into a simpler form. One 
possibility to do this is to use
\begin{align}\notag
  H(y^0-x^0+|\vx-\vx'|+|\vx'-\vy'| - |\vy-\vy'|) \\
  \leq H(y^0-x^0+|\vx-\vx'|+|\vx'-\vy'|)
\end{align}
for the second Heaviside function in the second summand.
The first Heaviside function in the first summand equals 1 anyway, as 
$|\vx-\vx'| < x^0$. We furthermore use
\begin{align}\notag
  H(y^0-x^0+|\vx-\vx'|-|\vx'-\vy'| - |\vy-\vy'|) \\
  \leq H(y^0-x^0+|\vx-\vx'|-|\vx'-\vy'|),
\end{align}
as it simplifies the domain of integration. Overall, the domain of 
integration remains bounded. 
Introducing $\vz_1 = \vx-\vx'$, $\vz_2 = \vx'-\vy'$ (with Jacobi determinant 
of modulus 1) and using spherical coordinates for $\vz_2$, this leads to:
\begin{align}
	&|A_2 \psi|(x,y) \frac{4(4\pi)^3}{\lambda \, m_2^2 \, \| \psi \|_g}\nonumber\\
& \leq \int_{B_{x^0}(0)} \!\!\!d^3 \vz_1 4\pi \!\int_0^{\max(0,y^0-x^0+|\vz_1|)}\!\!\!\!\!\! d^3 \vz_2  \frac{g(x^0\!\!-\!|\vz_1|) g(x^0\!\!-\!|\vz_1|\!+\!|\vz_2|)}{|\vz_1||\vz_2|} |\vz_2|^2  \nonumber\\
&+\!\int_{B_{x^0}(0)} \!\!\!d^3 \vz_1 4\pi\! \int_{\max( 0, x^0-y^0-|\vz_1|)}^{x^0-|\vz_1|} \!\!\!\!\!\! d|\vz_2| \frac{g(x^0-|\vz_1|) g(x^0-|\vz_1|-|\vz_2|)}{|\vz_1||\vz_2|}  |\vz_2|^2.
\end{align}

Using spherical coordinates also for $\vz_1$, this can be further simplified 
to:
\begin{align}
  &|A_2 \psi|(x,y)  \frac{16\pi}{\lambda  m_2^2 \, \| \psi \|_g} \notag\\
  &\leq  \int_0^{x^0} d r_1 \int_0^{\max(0,y^0-x^0+r_1)} \!\!\! dr_2~r_1 r_2\,g(x^0\!\!-\!r_1) g(x^0\!\!-\!r_1\!+\!r_2)\label{eq:A2firstestimate}\\
&+~\int_0^{x^0} d r_1 \int_{\max( 0,x^0-r_1-y^0)}^{t_1-r_1} \!\!\! dr_2~r_1 r_2 ~ g(x^0\!\!-\!r_1) g(x^0\!\!-\!r_1\!-\!r_2) \label{eq:A2secondestimate}.
\end{align}

Our next task is to simplify the remaining integrals. We begin with making 
the change of variables $\rho = x^0-r_1$:
\begin{align}
  &|A_2 \psi|(x,y)  \frac{16\pi}{\lambda  m_2^2 \, \| \psi \|_g} \notag\\
&\leq  \int_0^{x^0} d \rho~(x^0-\rho) g(\rho) \int_0^{\max(0,y^0-\rho)} \!\!\! dr_2~r_2\, g(\rho+r_2)\nonumber\\
&+\int_0^{x^0} d \rho~(x^0-\rho)g(\rho) \int_{\max( 0,\rho-y^0)}^{\rho} \!\!\! dr_2~r_2 \, g(\rho-r_2).
\label{eq:a2estimate1}
\end{align}
Now we consider the $r_2$-integral in both terms and integrate by parts. 
This yields:
\begin{align}
   \int_0^{\max(0,y^0\!\!-\!\rho)} \!\!\! dr_2~r_2\, g(\rho\!+\!r_2)
   &=\max(0,y^0\!\!-\!\rho) g_1(y^0) \notag\\
   &\quad \quad \quad \!-\! g_2(\max(\rho,y^0)) \!+\! g_2(\rho),\\
\int_{\max( 0,\rho\!-\!y^0)}^{\rho} \!\!\! dr_2~r_2 \, g(\rho\!-\!r_2) \notag
&= \max( 0,\rho\!-\!y^0) g_1(y^0) \\
&\quad\quad  \quad \!+\! g_2(\min(\rho,y^0)).
\end{align}
We now use $- g_2(\max(\rho,y^0)) + g_2(\rho) \leq 0$ in the first term and 
then re-insert the resulting estimate into \eqref{eq:a2estimate1}. 
Considering also $\max(0,y^0-\rho) + \max( 0,\rho-y^0) = |y^0-\rho|$, this 
yields:
\begin{align}
  &|A_2 \psi|(x,y) \, \frac{16\pi}{\lambda \, m_2^2 \, \| \psi \|_g}\notag\\
  &\quad \leq  \int_0^{x^0} d \rho~(x^0-\rho) g(\rho) \left[|y^0-\rho| g_1(y^0) + g_2(\min(\rho,y^0)) \right]
\label{eq:a2estimate2}
\end{align}
The first summand of \eqref{eq:a2estimate2} can be treated as follows. First 
we focus on whether $x^0>y^0$ or $x^0\leq y^0$. In the first case, we then 
differentiate between the cases $\rho < y^0$ and $\rho \geq y^0$ and split 
up the integrals accordingly. This yields:
\begin{align}
	 &\int_0^{x^0} d \rho~(x^0-\rho) g(\rho) |y^0-\rho| g_1(y^0)\nonumber\\
&=~ g_1(y^0)\, H(x^0-y^0) \int_0^{y^0} d \rho~(x^0-\rho)(y^0-\rho) g(\rho)\label{eq:a2estimate2a}\\
&~~~ - g_1(y^0)\, H(x^0-y^0) \int_{y^0}^{x^0} d \rho~(x^0-\rho)(y^0-\rho) g(\rho)\label{eq:a2estimate2b}\\
& ~~~+ g_1(y^0)\, H(y^0-x^0) \int_0^{x^0} d \rho~(x^0-\rho)(y^0-\rho) g(\rho).
\label{eq:a2estimate2c}
\end{align}
We now calculate these terms separately using integration by parts. The first 
term yields:
\begin{align}
	\eqref{eq:a2estimate2a} ~&=~ g_1(y^0) H(x^0-y^0) \left[ (x^0-y^0) g_2(y^0)+ 2g_3(y^0)\right].
\label{eq:a2estimate2a2}
\end{align}
We turn to \eqref{eq:a2estimate2b}:
\begin{align}\notag
  \eqref{eq:a2estimate2b} = -g_1(y^0) H(x^0\!\!-\!y^0)\!&\big[ (y^0\!\!-\!x^0)(g_2(x^0)\\
  &\quad \!+\!g_2(y^0))\!+\!2g_3(x^0) \!-\! 2g_3(y^0)\big].
\label{eq:a2estimate2b2}
\end{align}
The result of \eqref{eq:a2estimate2c} is:
\begin{align}
	\eqref{eq:a2estimate2c} ~&=~ g_1(y^0) H(y^0-x^0) \left[ (y^0-x^0) g_2(x^0)+ 2g_3(x^0)\right].
	\label{eq:a2estimate2c2}
\end{align}
Gathering the terms \eqref{eq:a2estimate2a2}, \eqref{eq:a2estimate2b2} and 
\eqref{eq:a2estimate2c2} yields:
\begin{align}
  |A_2 \psi|(x,y)& \frac{16\pi}{\lambda \, m_2^2 \, \| \psi \|_g} \notag\\
  &\leq g_1(y^0) H(x^0\!\!-\!y^0) \left[ 2(x^0\!\!-\!y^0)g_2(y^0) \!+\! 4 g_3(y^0)\!-\!2g_3(x^0) \right]\nonumber\\
&~~~+g_1(y^0) |x^0\!\!-\!y^0| g_2(x^0) + 2 g_1(y^0) H(y^0\!\!-\!x^0) g_3(x^0)\nonumber\\
&\leq 2g_1(y^0)|x^0\!\!-\!y^0| g_2(x^0) + 2 g_1(y^0)g_3(x^0) H(x^0-y^0)\nonumber\\
&~~~+g_1(y^0)|x^0\!\!-\!y^0|g_2(x^0) + 2 g_1(y^0) g_3(x^0) H(y^0\!\!-\!x^0)\nonumber\\
&= 3g_1(y^0)|x^0\!\!-\!y^0| g_2(x^0) + 2 g_1(y^0)g_3(x^0)\nonumber\\
&\leq 3(x^0+y^0)g_1(y^0) g_2(x^0) + 2 g_1(y^0)g_3(x^0).
\end{align}
In order to obtain $\| A_2 \psi \|_g$, we divide by 
$g(x^0)g(y^0)$ and take 
the supremum over $x,y \in \tfrac{1}{2}\M$. This results in:
\begin{align}
  &\sup_{\psi \in \mathcal{S}((\frac{1}{2}\M)^2)} \frac{\| A_2 \psi \|_g}{\| \psi \|_g} \\
  &\leq~ \frac{\lambda \, m_2^2}{16\pi} \left( \!3\!\!\!\sup_{x^0,y^0 \geq 0}\!\!\! \frac{(x^0\!\!+\!y^0)g_2(x^0)\, g_1(y^0)}{g(x^0)g(y^0)}  \!+\! 2\!\!\! \sup_{x^0,y^0\geq 0}\!\! \frac{g_3(x^0)g_1(y^0)}{g(x^0)g(y^0)}\!\right).\notag
\end{align}
After factorizing the two-dimensional suprema 
into one-dimensional ones, 
this exactly yields the claim, \eqref{eq:estimatea2}.

For the operator $A_1$, we find analogously:
\begin{align}
  &\sup_{\psi \in \mathcal{S}((\frac{1}{2}\M)^2)} \frac{\| A_1 \psi \|_g}{\| \psi \|_g} \notag\\
  &\leq\! \frac{\lambda m_1^2}{16\pi} \left(\!\! 3\!\!\sup_{x^0,y^0 \geq 0}\!\!\!\! \frac{(x^0\!\!+\!y^0)g_1(x^0) g_2(y^0)}{g(x^0)g(y^0)}  
  \!+\! 2\!\!\! \sup_{x^0,y^0\geq 0}\!\! \frac{g_1(x^0)g_3(y^0)}{g(x^0)g(y^0)}\right).
\end{align}
which, after factorization into one-dimensional suprema, yields 
the claim 
\eqref{eq:estimatea1}.


%%%
\paragraph{Estimate of the mass-mass term \eqref{eq:estimatea12}.} 
\label{sec:estimatemassmass}

We begin with \eqref{eq:defa12}. Taking the absolute value and using 
$|\psi(x,y)| \leq \| \psi \|_g \, g(x^0) g(y^0)$ as well as 
$|J_1(t)/t|\leq \frac{1}{2}$ yields:
\begin{align}
&|A_{12} \psi|(x,y) \notag\\
&\leq \frac{\lambda \, m_1 m_2 \, \| \psi \|_g}{4(4\pi)^3}\!  \int_0^\infty\!\!\! d{x'}^0\!\! \int d^3 \!\vx' \!\int_0^\infty \!\!\!d{y'}^0 \!\int_0^{2\pi}\!\!\! d\varphi \!\int_{0}^{\pi} \!\!\!d \vartheta \, \cos(\vartheta) |{x'}^0\!\!-\!{y'}^0| \,  \nonumber\\
&\hspace{1cm}\times\! H(x^0\!\!-{x'}^0\!\!-\!|\vx-\vx'|)H(y^0\!\!-{y'}^0\!\!-\!|\vy\!-\!\vx'\!+\!\vz|)\notag\\
&\hspace{3cm} \times g({x'}^0)g({y'}^0)\Big|_{|\vz| = |{x^0}'-{y^0}'|},
\end{align}
where, we recall, $\vz$ is the variable for which the spherical 
coordinates are used.

Next, we consider the ranges of integration which the Heaviside 
functions imply. $H(x^0-{x'}^0-|\vx-\vx'|)$ restricts the range 
of integration of $\vx'$ to the ball $B_{x^0-{x'}^0}(\vx)$ and the 
range of the ${x'}^0$-integration to $(0,x^0)$. The range implied 
by the second Heaviside function is more complicated. We therefore 
use the estimate
\begin{equation}
	H(y^0-{y'}^0-|\vy-\vx'+\vz|) \leq H(y^0-{y'}^0).
\end{equation}
Then ${y'}^0 \in (0,y^0)$ and there is no further restriction for 
the angular variables. We obtain:
\begin{align}
|A_{12} \psi|(x,y) &\leq \frac{\lambda \, m_1 m_2 \, \| \psi \|_g}{8(4\pi)^3}  \int_0^{x^0} d{'}^0 \int_{B_{x^0-{x'}^0}(\vx)} \!\!\!\!\!\!\!\!\!\! d^3 \vx' \int_0^{y^0} d{y'}^0 \int_0^{2\pi} d\varphi \int_{0}^{\pi} d \vartheta \nonumber\\
&~~~\times \cos(\vartheta) |{x'}^0-{y'}^0| \, g({x'}^0)g({y'}^0).
\end{align}
Performing the $\vx'$-integration, as well as the angular integrals 
yields:
\begin{align}
  &|A_{12} \psi|(x,y) \notag\\
  &\leq \frac{\lambda \, m_1 m_2 \, \| \psi \|_g}{96\pi}\!  \int_0^{x^0}\!\! d{x'}^0 |x^0\!\!-\!{x'}^0|^3 g({x'}^0)\! \int_0^{y^0}\!\! d{y'}^0 |{x'}^0\!\!-\!{y'}^0| \, g({y'}^0).
\end{align}
Our next task is to estimate the term explicitly in terms of the 
functions $g_n$ only. To do so, we use
\begin{equation}
	|{x'}^0-{y'}^0| \leq {x'}^0 + {y'}^0.
\end{equation}
This yields:
\begin{align}
  &|A_{12} \psi|(x,y) \notag\\
  &\leq \!\frac{\lambda  m_1 m_2  \| \psi \|_g}{48\pi}\!  \int_0^{x^0} \!\!\!d{x'}^0 |x^0\!\!-\!{x'}^0|^3 g({x'}^0) \!\!\int_0^{y^0} \!\!\!d{y'}^0 ({x'}^0 \!\!+\! {y'}^0) g({y'}^0).
\label{eq:a12estimatecalc}
\end{align}
Let
\begin{equation}
	I(x^0,y^0) = \int_0^{x^0} d{x'}^0 |x^0-{x'}^0|^3 g({x'}^0) \int_0^{y^0} d{y'}^0~ ({x'}^0 + {y'}^0) g({y'}^0)
\end{equation}
and
\begin{equation}
	L({x'}^0,y^0) = \int_0^{y^0} d{y'}^0~ ({x'}^0 + {y'}^0) g({y'}^0).
\end{equation}
Integration by parts yields:
\begin{align}
  L({x'}^0,y^0) =& {x'}^0 g_1(y^0) \!+\! y^0 g_1(y^0) \!-\! g_2(y^0) \notag\\
  \leq& {x'}^0 g_1(y^0) \!+\! y^0 g_1(y^0).
\end{align}
Next, let
\begin{align}
	I_a(x^0) &= \int_0^{x^0} d {x'}^0~|x^0-{x'}^0|^3 g({x'}^0),\nonumber\\
	I_b(x^0) &=  \int_0^{x^0} d {x'}^0~{x'}^0 |x^0-{x'}^0|^3 g({x'}^0).
\end{align}
Then:
\begin{equation}
	I(x^0,y^0) \leq I_a(x^0) \, y^0 g_1(y^0) + I_b(x^0) \, g_1(y^0).
\label{eq:massmassintparts}
\end{equation}
We consider $I_a$ first, using $(x^0-{x'}^0)^2 \leq (x^0)^2$ and 
integrating by parts:
\begin{align}
	I_a(x^0) &\leq (x^0)^2 \int_0^{x^0} d {x'}^0~(x^0-{x'}^0)g({x'}^0)\\
&= (x^0)^2 \left( \underbrace{(x^0-{x'}^0)g_1({x'}^0)|_{{x'}^0 = 0}^{x^0}}_{=0} + g_2(x^0)\right) = (x^0)^2 g_2(x^0).\nonumber
\end{align}
We turn to $I_b$, using ${x'}^0(x^0-{x'}^0)\leq \frac{1}{4}(x^0)^2$ 
and integrating by parts twice. This results in:
\begin{align}
	I_b(x^0) ~\leq~ \frac{(x^0)^2}{4} \int_0^{x^0} d{x'}^0~ (x^0-{x'}^0)^2 g({x'}^0) ~=~ \frac{(x^0)^2}{2}\, g_3(x^0).
\end{align}
Considering \eqref{eq:massmassintparts}, we therefore obtain:
\begin{equation}
	I(x^0,y^0) ~\leq~ (x^0)^2 g_2(x^0)\, y^0 g_1(y^0) + \frac{(x^0)^2}{2}\, g_3(x^0)\, g_1(y^0).
\label{eq:resultmassmasstermgeneralg}
\end{equation}
Returning to \eqref{eq:a12estimatecalc}, we divide by $g(x^0)g(y^0)$ 
and take the supremum, with the result:
\begin{align}
	\sup_{\psi \in \mathcal{S}((\frac{1}{2}\M)^2)} \frac{\| A_{12} \psi \|_g}{\| \psi \|_g} ~&\leq~\frac{\lambda \, m_1 m_2 \, \| \psi \|_g}{96\pi} \left[ \sup_{x^0,y^0\geq 0} \frac{(x^0)^2 g_2(x^0)\, y^0 g_1(y^0)}{g(x^0)g(y^0)} \right.\nonumber\\
&~~~\left.+ \frac{1}{2} \sup_{x^0,y^0\geq 0} \frac{(x^0)^2 g_3(x^0)\, g_1(y^0)}{g(x^0)g(y^0)} \right].
\end{align}
Factorizing the two-dimensional suprema into one-dimensional ones 
yields the claim, \eqref{eq:estimatea12}.

%%%%%
\subsubsection{Proof of Theorem \ref{thm:exponentialg}} 
\label{sec:proofexponentialg}

Let $\psi \in \mathcal{S}$. It only remains to calculate the 
supremum in \eqref{eq:estimatea0} for $g(t)=e^{\gamma t}$. We have:
\begin{equation}
	g_1(t) = \frac{1}{\gamma} \left( e^{\gamma t} - 1\right)
\end{equation}
and hence
\begin{align}
  \sup_{\psi \in \mathcal{S}((\frac{1}{2}\M)^2)} &\frac{\| A_0 \psi \|_g}{\| \psi \|_g} \leq \frac{\lambda}{8\pi} \left( \sup_{t \geq 0} \frac{g_1(t)}{g(t)} \right)^2 \notag\\
  &= \frac{\lambda}{4\pi} \left( \sup_{t \geq 0} \frac{1}{\gamma} (1 - e^{-\gamma t}) \right)^2 = \frac{\lambda}{8\pi \gamma^2}. 
\end{align}
This shows that $A_0$ can be linearly extended to a bounded operator 
on $\Banach_g$ which satisfies the same estimate, 
\eqref{eq:norma0exponential}. Moreover, for $\gamma > 
\sqrt{\frac{\lambda}{4\pi}}$, $A_0$ is a contraction and Banach's 
fixed point theorem implies the existence of a unique solution 
$\psi \in \Banach_g$ of the equation $\psi = \psi^\free + A_0 \psi$ 
for every $\psi^\free \in \Banach_g$.


%%%%%
\subsubsection{Proof of Theorem \ref{thm:existence}} \label{sec:proofexistence}

Let again $\psi \in \mathcal{S}$. We need to calculate the suprema 
in \eqref{eq:estimatea0}-\eqref{eq:estimatea12} for 
$g(t)=(1+\alpha t^2)e^{\alpha t^2/2}$. We first note:
\begin{align}
	g_1(t) ~&=~ t e^{\alpha t^2/2},\nonumber\\
	g_2(t) ~&=~\frac{1}{\alpha} \left( e^{\alpha t^2/2}-1 \right),\nonumber\\
	g_3(t) ~&=~\frac{1}{\alpha} \left[ \sqrt{\frac{\pi}{2\alpha}} \erfi(\sqrt{\alpha/2} t)-t \right].
\end{align}
We can see that with each successive integration, the functions 
$g_n$ grow slower as $t\rightarrow \infty$. Furthermore, the leading 
terms in $g_n$ are inversely proportional to increasing powers of 
$\alpha$. These two properties (and of course the fact that $g_1, 
g_2,g_3$ can be written down in terms of elementary functions) make 
this particular function $g(t)$ a suitable choice for the proof.

As we need to estimate the behavior of quotients like $g_3(t)/g(t)$ 
for $t\rightarrow \infty$, we look for a simpler estimate of $g_3$ 
in terms of exponential functions. We note:
\begin{align}
	g_3(t) ~&=~ \int_0^t dt'\,  \frac{1}{\alpha} \left( e^{\alpha {t'}^2/2} -1\right)\nonumber\\
	&\le~ \frac{e^{\alpha t^2/2}}{\alpha} e^{-\alpha t^2/2} \sqrt{2/\alpha} \int_0^{\sqrt{\alpha/2} t}  d\tau  \,e^{\tau^2}\nonumber\\
	&=~\frac{\sqrt{2}}{\alpha^{3/2}} \, e^{\alpha t^2/2} \, D(\sqrt{\alpha/2}\,t),
\end{align}
where $D(t) = e^{-t^2}\int_0^{t}  d\tau \, e^{\tau^2}$ denotes the 
Dawson function.
Using the property \(|tD(t)|<\frac{2}{3}\), we obtain:
\begin{equation}
	t g_3(t)~\leq~\frac{4}{3} \frac{e^{\alpha t^2/2}}{\alpha^{2}}.
\label{eq:g3estimate}
\end{equation}
We are now well-equipped to calculate the suprema occurring in 
\eqref{eq:estimatea0}-\eqref{eq:estimatea12}. Using
\begin{equation}
 \sup_{t\geq 0}\frac{t^\beta}{1+t^2} ~=~ \left\{ \begin{matrix} 1\quad \text{ for } \beta=0\\ \frac{1}{2} \quad \text{ for } \beta=1 \\ 1\quad \text{ for } \beta=2 \end{matrix}\right. 
\end{equation}
we obtain:
\begin{align}
	\sup_{t\geq 0} \frac{g_1(t)}{g(t)} ~&=~ \sup_{t\geq 0} \frac{t}{1+\alpha t^2} ~=~ \frac{1}{2}\frac{1}{\sqrt{\alpha}},\label{eq:sup1}\\
	\sup_{t\geq 0} \frac{t g_1(t)}{g(t)} ~&=~ \sup_{t\geq 0} \frac{t^2}{1+\alpha t^2} ~=~ \frac{1}{\alpha},\label{eq:sup2}\\
	\sup_{t\geq 0} \frac{g_2(t)}{g(t)} ~&\leq~ \sup_{t\geq 0} \frac{1}{\alpha} \frac{1}{1+\alpha t^2} ~=~ \frac{1}{\alpha},\label{eq:sup3}\\
\sup_{t\geq 0} \frac{tg_2(t)}{g(t)} ~&\leq~ \sup_{t\geq 0} \frac{1}{\alpha} \frac{t}{1+\alpha t^2} ~=~ \frac{1}{2}\frac{1}{\alpha^{3/2}},\label{eq:sup4}\\
\sup_{t\geq 0} \frac{t^2g_2(t)}{g(t)} ~&\leq~ \sup_{t\geq 0} \frac{1}{\alpha} \frac{t^2}{1+\alpha t^2} ~=~ \frac{1}{\alpha^2}\label{eq:sup5}.
\end{align}
Using, in addition, the property $|D(t)| < \frac{3}{5}$, we find:
\begin{align}
	\sup_{t\geq 0} \frac{g_3(t)}{g(t)} ~&\leq~ \sup_{t\geq 0} \frac{\sqrt{2}}{\alpha^{3/2}} \frac{D(\sqrt{\alpha/2}t)}{1+\alpha t^2} ~=~ \frac{3\sqrt{2}}{5}\frac{1}{\alpha^{3/2}} ~<~ \frac{1}{\alpha^{3/2}},\label{eq:sup6}\\
\sup_{t\geq 0} \frac{t^2 g_3(t)}{g(t)} ~&\leq~ \sup_{t\geq 0} \frac{4}{3}\frac{1}{\alpha^2} \frac{t}{1+\alpha t^2} ~=~\frac{2}{3}\frac{1}{\alpha^{5/2}}. \label{eq:sup7}
\end{align}
In the last line, we have made use of \eqref{eq:g3estimate}.

With these results, we find for $A_0$:
\begin{equation}
	\eqref{eq:estimatea0} ~\leq~ \frac{\lambda}{8\pi} \left(\frac{1}{2} \frac{1}{\sqrt{\alpha}}\right)^2 ~=~  \frac{\lambda}{32\pi} \frac{1}{\alpha}.
\end{equation}
This yields \eqref{eq:estimatea0final}.

We continue with $A_1$.
\begin{align}\notag
  \eqref{eq:estimatea1} \leq \frac{\lambda  m_1^2}{16\pi} \left[  3  \frac{1}{\alpha}  \frac{1}{\alpha} + 3  \frac{1}{2} \frac{1}{\sqrt{\alpha}}  \frac{1}{2} \frac{1}{\alpha^{3/2}} +2  \frac{1}{2} \frac{1}{\sqrt{\alpha}}  \frac{1}{\alpha^{3/2}} \right] \\
  = \frac{\lambda  m_1^2}{16\pi} \frac{19}{4} \frac{1}{\alpha^2} ~<~\frac{\lambda  m_1^2}{16\pi} \frac{5}{\alpha^2}.
\end{align}
This yields \eqref{eq:estimatea1final}. Analogously, we obtain the 
estimate \eqref{eq:estimatea2final} for $A_2$.

Finally, for $A_{12}$, we have
\begin{align}
  \eqref{eq:estimatea12} \leq \frac{\lambda  m_1^2 m_2^2}{96 \pi} \left[ \frac{1}{\alpha^2}  \frac{1}{\alpha} + \frac{1}{2}  \frac{2}{3} \frac{1}{\alpha^{5/2}}  \frac{1}{2} \frac{1}{\sqrt{\alpha}} \right] \notag\\
  = \frac{\lambda  m_1^2 m_2^2}{96\pi}  \frac{7}{6} \frac{1}{\alpha^3} < \frac{\lambda  m_1^2 m_2^2}{80 \pi}  \frac{1}{\alpha^3},
\end{align}
which yields \eqref{eq:estimatea12final}.

Now, the estimates \eqref{eq:estimatea0final}-
\eqref{eq:estimatea12final} show that the operators 
$A_0$, $A_1$, $A_2$ and $A_{12}$ are bounded on test functions. 
Thus, they can be linearly extended to bounded operators on 
$\Banach_g$ with the same bounds.

The operator $A = A_0 + A_1 + A_2 + A_{12}$ then also defines a 
bounded linear operator on $\Banach_g$ with norm
\begin{equation}
	\| A \| ~\leq~ \| A_0 \| + \| A_1 \| + \| A_2 \| + \| A_{12} \|.
\end{equation}
Using the previous results \eqref{eq:estimatea0final}-
\eqref{eq:estimatea12final}, we obtain:
\begin{equation}
	\|A \| ~\leq~ \frac{\lambda}{8\pi \alpha} \left( \frac{1}{4} + \frac{5(m_1^2 + m_2^2)}{2} \frac{1}{\alpha} + \frac{m_1^2 \, m_2^2}{10} \frac{1}{\alpha^2} \right).
\end{equation}
If $\alpha$ is chosen such that this expression is strictly smaller 
than unity, $A$ becomes a contraction and the existence and 
uniqueness of solutions of the equation $\psi = \psi^\free + A\psi$ 
follows. This yields condition \eqref{eq:condexistencemassive} and 
ends the proof.

%%%%%
\subsubsection{Proof of Theorem \ref{thm:existencecurved}} 
\label{sec:proofexistencecurved}

The proof can be reduced to the one for $\tfrac{1}{2}\M$. To do so, 
we take the absolute value of \eqref{eq:defa0tilde} and use 
$|\psi|(\eta_1,\vx,\eta_2,\vy) \leq g(\eta_1) g(\eta_2) \|\psi\|_g$. 
With
\begin{align}
	G(\eta) ~&=~ a(\eta) \exp \left(\gamma \int_0^\eta d\eta'~a(\eta') \right)	\label{eq:defG}\\
	G_1(\eta)~&=~ \int_0^\eta d\eta'~G(\eta)
\end{align}
we obtain the estimate
\begin{align}\notag
    &|\widetilde{A}_0\psi|(x,y) \\
    &\le \frac{\lambda \|\psi\|_g}{4(4\pi)^3}\!\! \int_{B_{\eta_2}(\vy)}\!\!\!\!\!d^3 \vy' \!\!\int_0^{2\pi} \!\!\!d\varphi \int_{-1}^1 \!\!\!d\cos\vartheta \, \frac{|b^2|}{(b^0\!\!+\!|\vb|\cos\vartheta)^2 |\vy'|} G(\eta_2\!-\!|\vy'|)\nonumber\\
    &\times G\left(\eta_1-\frac{1}{2}\frac{b^2}{b^2+|\vb|\cos\vartheta}\right)\notag\\
    &\times \left(1_{b^2>0}1_{b^0>0} 1_{\cos\vartheta > \frac{b^2}{2\eta_1^0|\vb|} - \frac{b^0}{|\vb|}}+1_{b^2<0}1_{\cos\vartheta<\frac{b^2}{2\eta_1|\vb|} - \frac{b^0}{|\vb|}}\right).
\label{eq:a0tildecalc01}
\end{align}
This estimate is identical to \eqref{eq:a0calc01} with the only 
difference that the function $g$ is exchanged with $G$ in the 
integral (but not in $\| \cdot\|_g$). Thus, going through the same 
steps as in Secs. \ref{sec:proofbounds}, \ref{sec:proofexistence}, 
we obtain:
\begin{equation}
	\sup_{\psi \in \mathcal{S}\left(([0,\infty)\times \R^3)^2\right)} \frac{\| \widetilde{A}_0 \psi \|_g}{\| \psi \|_g} ~\leq~ \frac{\lambda}{8\pi} \left(\sup_{t\geq 0} \frac{G_1(t)}{g(t)}\right)^2.\label{eq:estimatea0tilde}
\end{equation}
Now, recalling $g(t) = \exp\left(\gamma \int_0^t d\tau \, 
a(\tau)\right)$ we have
\begin{equation}
	G_1(t) = \frac{1}{\gamma} g(t)
\end{equation}
and it follows that
\begin{equation}
	\sup_{\psi \in \mathcal{S}\left(([0,\infty)\times \R^3)^2\right)} \frac{\| \widetilde{A}_0 \psi \|_g}{\| \psi \|_g} ~\leq~ \frac{\lambda}{8\pi \gamma^2},
\end{equation}
which yields \eqref{eq:a0tildebound}. The rest of the claim follows 
as before.


%%%%%
\subsection{Conclusions}
\label{sec:conclusions}

In this paper we have given what we think of as a satisfactory 
answer to the problem posed: to prove the existence and 
uniqueness of solution of the integral equation 
\eqref{eq:inteq} and its $N$-particle generalization 
\eqref{eq:npartint}. Following previous works, we have 
assumed a cutoff in time. By considering an example for our 
integral equation on a cosmological spacetime with a Big Bang 
singularity, we have shown that such a cutoff can arise 
naturally and without violating any spacetime symmetries.

Our work provides a rigorous proof of the existence of 
interacting relativistic quantum dynamics in 1+3 spacetime 
dimensions; in particular, our model does not suffer from 
ultraviolet divergences which are typically encountered in 
quantum field theoretic models. Of course, our model does not 
describe particle creation and annihilation and is therefore a 
toy model rather than an alternative to QFT. Nevertheless, we 
find the fact that direct interactions, even singular ones 
along the light cone, can be made mathematically rigorous, 
remarkable. We wonder whether in the long run the mechanism of 
interaction through multi-time integral equations and direct 
interactions could contribute to a rigorous formulation of 
quantum field theory.

In the more immediate future, it would first of all be 
desirable to extend our results to Dirac particles (meaning 
that the Green's functions in \eqref{eq:inteq} are replaced 
with Green's functions of the Dirac equation). As the Dirac 
Green's functions involve distributional derivatives, it 
would then be more difficult than in the Klein-Gordon case to 
define the combination of the three distributions 
$G_1^\ret$, $G_2^\ret$ and $\delta((x-y)^2)$ which occur 
in \eqref{eq:inteq}. Moreover, as the previous work 
\cite{selfDirac} on Dirac particles but regular interaction 
kernels $K$ suggests, the occurrence of the distributional 
derivatives in the Green's functions alone leads to 
technical complications, as one then needs to prove a higher 
regularity of the solutions. In the $N$-particle case, this 
regularity would have to be greater than in the two-particle 
case so that one cannot simply add up estimates for the norm 
of the two-particle integral operator to obtain an estimate 
for the $N$-particle integral operator anymore. There would 
be further terms to consider.

This is the set of questions which a work on the Dirac case 
of Eq.\@ \eqref{eq:inteq} would have to answer.

Besides the Dirac case, there is also a range of more detailed 
technical questions for the Klein-Gordon case which would be 
interesting to address. While we have here worked with a 
weighted $L^\infty$ norm both for time and space variables, 
one could also try to use a weighted $L^\infty L^2$ norm 
instead ($L^\infty$ for the time variables and $L^2$ for the 
space variables). It would then be a challenging task to 
find the right inequalities to obtain similar estimates as we 
did. Moreover, one could also try to prove higher regularity 
not only in the sense of integrability but also 
differentiability. An interesting question, for example, 
is whether one can apply the Klein-Gordon operators 
$(\Box_k + m_k^2)$ to the solutions of \eqref{eq:inteq} in 
a weak sense. For the Dirac case, an analogous property was, 
in fact, established in \cite{selfDirac}. 















\chapter[Quantum Field Theoretic Approach to Interactions][Interaction in QFT]{Quantum Field Theoretic Approach to Interactions} \label{sec:QFT}

\section{Introduction}

\todo{thorough introduction to Franz and Dirk stuff for handling the geometry part as independent chapter, 
orientiere dich an perspective of external field qed paper}



\section{The Relationship Between Hadamard States and Admissible Polarisation Classes}
\todo{insert QFT communications paper}



\section{Analyticity of the One Particle Scattering Operator}

In this section we analyse the construction of the one particle 
scattering operator \(S_A\)
carried out in \cite{ivp0} and answer the question whether
operators like
\begin{equation}
P^+ \partial_B S_{A}^* S_{A+B}P^-
\end{equation}
are Hilbert-Schmidt operators. This will turn out to be important 
for the geometric construction carried out in 
chapter \ref{chapter geometery}. 

Since this section is heavily inspired from \cite{ivp0}, we need
to introduce some notation from this paper. 
\begin{Def}
We define the set \(\mathcal{V}\) of four potentials
\begin{equation}
\mathcal{V}:= C_c^\infty(\mathbb{R}^4,\mathbb{R}^4).
\end{equation}
Let \(A\in\mathcal{V}\), we define the the integral operator
\(Q^A:\mathcal{H}\righttoleftarrow\) by giving its integral kernel, which 
is also denoted by \(Q^A\):
\begin{align}
\mathbb{R}^3\times\mathbb{R}^3\ni (p,q)\mapsto Q^A(p,q)
:=\frac{Z^A_{+-}(p,q)-Z^A_{-+}(p,q)}{i(E(p)+E(q))}\\
\text{with }Z^A_{\pm\mp}(p,q):=P_\pm (p)Z^A(p-q)P_\mp (q),\\
Z^A=-i e\gamma^0 \gamma^\alpha \hat{A}_\alpha,\\
%\hat{A}_\mu(t):\mathcal{H} \righttoleftarrow, \hat{A}_\mu(t)\psi:= \hat{A}_\mu(t) \ast \psi,\\
\hat{A}_\mu := \frac{1}{(2\pi)^{3/2}}\int_{\mathbb{R}^3} A_\mu (x)e^{-i p x} d^3x,\\ 
\text{and } E(p):=\sqrt{m^2+|p|^2}.
\end{align}

We introduce the standard polarisation of \(\mathcal{H}\) for the free Dirac 
equation\todo{think of some more intuitive notation for the projector.}
\begin{align}
P^-:=1_{\mathrm{spec}(H^0)<0}, \quad P^+=1-P^-.
\end{align}

Lastly we introduce the partial derivative in 
the direction of any four-potential \(F\) of an operator valued 
function \(F:\mathcal{V}\rightarrow \mathcal{B}(\mathcal{F})\) by
\begin{equation}\label{def derivative}
\partial_F T(F):=\partial_{\varepsilon}T(\varepsilon F)|_{\varepsilon =0},
\end{equation}
where the limit is taken with respect to the operator norm topology.
\end{Def}



\begin{Lemma}
For general \(A,F\in\mathcal{V}\) and \(t_0,t_1\in\mathbb{R}\) 
we have the well known equations for
the one-particle time evolution operators

\begin{align}
U^A(t_1,t_0)=U^0(t_1,t_0) + \int_{t_0}^{t_1}dt~ U^0(t_1,t) Z^A(t) U^{A}(t,t_0)\\
U^{A+F}(t_1,t_0)=U^A(t_1,t_0) + \int_{t_0}^{t_1}dt ~U^A(t_1,t) Z^F(t) U^{A+F}(t,t_0).
\end{align}
\end{Lemma}
\begin{proof}
\dots
\end{proof}

\begin{Def}
For any \(A\in\mathcal{V}\), we introduce the 
integral operator 
\({Q'}^A:\mathcal{H}\righttoleftarrow\) by it's kernel
\begin{equation}
\mathbb{R}\times\mathbb{R}^3\times\mathbb{R}^3
\ni(t,p,q)\mapsto {Q'}^A(t,p,q)=\partial_t Q^A(t,p,q),
\end{equation}
where the time dependence is due to the time dependence of the 
four-potential \(A\).
The following notion of even and odd part of 
an arbitrary bounded linear operator 
\(T:\mathcal{F}\righttoleftarrow\) on 
Fock space will come in handy:
\begin{align}
T_{\mathrm{odd}}&:=P^+TP^- + P^-TP^+\\
T_{\mathrm{ev}}&:=P^+TP^+ + P^-TP^-.
\end{align}

Additionally, we define the norm 
\begin{equation}
    T:\mathcal{H}\righttoleftarrow \|T\|_{\mathrm{op}+I_2}=\|T\|+\|T_{\odd}\|_{I_2},
\end{equation}
where \(\|\cdot\|\) is the operator norm and \(\|\cdot\|_{I_2}\) is the 
Hilbert-Schmidt norm and the space
\begin{equation}
  I_2^{\odd}:=\{T:\mathcal{F}\rightarrow \mathcal{F}\mid
  \|T\|<\infty, \|T_{\odd}\|_{I_2}<\infty\}.
\end{equation}

\end{Def}




\begin{Lemma}\label{completeness of I_2 odd}
The space \(I_2^{\odd}\)
equipped with the norm 
\(\|\cdot\|_{\mathrm{op}+I_2}\) is a 
Banach space.
\end{Lemma}
\begin{proof}
Let \((T_n)_{n\in\mathbb{R}}\subset I_2^{\odd}\)
be a Cauchy sequence with 
respect to \(\left\|\cdot\right\|_{\mathrm{op}+I_2}\). 
Then it follows directly that 
\((T_n)_{n\in\mathbb{N}}\)
is also a Cauchy sequence with 
respect to \(\|\cdot\|\) and
\((T_{n,\odd})_{n\in\mathbb{N}}\) is a Cauchy
sequence with respect to
\(\|\cdot\|_{I_2}\). Since 
the space of bounded operators equipped with
\(\|\cdot\|\) and
the space of Hilbert-Schmidt oeprators 
equipped with \(\|\cdot\|_{I_2}\) both are
complete we have

\begin{align}
  T_n\xrightarrow[\|\cdot\|]{n\rightarrow \infty} T^1\\
  T_{n,\odd}\xrightarrow[\|\cdot\|_{I_2}]{n\rightarrow \infty} T^2
\end{align}
for some bounded operator \(T^1\) and some 
Hilbert-Schmidt operator \(T^2\). Now because 
the Hilbert-Schmidt norm  fulfills
\begin{equation}
\|T\|\le \|T\|_{I_2},
\end{equation} 
we obtain directly
\begin{equation}
  T_{n,\odd}\xrightarrow[\|\cdot\|]{n\rightarrow \infty} T^2,
\end{equation}
hence \(T^1_{\odd}=T^2\). 
Therefore, \(T^1\in I_2^{\odd}\) holds. 
Finally, since 
\(\|\cdot\|_{\mathrm{op}+I_2}
=\|\cdot\|+\|\cdot_{\odd}\|_{I_2}\) is true, 
we find 
\begin{equation}
T_n\xrightarrow[\|\cdot\|_{\mathrm{op}+I_2}]{n\rightarrow \infty} T^1,
\end{equation}
proving completeness.
\end{proof}


\begin{Thm}[Smoothness of S]\label{thm smoothness of S}
  Let \(n\in\mathbb{N}\),  \(A,H_{k}\in\mathcal{V}\) for 
  \(k\le n\), pick \(t_1\) 
  after \(\supp A\cup \bigcup_{k\le n}\supp H_k\) and \(t_0\)
  before \(\supp A\cup\bigcup_{k\le n}\supp H_k\) then 
  the derivative 
  \begin{equation}
    \partial_{H_1}\dots \partial_{H_k}U^{A+\sum_{b=1}^k H_b}(t_1,t_0)
  \end{equation}
  exists with respect to the topology induced by 
  the norm \(\|\cdot\|_{\mathrm{op}+I_2}\).
\end{Thm}
\begin{proof}
  Following \cite{ivp0} throughout this proof, we make use of the 
  following shorthand notation. For operator valued maps 
  \(T_1,T_2:\mathbb{R}^2\to \mathcal{B}(\mathcal{H}) \) we define 
  for \(t_1,t_0\in\mathbb{R}\)
  \begin{equation}
  T_1T_2 = \int_{t_0}^{t_1}dt~ T_1(t_1,t)T_2(t,t_0),
  \end{equation}
  as a map of the same type as \(T_1\) and \(T_2\)
  whenever this is well defined. Furthermore, for operator valued functions
  \(W_1,W_2:\mathbb{R}\to \mathcal{B}(\mathcal{H})\) we define
  \begin{align}\label{ivp0 shortnotation 1}
  T_1W_1~(t',t)=T_1(t',t)C_1(t)\\\label{ivp0 shortnotation 2}
  W_1 T_1~(t',t)=W(t_1)T_1(t',t)\\
  W_1W_2~(t)=W_1(t)W_2(t),
  \end{align}
  as maps of the same type as \(T_1, T_1\) and \(C_1\) 
  respectively.

  Pick \(k\in\mathbb{N}\), \(A,H_b\in\mathcal{V}\) for \(b\le k\) 
  and \(t_1,t_0\) as in the theorem. Whenever the shorthand 
  \eqref{ivp0 shortnotation 1} and \eqref{ivp0 shortnotation 1}
  is used without specific arguments, by convention 
  \(t'=t_1,t=t_0\).  We abbreviate
  \begin{equation}
  H:=\sum_{b=1}^k H_b, \quad B:=A+H.
  \end{equation}
  We introduce 
  \begin{equation}
    R^B(t',t)=(1-Q^B)U^B(1+Q^B)(t',t),
  \end{equation}
  for general \(t',t\in\mathbb{R}\).
  Because of the choice of \(t_1,t_0\) we have
  \begin{equation}\label{def R}
  R^B(t_1,t_0)=(1-Q^B)U^B(1+Q^B)=U^B(t_1,t_0),
  \end{equation}
  because \(B=0\) both at \(t_1\) and \(t_0\).
  
  So it suffices to study the family of operators \(R^B\). 
  As shown in the proof of \cite[lemma 3.5]{ivp0}
  \(R^B\) for \(B\in\mathcal{V}\) is the limit in the sense of the 
  operator norm of the sequence
  \begin{align}
  R^B_{0}:=0, \quad R^B_{n+1}:=U^0 F^B R^B_{n}+ U^0 + G^B,
  \end{align}
  where \(F\) and \(G\) are given By
  
  \begin{align}
  F^B:=& (-{Q'}^B+Z^B_{\ev}-Q^BZ^B)(1+Q^B),\\
  G^B:=&- U^0Q^BQ^B \\
  +& U^0(-{Q'}^B+Z_{\ev}-Q^B Z^B)Q^B Q^B U^B (1+Q^B).
  \end{align}
  
  First we introduce the auxiliary norms for operators \(T\) 
  and \(W\) depending on one and two 
  scalar variables respectively.
  
  \begin{align}
    \|T \|_{\mathrm{op}+I_2,\gamma}
    &:=\sup_{t\in[t_1,t_0]}e^{-\gamma (t-t_0)} \left(\|T(t)\|
    +\| T_{\odd}(t)\|_{I_2}\right)\\
    \|T \|_{\gamma}&:=\sup_{t\in[t_1,t_0]}e^{-\gamma (t-t_0)} \|T(t)\|\\
    \|W \|_{0}&:=\sup_{t,t'\in[t_1,t_0]} \|W(t,t')\|\\
    \|T \|_{I_2 ,\gamma}&:=\sup_{t\in[t_1,t_0]}e^{-\gamma (t-t_0)} \| T(t)\|_{I_2},\\
    \|W \|_{I_2 ,0}&:=\sup_{t,t'\in[t_1,t_0]} \| W(t,t')\|_{I_2},
  \end{align}
  for \(\gamma\ge 0 \).
  In the proof of the same lemma in \cite[equation (3.42)]{ivp0}
  we also have the recursive equation 
  \begin{align}
  R^B_{n}=U^0F^B_{\ev}R^B_{n-1}+U^0F^B_{\odd} U^0F^B R^B_{n-2}\\
  +U^0 F^B_{\odd} +U^0F^B_{\odd} U^0 + U^0 +G^B
  \end{align}
  is fulfilled by the same sequence of operators for \(n\ge 2\).
  Furthermore, we introduce the notation
  \begin{align}\label{derivative set 1}
  [k]&=\{l\in\mathbb{N}\mid l\le k\}\\\label{derivative set 2}
  \forall u\subseteq [k]:\partial_{u}&=\prod_{k\in u}\partial_{H_k},\\
  \Delta^n&=R^B_{n+1}-R^B_n,
  \end{align}
  where the product of derivatives is to be understood as the 
  mixed derivative with respect to all the factors.

  Hence we have for such \(n\):
  \begin{align}\notag
    \Delta_{n}=&U^0F^B_{\ev}\Delta_{n-1}
    +U^0F^B_{\odd} U^0F^B \Delta_{n-2}.
  \end{align}
  Abbreviating \(U^0F^B_{\ev}=:a,\quad U^0F^B_{\odd}U^0 F^B:=b\), we obtain
  \begin{align}\label{R recurisve}
    \Delta_{n}=a\Delta_{n-1}
    +b \Delta_{n-2}.
  \end{align}

  Using the derivative defined in 
  \eqref{def derivative} and the recursion 
  \eqref{R recurisve},
  we estimate for any set \(u\subset [k]\):
  
  \begin{align}
  &\sup_{p\subseteq u}\|\partial_p \Delta^n_{\odd} \|_{I_2,\gamma}\\
  &\le \sup_{p\subseteq u} \sup_{t\in[t_0,t_1]} e^{-\gamma(t-t_0)}
  \left\| \sum_{w\subseteq u} (\partial_{u\backslash w}a ~
  \partial_w \Delta^{n-1}_{\odd})(t,t_0)\right\|_{I_2}\\
  &+\sup_{p\subseteq u} \sup_{t\in[t_0,t_1]} e^{-\gamma(t-t_0)}
  \left\|P^+ \sum_{w\subseteq u} (\partial_{u\backslash w}b ~
  \partial_w \Delta^{n-2})(t,t_0) P^-\right\|_{I_2}\\
  &+\sup_{p\subseteq u} \sup_{t\in[t_0,t_1]} e^{-\gamma(t-t_0)}
  \left\|P^- \sum_{w\subseteq u} (\partial_{u\backslash w}b ~
  \partial_w \Delta^{n-2})(t,t_0) P^+\right\|_{I_2}\\
  &\le 
  \sup_{p\subseteq u} \sup_{t\in[t_0,t_1]} e^{-\gamma(t-t_0)} 
  \sum_{w\subseteq u} 
  \| \partial_{u\backslash w}(a ~
  \partial_w \Delta^{n-1}_{\odd})(t,t_0)\|_{I_2}\\
  &+2\sup_{p\subseteq u} \sup_{t\in[t_0,t_1]} 
  e^{-\gamma(t-t_0)} \sum_{w\subseteq u} 
  \|  (\partial_{u\backslash w}b ~
  \partial_w \Delta^{n-2})(t,t_0) \|_{I_2}\\
  &\le
  \sup_{p\subseteq u}\sup_{t\in[t_0,t_1]} e^{-\gamma(t-t_0)} 
  \sum_{w\subseteq u} 
  \int_{t_0}^{t}dt' 
  \| \partial_{u\backslash w}a(t,t') ~
  \partial_w \Delta^{n-1}_{\odd}(t',t_0)\|_{I_2}\\
  &+2 \sup_{p\subseteq u}
  \sup_{t\in[t_0,t_1]} e^{-\gamma(t-t_0)}\sum_{w\subseteq u} 
  \int_{t_0}^{t}dt'  
  \|  \partial_{u\backslash w}b(t,t') ~
  \partial_w \Delta^{n-2}(t',t_0) \|_{I_2}\\
  &\le
  \sup_{p\subseteq u}\sup_{t\in[t_0,t_1]} 
  e^{-\gamma(t-t_0)} \sum_{w\subseteq u} 
  \int_{t_0}^{t}dt' 
  \| \partial_{u\backslash w}a\|_{0}
  \|\partial_w \Delta^{n-1}_{\odd}(t',t_0)\|_{I_2}\\
  &+2 \sup_{p\subseteq u}
  \sup_{t\in[t_0,t_1]} e^{-\gamma(t-t_0)}\sum_{w\subseteq u} 
  \int_{t_0}^{t}dt'  
  \|  \partial_{u\backslash w}b(t,t')\|_{I_2}
  \|\partial_w \Delta^{n-2}(t',t_0) \|\\
  &\le
  \sup_{p\subseteq u}\sup_{t\in[t_0,t_1]} 
  e^{-\gamma t} \sum_{w\subseteq u} 
  \int_{t_0}^{t}dt' e^{\gamma t'}
  \| \partial_{u\backslash w}a\|_{0}
  \|\partial_w \Delta^{n-1}_{\odd}(\cdot,t_0)\|_{I_2,\gamma}\\
  &+2 \sup_{p\subseteq u}\sup_{t\in[t_0,t_1]} 
  e^{-\gamma t}\sum_{w\subseteq u} 
  \int_{t_0}^{t}dt'  e^{\gamma t'}
  \|  \partial_{u\backslash w}b\|_{I_2,0}
  \|\partial_w \Delta^{n-2}(\cdot,t_0) \|_{\gamma}\\
  &\le
  \frac{1}{\gamma} \sup_{p\subseteq u} \sum_{w\subseteq u} 
  \| \partial_{u\backslash w}a\|_{0}
  \|\partial_w \Delta^{n-1}_{\odd}(\cdot,t_0)\|_{I_2,\gamma}\\
  &+\frac{2}{\gamma} \sup_{p\subseteq u} \sum_{w\subseteq u} 
  \|  \partial_{u\backslash w}b\|_{I_2,0}
  \|\partial_w \Delta^{n-2}(\cdot,t_0) \|_{\gamma}\\
  &\le
  \frac{2^{|u|}}{\gamma} \sup_{u'\subseteq u} 
  \| \partial_{u'}a\|_{0} \sup_{p\subseteq u} 
  \|\partial_p \Delta^{n-1}_{\odd}(\cdot,t_0)\|_{I_2,\gamma}\\
  &+\frac{2^{|u|+1}}{\gamma} \sup_{u'\subseteq u} 
  \|  \partial_{u'}b\|_{I_2,0}\sup_{p\subseteq u}
  \|\partial_p \Delta^{n-2}(\cdot,t_0) \|_{\gamma}
  \end{align}
  
  Similarly we compute the operator norm:
  
  \begin{align}
    &\sup_{p\subseteq u}\|\partial_p \Delta^n \|_{\gamma}\\
    &\le 
    \sup_{t\in[t_0,t_1]} e^{-\gamma(t-t_0)}\sup_{p\subseteq u} 
    \sum_{w\subseteq p} 
    \| \partial_{p\backslash w}(a ~
    \partial_w \Delta^{n-1})(t,t_0)\|\\
    &+\sup_{t\in[t_0,t_1]} e^{-\gamma(t-t_0)}\sup_{p\subseteq u} 
    \sum_{w\subseteq p} 
    \|  (\partial_{p\backslash w}b ~
    \partial_p \Delta^{n-2})(t,t_0) \|\\
    &\le
    \sup_{t\in[t_0,t_1]} e^{-\gamma(t-t_0)}
    \sup_{p\subseteq u} \sum_{w\subseteq p} 
    \int_{t_0}^{t}dt' 
    \| \partial_{p\backslash w}a(t,t') ~
    \partial_p \Delta^{n-1}(t',t_0)\|\\
    &+ \sup_{t\in[t_0,t_1]} e^{-\gamma(t-t_0)}
    \sup_{p\subseteq u}\sum_{w\subseteq u} 
    \int_{t_0}^{t}dt'  
    \|  \partial_{p\backslash w}b(t,t') ~
    \partial_w \Delta^{n-2}(t',t_0) \|\\
    &\le
    \sup_{t\in[t_0,t_1]} e^{-\gamma(t-t_0)}
    \sup_{p\subseteq u} \sum_{w\subseteq p} 
    \int_{t_0}^{t}dt' 
    \| \partial_{p\backslash w}a(t,t')\|
    \|\partial_w \Delta^{n-1}(t',t_0)\|\\
    &+ \sup_{t\in[t_0,t_1]} e^{-\gamma(t-t_0)}
    \sup_{p\subseteq u} \sum_{w\subseteq p} 
    \int_{t_0}^{t}dt'  
    \|  \partial_{p\backslash w}b(t,t')\|
   \|\partial_w \Delta^{n-2}(t',t_0) \|\\
   &\le
   \sup_{t\in[t_0,t_1]} e^{-\gamma t}
   \sup_{p\subseteq u} \sum_{w\subseteq p} 
   \int_{t_0}^{t}dt' e^{\gamma t'}
   \| \partial_{p\backslash w}a \|_{0}
   \|\partial_w \Delta^{n-1}(\cdot,t_0)\|_{\gamma}\\
   &+ \sup_{t\in[t_0,t_1]} e^{-\gamma t}
   \sup_{p\subseteq u}\sum_{w\subseteq p} 
   \int_{t_0}^{t}dt'  e^{\gamma t}
   \|  \partial_{u\backslash p}b\|_{0}
  \|\partial_w \Delta^{n-2}(\cdot,t_0) \|_{\gamma}\\
  &\le
  \frac{1}{\gamma}\sup_{p\subseteq u} \sum_{w\subseteq p} 
  \| \partial_{p\backslash w}a \|_{0}
  \|\partial_w \Delta^{n-1}(\cdot,t_0)\|_{\gamma}\\
  &+ \frac{1}{\gamma}
  \sup_{p\subseteq u} \sum_{w\subseteq p} 
  \|  \partial_{p\backslash w}b\|_{0}
  \|\partial_w \Delta^{n-2}(\cdot,t_0) \|_{\gamma}\\
  &\le
  \frac{2^{|u|}}{\gamma}
  \sup_{u'\subseteq u}\| \partial_{u'}a \|_{0}
   \sup_{w\subseteq u} 
  \|\partial_w \Delta^{n-1}(\cdot,t_0)\|_{\gamma}\\
  &+ \frac{2^{|u|}}{\gamma}
  \sup_{u'\subseteq u}\| \partial_{u'}b \|_{0}
  \sup_{w\subseteq u}
  \|\partial_w \Delta^{n-2}(\cdot,t_0) \|_{\gamma}
  \end{align}
  
  We can summarise the last calculations more briefly
  using the abbreviation
  \begin{equation}
  \alpha=\frac{2^{|u|+1}}{\gamma} \sup_{u'\subseteq u}
  \left\{\|\partial_{u'}a\|_{0},
  \|\partial_{u'}b\|_{I_2,\infty},
  \|\partial_{u'}b\|_{0}  \right\}.
  \end{equation}
  Here \(\alpha\) is finite. This can be seen as follows:
  firstly, \(\partial_u b=U^0 \partial_u F^B_{\odd}U^0\) 
  vanishes if \(|u|\ge 6\) because the factors 
  \({Q'}^B\), \(Q^B\) and \(Z^B\) are all linear in 
  \(B\) and the longest product of such operators 
  appearing in \(b\) has three factors, analogously 
  all derivatives \(\partial_u a=0\) for \(|b|\ge 3\).
  Secondly, each of the operators  \({Q'}^C\), \(Q^C\) 
  and \(Z^C\) are bounded for every \(C\in\mathcal{V}\),
  hence the polynomials \(a\) and \(b\) of these 
  operators are also bounded. This shows finiteness 
  of the two operator norms appearing in the expression 
  for \(\alpha\). For the Hilbert-Schmidt norm we 
   see 
  that \(\partial_u b\) is always a sum of terms 
  where each term has a factor 
  \(U^0 \partial_p F_{\odd}^B U^0\) with \(p\subseteq u\).
  This factor has finite Hilbert-Schmidt norm due to 
  the \(I_2\) estimate lemma 
  \ref{F, G off diagonal hilbert schmidt}.

  We can thus summarise the last two calculations 
  
  \begin{align}
  &\begin{pmatrix} \sup_{p\subseteq u} \|\partial_p \Delta^{n} \|_{\mathrm{op}+I_2,\gamma}\\
  \sup_{p\subseteq u} \|\partial_p \Delta^{n-1}\|_{\mathrm{op}+I_2,\gamma}\end{pmatrix}\\
  &=\begin{pmatrix}\alpha & \alpha \\ 1 & 0 \end{pmatrix}
  \begin{pmatrix} \sup_{p\subseteq u} \|\partial_p \Delta^{n-1}\|_{\mathrm{op}+I_2,\gamma}\\
  \sup_{p\subseteq u} \|\partial_p \Delta^{n-2}\|_{\mathrm{op}+I_2,\gamma} \end{pmatrix}\\\label{analyticity converging recursion}
  &=\begin{pmatrix}\alpha & \alpha \\ 1 & 0 \end{pmatrix}^{n-1}
    \begin{pmatrix} \sup_{p\subseteq u} \|\partial_p \Delta^{1}\|_{\mathrm{op}+I_2,\gamma}\\
    \sup_{p\subseteq u} \|\partial_p \Delta^{0}\|_{\mathrm{op}+I_2,\gamma} \end{pmatrix}.
  \end{align}
  
  This matrix can be diagonalised, it's eigenvalues are
  \begin{align}
  \lambda_{\pm}= \frac{\alpha}{2}\left(1\pm \sqrt{1+\frac{4}{\alpha}}\right).
  \end{align}
  The larger eigenvalue \(\lambda_+\) is less than \(1\) if
  and only if
  \(0<\alpha<0.5\) holds true, as can be seen from a quick 
  calculation:
  \begin{align}
  \frac{\alpha}{2}\left(1+\sqrt{1+\frac{4}{\alpha}}\right)<1 \\
  \iff \sqrt{1+\frac{4}{\alpha}}<\frac{2}{\alpha}-1.
  \end{align}
  If \(\alpha\ge \frac{1}{2}\) or \(\alpha<0\) this inequality is not satisfied, otherwise
  we may square both sides to find
  \begin{align}
  1+\frac{4}{\alpha}<(2/\alpha -1)^2=4/\alpha^2 - 4/\alpha +1\\
  \iff \alpha < \frac{1}{2}.
  \end{align}
  So we conclude that for \(\gamma\) large enough 
  the right hand 
  side of \eqref{analyticity converging recursion}
  tends to zero as \(c \lambda_+^n\) for
  \(n\rightarrow \infty\), with
\begin{equation}
  c=\sqrt{\sup_{p\subseteq u} \|\partial_p \Delta^1\|^2_{\mathrm{op}+I_2,\gamma}
  +\sup_{p\subseteq u} \|\partial_p \Delta^0\|^2_{\mathrm{op}+I_2,\gamma}}.
\end{equation}

The two summands in the last equation are finite due 
to lemma \ref{F, G off diagonal hilbert schmidt}.

That is, we have  
  \begin{equation}
  \sup_{p\subseteq u}
  \|\partial_p \Delta^n\|_{\mathrm{op}+I_2,\gamma}
  \le \lambda_+^n c
  \xrightarrow{n\rightarrow \infty} 0.
  \end{equation}
  For \(2\le m\le n\) we obtain  
  \begin{align}
  &\sup_{p\subseteq u}\| \partial_p R^B_n -
  \partial_p R^B_m\|_{\mathrm{op}+I_2,\gamma} 
  \le 
  \sum_{k=m}^{n-1}
  \sup_{p\subseteq u}
  \| \partial_p \Delta^k\|_{\mathrm{op}+I_2,\gamma} \\
  &\le \sum_{k=m}^{\infty}
  \lambda_+^k c
  = \frac{\lambda_+^m }{1-\lambda_+} c
  \xrightarrow{m\rightarrow \infty}0
  \end{align}
  since the the norms
  \(\|\cdot\|_{\mathrm{op}+I_2}\) and
  \(\|\cdot\|_{\mathrm{op}+I_2,\gamma}\) are 
  equivalent, we have just proven 
  that \(\partial_{[k]} R^B_m\) is a 
  Cauchy sequence with respect to the norm 
  \(\|\cdot\|_{\mathrm{op}+I_2}\) and 
  hence convergence by 
  lemma \ref{completeness of I_2 odd}.
  
  
\end{proof}

The following lemma is a necessary ingredient for
theorem \ref{thm smoothness of S}. Morally, it 
has already been proven in 
\cite[Lemma 3.7]{ivp0}; however, as that paper
was not concerned with multiple four-potentials 
the lemma was not formulated general enough 
for our needs here. So we restate it and 
show how to modify the original proof.

\begin{Lemma}[\(I_2\) estimates]\label{F, G off diagonal hilbert schmidt}
Let \(k\in\mathbb{N}\) and 
\(A,H_b\in\mathcal{V}\) for \(b\le k\). 
Using the abbreviations introduced in \eqref{derivative set 1}
and \eqref{derivative set 2}
we have for any \(u\subset [k]\) 
the following bounds:

\begin{align}\label{I2 estimate 1}
\|\partial_u U^0 F_{\odd}^{A+\sum_{b=1}^k H_b} U^0\|_{I_2,0}&<\infty \\\label{I2 estimate 2}
\|\partial_u G^{A+\sum_{b=1}^k H_b}\|_{I_2,0}&<\infty.
\end{align}
\end{Lemma}
\begin{proof}
For \(B\in\mathcal{V}\) recall 
\begin{align}
  F^B_{\odd}:=& ((-{Q'}^B+Z^B_{\mathrm{ev}}-Q^BZ^B)(1+Q^B))_{\odd}\\\label{Fodd sum}
  =&-{Q'}^{B}+Z_{\ev}^B Q^B -Q^BZ^B_{\ev} - Q^B Z_{\odd}^B Q^B,\\
  G^B:=&- U^0Q^BQ^B \\
  +& U^0(-{Q'}^B+Z_{\mathrm{ev}}-Q^B Z^B)Q^B Q^B U^B (1+Q^B).
\end{align}

Pick \(k\in\mathbb{N}\)
and  \(A,B, H_b\in\mathcal{V}\) for \(b\le k\).

According to \cite[lemma 3.7]{ivp0} the operators 
\(U^0Z_{\ev}^B Q^B U^0,\) \(U^0 Q^B Z_{\ev}^B U^0,\) 
\(U^0 {Q'}^{B}U^0,\) \(Q^B Q^B, {Q'}^B Q^B\) and 
\(Q^B Z^B Q^B\) are Hilbert-Schmidt operators.
Additionally, their  Hilbert-Schmidt norm is uniformly
bounded in time and \(F\) and \(G\) fulfil the following 
norm bound:

\begin{equation}
\|U^0 F_{\odd}^BU^0\|_{I_2,0}<\infty 
\text{ and } \|G^B\|_{I_2,0}<\infty.
\end{equation}

In fact, the proof given in \cite{ivp0} also workes
for non identical four-potentials \(A,B,C\in\mathcal{V}\)
proving

\begin{align}
  &\| U^0Z_{\ev}^A Q^B U^0\|_{I_2,0}<\infty,\\
  &\|U^0 Q^A Z_{\ev}^B U^0\|_{I_2,0}<\infty,\\ 
  &\|U^0 {Q'}^{B}U^0\|_{I_2,0}<\infty,\\
  &\|Q^B Q^A\|_{I_2,0}<\infty,\\ 
  &\|{Q'}^B Q^A\|_{I_2,0}<\infty,\\ 
  &\|Q^A Z^B Q^C\|_{I_2,0}<\infty
\end{align}
and therefore also 
\begin{equation}
  \|\partial_u U^0 F_{\odd}^{A+\sum_{b=0}^k H_b}U^0\|_{I_2,0}<\infty 
  \text{ and } \|\partial_u G^{A+\sum_{b=0}^k H_b}\|_{I_2,0}<\infty
\end{equation}
for any \(u\subseteq [k]\). 

For the benefit of the reader,
we will reproduce the proof of the estimate
\begin{equation}\label{HS property of G}
  \|\partial_u G^{A+\sum_{b=0}^k H_b}\|_{I_2,0}<\infty,
\end{equation}
to make clear the structure of the entire proof.

The operator \(G^{A+\sum_{b=0}^k H_b}\) consists of 
two summands. Each summand is a product of operators 
with operator norm uniformly bounded in time and
containing a factor of 
\(Q^{A+\sum_{b=0}^k H_b} Q^{A+\sum_{b=0}^k H_b}\).
All the other factors contributing to \(G\)
stay bounded when differentiated and
the map \(Q:\mathcal{V}\rightarrow I_2\) is 
linear, so the bound 
\begin{equation}
  \|Q^{B} Q^{D}\|_{I_2,0}<\infty
\end{equation}
for general \(B,D\in\mathcal{V}\) will suffice to prove
\eqref{HS property of G}. 


Pick \(B,D\in\mathcal{V}\), we estimate 
\begin{align}\label{QQ norm}
\sup_{t\in \mathbb{R}}\| Q^B (t) Q^D(t)\|_{I_2}\\
\le \sum_{\mu,\nu=0}^3 
\big(\sup_{p,k,q\in\mathbb{R}^3} 
|P_+(p)\gamma^0\gamma^\mu P_-(k)\gamma^0\gamma^\nu P_+(q)|\\
+\sup_{p,k,q\in\mathbb{R}^3} 
|P_-(p)\gamma^0\gamma^\mu P_+(k)\gamma^0\gamma^\nu P_-(q)|\big)\\\label{QQ norm end}
\sup_{t\in\mathbb{R}}\left\| \int_{\mathbb{R}^3} dk 
\frac{\hat{B}_\mu (t,p-k)\hat{D}_\nu(t,k-q)|}{(E(p)+E(k))(E(k)+E(q))} 
\right\|_{I_2,(p,q)},
\end{align}
where the index in the norm indicates 
with respect to which variables the integral of the norm 
is to be performed. The prefactor is finite since 
\(P_\pm (p):\mathbb{C}^4\rightarrow\mathbb{C}^4\) 
is a projector for any \(p\in\mathbb{R}^3\).
Abbreviating 
\begin{align}
\tilde{c}:=\sum_{\mu,\nu=0}^3 
(\sup_{p,k,q\in\mathbb{R}^3} 
|P_+(p)\gamma^0\gamma^\mu P_-(k)\gamma^0\gamma^\nu P_+(q)|\\
+\sup_{p,k,q\in\mathbb{R}^3} 
|P_-(p)\gamma^0\gamma^\mu P_+(k)\gamma^0\gamma^\nu P_-(q)|),
\end{align}
and using the integral estimate lemma 
\cite[lemma 3.8 (iii)]{ivp0} we find
\begin{align}\label{QQ estimate final}
\eqref{QQ norm}\le 
\tilde{c}~ C_{8,\text{of }\cite{ivp0}} \sum_{\mu,\nu=0}^3 
\sup_{t\in\mathbb{R}}\|\hat{B}_\mu(t) \|_{I_1} \|\hat{D}_{\nu}(t) \|_{I_2}.
\end{align}
Because of 
\(B,D\in C_c^\infty(\mathbb{R}^4)\), we have
\(\hat{B}(t),\hat{D}(t)\) are analytic functions
decaying faster than any negative power at anfinity
for any \(t\), so \eqref{QQ estimate final} is finite. 

Also in order to proof the first estimate in 
\eqref{I2 estimate 1} the proof of \cite[lemma 3.7]{ivp0}
can be followed almost verbatim. 
First one dissects \(F_{\odd}\) into the sum 
\(U^0\eqref{Fodd sum}U^0\),
next the summands have to be bounded individually.
This is achieved by repeating the proof of the
partial integration lemma \cite[lemma 3.6]{ivp0},
estimates of the form of \eqref{QQ norm}-\eqref{QQ norm end}
and making use of the integral estimate lemma 
\cite[lemma 3.8]{ivp0}.

\end{proof}








\begin{Thm}[Properties of Derivatives of S]
Let  \(A,H\in\mathcal{V}\), pick \(t_1\) 
after \(\supp A\cup\supp H\) and \(t_0\)
before \(\supp A\cup\supp H\), 
let \(T_1\in I_2(\mathcal{F})\) 
then the following equalities are satisfied:
\begin{align}\notag
&\partial_{H} \tr(T_1 P^+ U^{A}(t_0,t_1)U^{A+H}(t_1,t_0)P^-)\\\label{pull derivative into trace}
&= \tr(T_1 P^\pm U^{A}(t_0,t_1)\partial_{H} U^{A+H}(t_1,t_0)P^\mp )
\end{align}
\end{Thm}
\begin{proof}
Let \(A,H\in\mathcal{V}\) and \(t_0,t_1\in\mathbb{R}\)
and \(T_1\) be 
as in the theorem. The proof of the two equalities is 
analogous, so we only explicitly prove the first one.
The trace is linear, so we have
\begin{align}\notag
&\bigg|
\tr\left(T_1 P^+ U^{A}(t_0,t_1) 
\frac{1}{\varepsilon}(U^{A+\varepsilon H}(t_1,t_0)-U^{A}(t_1,t_0))P^-\right)\\
&\quad- \tr(T_1 P^+ U^{A}(t_0,t_1)\partial_{H}U^{A+H}(t_1,t_0)P^-)\bigg|\\\notag
&\le \|T_1\|_{I_2} \bigg\|  P^+ U^{A}(t_0,t_1) 
\frac{1}{\varepsilon}\big(U^{A+\varepsilon H}(t_1,t_0)-U^{A}(t_1,t_0)\big)P^- \\\label{derivative trace estimate}
&\quad-P^+ U^{A}(t_0,t_1)\partial_{H}U^{A+H}(t_1,t_0)P^-\bigg\|_{I_2}
\end{align}

For the first summand we insert the identity in the 
form \(P^++P^-\) and obtain
\begin{align}
P^+ U^{A}(t_0,t_1) 
  \frac{1}{\varepsilon}\left(U^{A+\varepsilon H}(t_1,t_0)-U^{A}(t_1,t_0)\right)P^-\\
=P^+ U^{A}(t_0,t_1) P^+
\frac{1}{\varepsilon}\left(U^{A+\varepsilon H}(t_1,t_0)-U^{A}(t_1,t_0)\right)P^-\\
+P^+ U^{A}(t_0,t_1) P^-
\frac{1}{\varepsilon}\left(U^{A+\varepsilon H}(t_1,t_0)-U^{A}(t_1,t_0)\right)P^-.
\end{align}
Analogously for the second summand.
Now because of the Smoothness of S theorem 
\ref{thm smoothness of S}
we know that 
\begin{align}
  P^-
  \frac{1}{\varepsilon}\left(U^{A+\varepsilon H}(t_1,t_0)-U^{A}(t_1,t_0)\right)
  P^-\xrightarrow[\|\cdot\|]{\varepsilon\rightarrow 0} P^- \partial_H U^{A+H}(t_1,t_0)P^-\\
  P^+
  \frac{1}{\varepsilon}\left(U^{A+\varepsilon H}(t_1,t_0)-U^{A}(t_1,t_0)\right)
  P^-\xrightarrow[\|\cdot\|_{I_2}]{\varepsilon\rightarrow 0} P^+ \partial_H U^{A+H}(t_1,t_0)P^-
\end{align}
holds true. Hence we find in total 
\begin{align}
&\frac{\eqref{derivative trace estimate}}{\|T_1\|_{I_2}}\\
&\le \notag
\bigg\|  P^+ U^{A}(t_0,t_1)\bigg\| \bigg\| P^+
\frac{1}{\varepsilon}\big(U^{A+\varepsilon H}(t_1,t_0)-U^{A}(t_1,t_0)\big)P^- \\
&\quad- P^+\partial_{H}U^{A+H}(t_1,t_0)P^-\bigg\|_{I_2}\\
&+\bigg\|  P^+ U^{A}(t_0,t_1)P^- \bigg\| \bigg\|  \notag
\frac{1}{\varepsilon}\big(U^{A+\varepsilon H}(t_1,t_0)-U^{A}(t_1,t_0)\big)P^- \\
&\quad-\partial_{H}U^{A+H}(t_1,t_0)P^-\bigg\|_{I_2}\xrightarrow{\varepsilon \rightarrow 0} 0.
\end{align}

\end{proof}









\section{Geometric Construction of the Phase}\label{chapter geometery}
Next we introduce the set of four potentials we work with, as well as the argument of a complex number and an 
invertible bounded operator. For complex numbers the convention we chose here differs slightly from the standard in the literature,
which is why we also use a slightly non standard name for this function.
\begin{Def}[ polar decomposition and spectral projections]
We denote by \(\mathcal{H}=L^2(\mathbb{R}^3,\mathbb{C})\).
For \(X:\mathcal{H}\rightarrow \mathcal{H}\) bounded
\begin{equation}\label{def AG}
\AG(X):=X |X|^{-1}.
\end{equation}
Furthermore, we define for any complex number \(z\in \mathbb{C}\backslash \{0\}\)
\begin{equation}
\ag(z):=\frac{z}{|z|} .
\end{equation}
In abuse of notation we will define the expression
\begin{equation}
\partial_t \ln f(t):=\frac{\partial_t f(t)}{f(t)},
\end{equation}
for any differentiable \(f:\mathbb{R}\rightarrow \mathbb{C}\backslash \{0\} \), 
even if the expression \(\ln f(t)\) cannot be interpreted as the principle
branch of the logarithm.

We also introduce \(S^1:=\{z\in\mathbb{C}\mid |z|=1\}\).

\end{Def}





\begin{Def}[scattering operator and phases]\label{def: S bar, gamma, cA}
We define for all \(A,B\in\mathcal{V} \)
\begin{align}
S_{A,B}:=U^{A}_{\Sigma_{\mathrm{in}},\Sigma_{\mathrm{out}}}U^{B}_{\Sigma_{\mathrm{out}},\Sigma_{\mathrm{in}}},
\end{align}
where \(\Sigma_{\mathrm{out}}\) and \(\Sigma_{\mathrm{int}}\)  are Cauchy-surfaces of Minkowski spacetime such that 
\begin{align}
&\forall (x,y)\in \supp A\cup\supp B\times \Sigma_{\mathrm{in}}: (x-y)^2 \ge 0\Rightarrow x^0>y^0,\\
&\forall (x,y)\in \supp A\cup\supp B\times \Sigma_{\mathrm{out}}: (x-y)^2 \ge 0\Rightarrow x^0<y^0
\end{align}
holds.
Let 
\begin{align}
\mathrm{dm}:=\{(A,B)\in\mathcal{V}^2\mid P^- S_{A,B}P^-\text{ and }\\
 P^- S_{B,A}P^-:\mathcal{H}^- \righttoleftarrow  \text{ are invertible}\},
\end{align}
we define
\begin{equation}\label{dom s bar}
\dom\overline{S}:=\{(A,B)\in \mathrm{dm}\mid  \overline{A~B}\times\overline{A~B} \subseteq \mathrm{dm} \},
\end{equation}
where \(\overline{A~B}\) is the line segment connecting \(A\) and \(B\) in \(\mathcal{V}\).
Furthermore, we choose for all \(A,B\in \dom \overline{S}\) the lift
\begin{align}
\overline{S}_{A,B}=\mathcal{R}_{\AG((P^- S_{A,B}P^-)^{-1})} \mathcal{L}_{S_{A,B}}.
\end{align}

For \((A,B),(B,C), (C,A)\in\dom\overline{S}\), we define 
 the complex numbers 
\begin{align}\label{def: gamma}
\gamma_{A,B,C}&:=\ag(\det_{\mathcal{H}^-} (P^- S_{A,B} P^- S_{B,C} P^- S_{C,A}P^-)),\\
\Gamma_{A,B,C}&:=\ag(\gamma_{A,B,C}).
\end{align}
We will see in lemma \ref{gamma attri} that 
\(\gamma_{A,B,C}\neq 0\) and
\( P^-S_{A,B}P^-S_{B,C}P^-S_{C,A}-1\) is traceclass, so 
that \(\Gamma_{A,B,C}\) is well-defined.
Lastly we introduce for \(A,B,C\in\mathcal{V}\) the function
\begin{equation}
c_A(F,G):=-i \partial_F \partial_G  \Im \tr [P^- S_{A,A+F} P^+ S_{A,A+G} P^-] .
\end{equation}
\end{Def}

\begin{Lemma}[properties of \(\dom\overline{S}\)]\label{lem properties of dom s bar}
The set \(\dom \overline{S}\) has the following properties:
\begin{enumerate}
\item contains the diagonal: \(\{(A,A)\mid A\in\mathcal{V}\}\subseteq \dom\overline{S}\).
\item openness: \(\forall n \in \mathbb{N}: \left\{s\in\mathbb{R}^{2n}\mid \left(\sum_{k=1}^n s_k A_k,\sum_{k=n+1}^{2n} s_{k} A_{k} \right)\in \dom\overline{S}\right\}\) is an open subset of \(\mathbb{R}^{2n}\) for all \(A_1,\dots A_{2n}\in\mathcal{V}\).
\item symmetry:  \((A,A')\in\dom\overline{S}\iff (A',A)\in\dom\overline{S}\)
\item star-shaped: \((A,t A)\in \dom\overline{S}\Rightarrow \forall s\in \overline{1~~t} : (A,s A)\in\dom\overline{S}\)
\item well defined-ness of \(\overline{S}\): \(\dom\overline{S}\subseteq \{A,B\in\mathcal{V}\mid P^- S_{A,B} P^-:\mathcal{H}^- \righttoleftarrow \text{is invertible} \}\).
\end{enumerate}
We will only prove openness, as the other properties follow directly from the definition \eqref{dom s bar}. So pick \(n\in\mathbb{N}\), 
\(A_i\in\mathcal{V}\) for \(i\in \mathbb{N}, i\le 2 n\) and \(s\in\mathbb{R}^{2n}\)
such that \(\left(\sum_{k=1}^n s_k A_k,\sum_{k=n+1}^{2n} s_{k} A_{k} \right)\in\dom\overline{S}\). 
We have to find a neighbourhood \(U\subseteq \mathbb{R}^{2n}\) of \(s\) such that 
\(\{ \left(\sum_{k=1}^n s_k' A_k,\sum_{k=n+1}^{2n} s_{k}' A_{k} \right) \mid s'\in U\}\subseteq \dom\overline{S}\) holds. 
In doing so we have to ensure that the square

\begin{align}
  \overline{\sum_{k=1}^n s_k A_k~~~\sum_{k=n+1}^{2n} s_{k} A_{k} }^2
\end{align}
stays a subsets of \(\mathrm{dm}\) for all \(s'\in U\). 
Now pick a metric \(d\) on \(\mathbb{R}^n\) and define 
\begin{align}\nonumber
&r:=\inf \left\{d(s,s')\mid \overline{\sum_{k=1}^n s_k' A_k~~\sum_{k=n+1}^{2n} s_k' A_k }^2 ~~\cap \mathrm{dm}^c\neq \emptyset \right\}.
\end{align}
It cannot be the case that \(r=0\), because the 
metric is continuous, the square compact in 
\(\mathbb{R}^{2n}\) 
and the set of invertible bounded operators 
(defining \(\mathrm{dm}\)) is open in the topology 
generated by the operator norm.
If \(r=\infty\) then \(U=\mathbb{R}^{2n}\) will 
suffice. If \(r\in\mathbb{R}^+\) then \(U=B_r(s,t)\) 
the open ball of radius \(r\) around \(s\) works.
\end{Lemma}

\subsection{Main Result of Construction}


\begin{Def}[causal splitting]
We define a causal splitting as a function 
\begin{align}
&c^+:\mathcal{V}^3\rightarrow \mathbb{C}, \\
&(A,F,G)\mapsto c_A^+(F,G),
\end{align}
such that \(c^+\) restricted to any finite dimensional subspace is smooth in the 
first argument and linear in the second and third argument.
Furthermore \(c^+\) should satisfy
\begin{align}\label{c+ 1}
c_A(F,G)=c_A^+(F,G)-c_A^+(G,F),\\\label{c+ 2}
\partial_H c^+_{A+H}(F,G)=\partial_G c^+_{A+G}(F,H),\\\label{c+ 3}
\forall F \prec G: c_A^+(F,G)=0.
\end{align}
\end{Def}

\begin{Def}[current]
Given a lift \(\hat{S}_{A,B}\) of the one-particle scattering operator \(S_{A,B}\) for which the derivative in the following expression exists,
 we define the associated current by Bogolyubov's formula:
\begin{equation}
j_A^{\hat{S}}(F):=i\partial_F \left\langle \Omega, \hat{S}_{A,A+F} \Omega\right\rangle.
\end{equation}
\end{Def}

\begin{Thm}[existence of causal lift]\label{thm: geometry}
Given a causal splitting \(c^+\), there is a second quantised scattering operator \(\tilde{S}\), lift of the one-particle scattering operator \(S\)
with the following properties
\begin{align}
&\forall A,B,C\in \mathcal{V}:\tilde{S}_{A,B}\tilde{S}_{B,C}=\tilde{S}_{A,C}\\
&\forall F\prec G: \tilde{S}_{A,A+F}=\tilde{S}_{A+G,A+F+G}
\end{align}
and the associated current satisfies
\begin{equation}
\partial_G j_{A+G}^{\tilde{S}}(F)=\left\{\begin{matrix} -2i c_A(F,G)  &\text{ for } G\prec F\\ 0 &\text{  otherwise.}  \end{matrix} \right.
\end{equation}
\end{Thm}

\subsection{Proofs}


Since the phase of a lift relative to any other lift is fixed by a single matrix element, we may use the vacuum expectation values to characterise the
phase of a lift. The function \(c\) captures the dependence of this object on variation of the external fields, the connection between vacuum expectation
values and \(c\) becomes clearer with the next lemma.

\begin{Lemma}[properties of \(\Gamma\)]\label{gamma attri}
The function \(\Gamma\) has the following properties for all  \(A,B,C,D\in\mathcal{V}\) such that the expressions occurring in each equation are well defined:
\begin{align}\label{gamma attri0}
&\gamma_{A,B,C}\neq 0\\\notag
&\Gamma_{A,B,C}=\det_{\mathcal{H}^-}(P^--P^-S_{A,C}P^+S_{C,A}P^- \\\label{gamma attri1}
&\hspace{5cm}- P^- S_{A,B} P^+ S_{B,C} P^- S_{C,A} P^-)\\\label{gamma attri2}
&\Gamma_{A,B,C}^{-1}=\ag(\langle \Omega, \overline{S}_{A,B} \overline{S}_{B,C} \overline{S}_{C,A}\Omega\rangle )\\\label{gamma attri3}
&\Gamma_{A,B,C}=\Gamma_{B,C,A}=\frac{1}{\Gamma_{B,A,C}}\\\label{gamma attri4}
&\Gamma_{A,A,B}=1\\\label{gamma attri5}
&\Gamma_{A,B,C}\Gamma_{B,A,D}\Gamma_{A,C,D}\Gamma_{C,B,D}=1 \\\label{gamma attri6}
&\Gamma_{A,B,C}=\Gamma_{D,B,C}\Gamma_{A,D,C}\Gamma_{A,B,D} \\\label{gamma attri7}
&\overline{S}_{A,C}=\Gamma_{A,B,C}\overline{S}_{A,B}\overline{S}_{B,C}\\\label{gamma attri8}
&c_A(B,C)=\partial_B \partial_C \ln \Gamma_{A,A+B,A+C}.
\end{align}
\end{Lemma}
\begin{proof}
Pick  \(A,B,C\in\mathcal{V}\) such that \(\|1-S_{X,Y}\|<1\) for \(X,Y\in\{A,B,C\}\). 
By definition \(\gamma\) is
\begin{equation}
\gamma_{A,B,C}=\det_{\mathcal{H}^-}(P^- S_{A,B} P^- S_{B,C} P^- S_{C,A} P^-).
\end{equation}
The operator whose determinant we take in the last line is a product
\begin{equation}
P^- S_{A,B} P^- S_{B,C} P^- S_{C,A} P^-=P^- S_{A,B}P^-~~ P^- S_{B,C}P^-~~ P^- S_{C,A} P^-.
\end{equation}
The three factors appearing in this product are all invertible, hence the product is also invertible as operators of type \(\mathcal{H}^-\rightarrow \mathcal{H}^-\)
 because of the conditions of \(\{A,B,C\}\) imply that \(\|P^--P^-S_{X,Y}P^-\|<1\) which means that the Von Neumann series of the inverse converges, 
therefore if the determinant exists we have \(\gamma_{A,B,C}\neq 0\).
To see that it does exist, we reformulate
\begin{align}
&\gamma_{A,B,C}=\det_{\mathcal{H}^-}(P^- S_{A,B} P^- S_{B,C} P^- S_{C,A} P^-)\\
&=\det_{\mathcal{H}^-}(P^-S_{A,C}P^-S_{C,A}P^- - P^- S_{A,B} P^+ S_{B,C} P^- S_{C,A} P^-)\\\label{eq: gamma proof of first attri}
&=\det_{\mathcal{H}^-}(P^--P^-S_{A,C}P^+S_{C,A}P^- - P^- S_{A,B} P^+ S_{B,C} P^- S_{C,A} P^-),
\end{align}
now we know by a classic result of Ruisnaars \cite{ruisnaar&ivp0} that \(P^+S_{X,Y}P^-\) is a Hilbert-Schmidt operator for our setting,
hence \(\gamma\) and also \(\Gamma\) are well defined.

 Equation 
\eqref{eq: gamma proof of first attri} also proves \eqref{gamma attri1}. Next we show \eqref{gamma attri2}. 
Borrowing notation from \cite[section 2]{ivp0} to identify \(\Omega=\bigwedge \Phi\) with the injection \(\Phi: \mathcal{H}^-\hookrightarrow\mathcal{H}\)
and \(\bigwedge\) is used to construct the infinite wedge spaces that are the perspective of Fock space introduced in \cite{ivp0}. 
We begin by reformulating the right hand side of  \eqref{gamma attri2}
\begin{align}
 &\langle \Omega, \overline{S}_{A,B} \overline{S}_{B,C} \overline{S}_{C,A}\Omega\rangle \\\notag
&=\langle \bigwedge \Phi, \bigwedge \left(S_{A,B}S_{B,C}S_{C,A}\Phi \AG(P^-S_{C,A} P^-)^{-1}\right.\\\notag
&\hspace{3cm}\left. \AG(P^-S_{B,C} P^-)^{-1} \AG(P^-S_{A,B} P^-)^{-1}  \right)\rangle \\
&=\langle \bigwedge \Phi, \bigwedge \left(\Phi \AG(P^-S_{C,A} P^-)^{-1}\right.\\\notag
&\hspace{3cm}\times\left. \AG(P^-S_{B,C} P^-)^{-1} \AG(P^-S_{A,B} P^-)^{-1}  \right)\rangle \\
&=\det_{\mathcal{H}^-}\left( (\Phi)^*  \left[\Phi \AG(P^- S_{C,A} P^-)^{-1} \AG(P^-S_{B,C} P^-)^{-1}\right.\right.\\\notag
&\hspace{6cm}\times\left.\left. \AG(P^-S_{A,B} P^-)^{-1} \right] \right)\\
&=\det_{\mathcal{H}^-}  \left()\AG(P^- S_{C,A} P^-)^{-1} \AG(P^- S_{B,C} P^-)^{-1}\right.\\ \notag
&\hspace{6cm}\left.\times\AG(P^-S_{A,B} P^-)^{-1}\right)  \\\label{eq:1/arg}
&=\frac{1}{\det_{\mathcal{H}^-}  \AG(P^- S_{A,B} P^-) \AG(P^- S_{B,C} P^-) \AG(P^-S_{C,A} P^-) }.
\end{align}
We first note that \(\det_{\mathcal{H}^-}|P^- S_{X,Y}P^-|\in\mathbb{R}^+\) for \(X,Y\in \{A,B,C\}\). This is well defined because 
\begin{align}
&\langle \Omega, \overline{S}_{X,Y}\Omega\rangle
=\langle \bigwedge \Phi, \bigwedge(S_{X,Y}\Phi \AG(P^- S_{X,Y}P^-)^{-1})\rangle\\
&=\det_{\mathcal{H}^-} \left( \Phi^* S_{X,Y}\Phi \AG(P^- S_{X,Y}P^-)^{-1}\right)\\
&=\det_{\mathcal{H}^-} \left( P^- S_{X,Y} P^- \AG(P^- S_{X,Y}P^-)^{-1}\right)\\
&=\det_{\mathcal{H}^-} \left(\AG(P^- S_{X,Y}P^-)^{-1} P^- S_{X,Y} P^- \right)\\
&=\det_{\mathcal{H}^-} \left(\AG(P^- S_{X,Y}P^-)^{-1} \AG(P^- S_{X,Y}P^-) |P^- S_{X,Y} P^-| \right)\\
&=\det_{\mathcal{H}^-} |P^- S_{X,Y}P^-|
\end{align}
holds. Moreover this determinant does not vanish, since the\\ \(P^- S_{X,Y}P^-\) is invertible. Also clearly the eigenvalues are positive
since \(|P^- S_{X,Y}P^-|\) is an absolute value. We continue with the result of \eqref{eq:1/arg}. Thus, we find
\begin{align}\label{eq: arg representation}
&\langle \Omega, \overline{S}_{A,B} \overline{S}_{B,C} \overline{S}_{C,A}\Omega\rangle^{-1}\\
&=\det_{\mathcal{H}^-}\left(  \AG(P^- S_{A,B} P^-) \AG(P^- S_{B,C} P^-) \AG(P^-S_{C,A} P^-)  \right)\\\notag
&=\det_{\mathcal{H}^-}\left(  \AG(P^- S_{A,B} P^-) \AG(P^- S_{B,C} P^-) P^-S_{C,A} P^-\right.\\
&\hspace{7.5cm}\times\left. |P^-S_{C,A} P^-|^{-1}  \right)\\\notag
&=\det_{\mathcal{H}^-}\left(  \AG(P^- S_{A,B} P^-) \AG(P^- S_{B,C} P^-) P^-S_{C,A} P^-  \right)\\
&\hspace{7cm}\times \det_{\mathcal{H}^-}|P^-S_{C,A} P^-|^{-1} \\\notag
&=\det_{\mathcal{H}^-}\left( P^-S_{C,A} P^- \AG(P^- S_{A,B} P^-) \AG(P^- S_{B,C} P^-)  \right)\\
&\hspace{7cm}\times \det_{\mathcal{H}^-}|P^-S_{C,A} P^-|^{-1} \\\notag
&=\frac{\det_{\mathcal{H}^-}\left(P^- S_{A,B} P^- P^- S_{B,C} P^- P^-S_{C,A} P^-  \right)}
{\det_{\mathcal{H}^-}|P^- S_{A,B} P^-| \cdot \det_{\mathcal{H}^-}|P^-S_{B,C} P^-|}\\
&\hspace{7cm}\times\frac{1}{\det_{\mathcal{H}^-}|P^-S_{C,A} P^-|}.
\end{align}
Now since the denominator of this fraction is real we can use \eqref{gamma attri1} to identity
\begin{equation}
\ag(\langle \Omega, \overline{S}_{A,B} \overline{S}_{B,C} \overline{S}_{C,A}\Omega\rangle)=\Gamma_{A,B,C}^{-1},
\end{equation}
which proves \eqref{gamma attri2}.

For the first equality in \eqref{gamma attri3} we use \(\det X (1+Y) X^{-1}=\det (1+Y)\) for any \(Y\) trace-class and 
 \(X\) bounded and invertible. So we can cyclicly permute the factors \(P^- S_{X,Y}P^-\) in the determinant and find
\begin{align*}
&\Gamma_{A,B,C}=\ag(\det_{\mathcal{H}^-} P^- S_{A,B}P^-S_{B,C}P^-S_{C,A}P^- )\\
&\quad=\ag(\det_{\mathcal{H}^-} P^-S_{C,A}P^- S_{A,B}P^-S_{B,C}P^- )
=\Gamma_{C,A,B}.
\end{align*}
For the second equality of \eqref{gamma attri3} we use \eqref{gamma attri1}
to represent both \(\Gamma_{A,B,C}\) and \(\Gamma_{B,A,C}\). Using this and the manipulations of the determinant 
we already employed, we arrive at
\begin{align}
&\Gamma_{A,B,C}\Gamma_{B,A,C}\\
&=\ag(\det_{\mathcal{H}^-} (P^-S_{A,B}P^-S_{B,C}P^-S_{C,A}P^-))\\
&\quad\times\ag(\det_{\mathcal{H}^-} (P^-S_{B,A}P^-S_{A,C}P^-S_{C,B}P^-))\\
&=\ag(\det_{\mathcal{H}^-} (P^-S_{A,B}P^-S_{B,C}P^-S_{C,A}P^-))\\
&\quad\times(\ag(\det_{\mathcal{H}^-} (P^-S_{B,C}P^-S_{C,A}P^-S_{A,B}P^-)))^*\\
&=\ag(\det_{\mathcal{H}^-} (P^-S_{A,B}P^-S_{B,C}P^-S_{C,A}P^-))\\
&\quad\times(\ag(\det_{\mathcal{H}^-} (P^-S_{A,B}P^-S_{B,C}P^-S_{C,A}P^-)))^*\\
&=|\ag(\det_{\mathcal{H}^-} (P^-S_{A,B}P^-S_{B,C}P^-S_{C,A}P^-))|^2=1,
\end{align}
which proves \eqref{gamma attri3}. 

Next, using \eqref{def: gamma} inserting twice the same argument yields
\begin{equation}
\gamma_{A,A,C}=\det_{\mathcal{H}^-} P^-S_{A,C}P^-S_{C,A}P^-
=\det_{\mathcal{H}^-} (P^- S_{C,A} P^-)^*P^-S_{C,A}P^-\in \mathbb{R}^+,
\end{equation}
hence \eqref{gamma attri4} follows.


For proving \eqref{gamma attri5} we will use the definition of \(\Gamma\) directly and repeatedly use that we can cyclicly permute operator groups of the form
\(P^- S_{X,Y}P^-\) for \(X,Y\in \{A,B,C,D\}\) in the determinant, i.e.
\begin{equation}\label{tetraeder 1}
\det P^- S_{X,Y}P^- O = \det O P^- S_{X,Y}P^-.\tag{\(\circlearrowleft\)}
\end{equation}
This is possible, because \(P^- S_{X,Y}P^-\) is bounded and invertible. Furthermore we will use that 
\begin{equation}\label{tetraeder 2}
\det O_1 O_2 = \det O_1 \det O_2\tag{\(\leftrightarrow\)}
\end{equation}
holds whenever 
both \(O_1\) and \(O_2\) have a determinant. Moreover for any  \((P^- S_{X,Y}P^-)^*P^- S_{X,Y}P^-\) is the modulus squared of an invertible operator and hence
its determinant is positive which means that 
\begin{equation}\label{tetraeder 3}
\ag \det (P^- S_{X,Y}P^-)^*P^- S_{X,Y}P^-=1.\tag{\(\ag\!|\,\, |\)}
\end{equation}
These three rules will be repeatedly used. We calculate
\begin{align}
&\Gamma_{A,B,C}\Gamma_{B,A,D}\Gamma_{A,C,D}\Gamma_{C,B,D}\\\notag
&=\ag \det_{\mathcal{H}^-}P^- S_{A,B}P^- S_{B,C}P^- S_{C,A}P^- \\
&\hspace{1,5cm}\times\ag \det_{\mathcal{H}^-}P^- S_{B,A}P^- S_{A,D}P^- S_{D,B}P^-~~ \Gamma_{A,C,D}\Gamma_{C,B,D}\\\notag
&\overset{\eqref{tetraeder 1}}{=}
\ag \det_{\mathcal{H}^-}P^-S_{A,D}P^- S_{D,B}P^- S_{B,A}P^- \\
&\hspace{1.5cm}\times \ag \det_{\mathcal{H}^-}P^- S_{A,B}P^- S_{B,C}P^- S_{C,A}P^- ~~  \Gamma_{A,C,D}\Gamma_{C,B,D}\\\notag
&\overset{\eqref{tetraeder 2}}{=}
\ag \det_{\mathcal{H}^-}\big(P^-S_{A,D}P^- S_{D,B} \big[P^- S_{B,A} P^- S_{A,B}P^-\big]\\
&\hspace{5cm}\times S_{B,C}P^- S_{C,A}P^-\big) ~ \Gamma_{A,C,D}\Gamma_{C,B,D}\\\notag
&\overset{\eqref{tetraeder 1}}{=}
\ag \det_{\mathcal{H}^-}P^-S_{B,C}P^- S_{C,A}P^-S_{A,D}P^- S_{D,B} \big[P^- S_{B,A} P^- S_{A,B}P^-\big]  \\
&\hspace{8cm}\times \Gamma_{A,C,D}\Gamma_{C,B,D}\\\notag
&\overset{\eqref{tetraeder 2}}{=}
\ag \det_{\mathcal{H}^-}P^-S_{B,C}P^- S_{C,A}P^-S_{A,D}P^- S_{D,B}P^-\\
&  \hspace{4cm}\times\ag \det_{\mathcal{H}^-} P^- S_{B,A} P^- S_{A,B}P^-  ~~ \Gamma_{A,C,D}\Gamma_{C,B,D}\\
&\overset{\eqref{tetraeder 3}}{=}
\ag \det_{\mathcal{H}^-}P^-S_{B,C}P^- S_{C,A}P^-S_{A,D}P^- S_{D,B}P^-   ~~ \Gamma_{A,C,D}\Gamma_{C,B,D}\\
&\overset{\eqref{tetraeder 1}}{=}
\ag \det_{\mathcal{H}^-}P^-S_{A,D}P^- S_{D,B}P^-S_{B,C}P^- S_{C,A}P^-   ~~ \Gamma_{A,C,D} \Gamma_{C,B,D}\\\notag
&=\ag \det_{\mathcal{H}^-}P^- S_{A,C}P^- S_{C,D}P^- S_{D,A}P^-\\
&\hspace{1.5cm}\times\ag \det_{\mathcal{H}^-}P^-S_{A,D}P^- S_{D,B}P^-S_{B,C}P^- S_{C,A}P^-   ~~ \Gamma_{C,B,D}\\\notag
&\overset{\eqref{tetraeder 2}}{=}
\ag \det_{\mathcal{H}^-}\big(P^- S_{A,C}P^- S_{C,D}P^-\big[ P^- S_{D,A}P^-P^-S_{A,D}P^- \big]\\
&\hspace{4cm}\times  S_{D,B}P^-S_{B,C}P^- S_{C,A}P^-\big)   ~~ \Gamma_{C,B,D}\\\notag
&\overset{\eqref{tetraeder 1}}{=}
\ag \det_{\mathcal{H}^-}\big(P^- S_{D,B}P^-S_{B,C}P^- S_{C,A}P^- S_{A,C}P^- S_{C,D}P^-\\
&\hspace{4cm}\times\big[P^- S_{D,A}P^-P^-S_{A,D}P^- \big]\big)    ~~ \Gamma_{C,B,D}\\\notag
&\overset{\eqref{tetraeder 2}}{=}
\ag \det_{\mathcal{H}^-}P^- S_{D,B}P^-S_{B,C}P^- S_{C,A}P^- S_{A,C}P^- S_{C,D}P^- \\
&\hspace{4cm}\times\ag \det_{\mathcal{H}^-} P^-S_{D,A}P^-P^-S_{A,D}P^-    ~~ \Gamma_{C,B,D}\\\notag
&\overset{\eqref{tetraeder 3}}{=}
\ag \det_{\mathcal{H}^-}\big(P^- S_{D,B}P^-S_{B,C}P^-\big[P^- S_{C,A}P^- S_{A,C}P^-\big]\\
&\hspace{7cm}\times P^- S_{C,D}P^-\big)     \Gamma_{C,B,D}\\\notag
&\overset{\eqref{tetraeder 1}}{=}
\ag \det_{\mathcal{H}^-}P^- S_{C,D}P^- S_{D,B}P^-S_{B,C}P^- \big[P^- S_{C,A}P^- S_{A,C}P^-\big]\\     
&\hspace{9cm}\times \Gamma_{C,B,D}\\\notag
&\overset{\eqref{tetraeder 2}}{=}
\ag \det_{\mathcal{H}^-}P^- S_{C,D}P^- S_{D,B}P^-S_{B,C}P^-\\
&\hspace{4cm}\times \ag \det P^- S_{C,A}P^- S_{A,C}P^-     ~~ \Gamma_{C,B,D}\\
&\overset{\eqref{tetraeder 3}}{=}
\ag \det_{\mathcal{H}^-}P^- S_{C,D}P^- S_{D,B}P^-S_{B,C}P^-  ~~ \Gamma_{C,B,D}\\\notag
&=
\ag \det_{\mathcal{H}^-}P^- S_{C,D}P^- S_{D,B}P^-S_{B,C}P^- \\
&\hspace{4cm}\times \ag \det_{\mathcal{H}^-} P^- S_{C,B}P^- S_{B,D}P^- S_{D,C}P^- \\
&=
|\ag \det_{\mathcal{H}^-}P^- S_{C,D}P^- S_{D,B}P^-S_{B,C}P^-|^2=1. 
\end{align}

Equation \eqref{gamma attri6} is a direct consequence of \eqref{gamma attri5} and \eqref{gamma attri3}.

For \eqref{gamma attri7} we realise that according to \cite{ivp0} that two lifts can only differ by a phase,
that is
\begin{equation}
\overline{S}_{A,C}=\alpha \overline{S}_{A,B} \overline{S}_{B,C}
\end{equation}
for some \(\alpha\in \mathbb{C}, |\alpha|=1\). 

In order to identify \(\alpha\) we recognise that \(\overline{S}_{X,Y}=\overline{S}_{Y,X}^{-1}\) for four potentials \(X,Y\) 
and find
\begin{equation}
1 \alpha^{-1}= \overline{S}_{A,B} \overline{S}_{B,C}\overline{S}_{C,A}.
\end{equation}
Now we take the vacuum expectation value on both sides of this equation and use \eqref{gamma attri2} to find
\begin{equation}
\alpha^{-1}=\langle\Omega,\overline{S}_{A,B} \overline{S}_{B,C}\overline{S}_{C,A}\Omega\rangle = \Gamma_{A,B,C}^{-1}.
\end{equation}

Finally we prove \eqref{gamma attri8}. We start from the right hand side of this equation 
and work our way towards the left hand side of it. In the following calculation we will repeatedly make use of the fact that 
\((P^-S_{A,A+B}P^-S_{A+B,A}P^- )\) is the absolute value squared of an invertible operator and has a determinant, which is therefore positive. 
For the marked equality we will use that for a differentiable function \(z:\mathbb{R}\rightarrow \mathbb{C}\) at points \(t\) where \(z(t)\in\mathbb{R}^+\)
holds, we have
\begin{align}\notag
(z/|z|)'(t)=\frac{z'}{|z|}(t)+\frac{-z}{|z|^2}\frac{z'z^*+ {z^*}'z}{2|z|}(t) =\frac{z'}{2|z|}(t)-\frac{z^2{z^*}'}{2 |z|^3}(t)\\
=i (\Im (z'))/z(t).
\end{align}
Furthermore, we will use the following  expressions for the derivative of the determinant which holds for all functions 
\(M:\mathbb{R}\rightarrow (\mathcal{H}\rightarrow \mathcal{H})\) such that \(M(t)-1\) is traceclass and \(M\) is invertible 
for all \(t\in\mathbb{R}\)
\begin{align}\label{diff det}
\partial_\varepsilon \det M(\varepsilon)|_{\varepsilon=0}=\det M(0) \tr (M^{-1}(0)\partial_\varepsilon M(\varepsilon)|_{\varepsilon}),
\end{align}
likewise we need the following expression for the derivative of \(M^{-1}\) for 
\(M:\mathbb{R}\rightarrow (\mathcal{H}\rightarrow \mathcal{H})\) such that \(M(t)\) is invertible and bounded for every \(t\in\mathbb{R}\)
\begin{equation}
\partial_{\varepsilon}M^{-1}(\varepsilon)|_{\varepsilon=0}=-M^{-1}(0) \partial_{\varepsilon}M(\varepsilon)|_{\varepsilon=0}M^{-1}(0).
\end{equation}
We compute
\begin{align}
\partial_B \partial_C \ln \Gamma_{A,A+B,A+C}\\
\overset{\eqref{gamma attri1}}{=} 
\partial_B \partial_C \ln \ag(\det_{\mathcal{H}^-} (P^-S_{A,A+B}P^-S_{A+B,A+C}P^- S_{A+C,A}P^-))\\
=\partial_B \frac{\partial_C\ag(\det_{\mathcal{H}^-}(P^-S_{A,A+B}P^-S_{A+B,A+C}P^- S_{A+C,A}P^-))}{\ag(\det_{\mathcal{H}^-} (P^-S_{A,A+B}P^-S_{A+B,A}P^- ))}\\
=\partial_B \partial_C\ag(\det_{\mathcal{H}^-}(P^-S_{A,A+B}P^-S_{A+B,A+C}P^- S_{A+C,A}P^-))\\
\overset{*}{=}
i\partial_B \frac{\Im \partial_C \det_{\mathcal{H}^-}(P^-S_{A,A+B}P^-S_{A+B,A+C}P^- S_{A+C,A}P^-)}{\det_{\mathcal{H}^-}(P^-S_{A,A+B}P^-S_{A+B,A}P^-)}\\\notag
=i\partial_B \Big[\frac{\det_{\mathcal{H}^-}(P^-S_{A,A+B}P^-S_{A+B,A}P^-)}{\det_{\mathcal{H}^-}(P^-S_{A,A+B}P^-S_{A+B,A}P^-)}\\\notag
\times \Im  \tr ((P^-S_{A,A+B}P^-S_{A+B,A}P^-)^{-1}\\
\times\partial_C P^-S_{A,A+B}P^-S_{A+B,A+C}P^- S_{A+C,A}P^-)\Big]\label{eq ca 1}
\end{align}
The fraction in front of the trace equals \(1\). As a next step we replace the second but last projector \(P^-=1-P^+\), the resulting first summand vanishes,
because the dependence on \(C\) cancels. This results in 
\begin{align}\notag
&\eqref{eq ca 1}=-i\partial_B \Im \tr((P^-S_{A,A+B}P^-S_{A+B,A}P^-)^{-1}\\
&\hspace{3cm}\times\partial_C P^-S_{A,A+B}P^-S_{A+B,A+C}P^+ S_{A+C,A}P^-).\label{eq ca 2}
\end{align}
Now, because \(P^+P^-=0\) only one summand of the product rule survives:
\begin{align}\notag
&\eqref{eq ca 2}=-i\partial_B \Im \tr((P^-S_{A,A+B}P^-S_{A+B,A}P^-)^{-1}\\
&\hspace{3cm}\times\partial_C P^-S_{A,A+B}P^-S_{A+B,A}P^+ S_{A+C,A}P^-).\label{eq ca 3}
\end{align}
Next we use 
\((M N )^{-1}= N^{-1} M^{-1}\) for invertible operators \(M\) and \(N\) for the first factor in the trace and cancel as much as possible of
the second factor:
\begin{align}\notag
\eqref{eq ca 3}=-i\partial_B \Im \tr((P^-S_{A+B,A}P^-)^{-1}  P^-S_{A+B,A}\\
\times P^+ \partial_C S_{A+C,A}P^-)\\\notag
=-i \Im \tr(\partial_B[(P^-S_{A+B,A}P^-)^{-1}  P^-S_{A+B,A}\\
\times P^+ \partial_C S_{A+C,A}P^-])\\
=-i \Im \tr(\partial_B  P^-S_{A+B,A}P^+ \partial_C S_{A+C,A}P^-)\\
=-i \Im \tr(\partial_B  P^-S_{A,A+B}P^+ \partial_C S_{A,A+C}P^-)\\
=-i \partial_B\partial_C\Im \tr(  P^-S_{A,A+B}P^+  S_{A,A+C}P^-)
\end{align}
which proves the claim.

\end{proof}



In order to construct the lift announced in theorem \ref{thm: geometry}, we first construct a reference lift \(\hat{S}\), that is well defined on all of \(\mathcal{V}\). 
Afterwards we will study the dependence of the relative phase between 
this global lift \(\hat{S}_{0,A}\) and a local lift given by \(\hat{S}_{0,B}\overline{S}_{B,A}\) for \(B-A\) small. 
By exploiting properties of this phase and the causal splitting \(c^+\) we will construct a global lift that has the desired properties.

Since \(\mathcal{V}\) is star shaped, we may reach any four-potential \(A\) from \(0\) through the straight line
\(\{t A\mid t \in [0,1]\}\). 

\begin{Def}[ratio of lifts]\label{def:ratio}
For any  \(A,B\in\mathcal{V}\) and any two lifts \(S_{A,B}', S_{A,B}''\) of the one particle scattering operator \(S_{A,B}\)
we define the ratio
\begin{equation}
\frac{S_{A,B}'}{S_{A,B}''}\in S^1
\end{equation}
to be the unique complex number \(z\in S^1\) such that 
\begin{equation}
z ~S_{A,B}'' = S_{A,B}'
\end{equation}
holds.
\end{Def}

\begin{Thm}[existence of global lift]\label{thm: ex s hat}
  There is a unique map \(\hat{S}_{0,\cdot}:\mathcal{V}\to U(\mathcal{F})\) 
    which maps  \(A\in\mathcal{V}\) to a lift of \(S_{0,A}\) and solves the 
    parallel transport differential equation
  \begin{equation}\label{diff s hat}
A,B\in\mathcal{V}\text{ linearly dependent}\Rightarrow \partial_B \frac{\hat{S}_{0,A+B}}{\hat{S}_{0,A}\overline{S}_{A,A+B}}=0,
\end{equation}
subject to the initial condition \(\hat{S}_{0,0}=1\).
\end{Thm}

The proof of theorem \ref{thm: ex s hat} is divided into two lemmas due to its length. We will introduce the integral flow \(\phi_A\) associated 
with the differential equation \eqref{diff s hat} for some \(A\in\mathcal{V}\). We will then study the properties of \(\phi_A\)
in the two lemmas and finally construct \(\hat{S}_{0,A}=1 \phi_A(0,1)\). In the first lemma we will establish the existence of a 
local solution. The solution will be constructed along the line \(\overline{0 ~~ A}\). In the second lemma we patch local solutions together
to a global one.



\begin{Lemma}[\(\phi\) local existence and uniqueness]\label{lem phi local}
There is a unique \(\phi_A:\{(t,s)\in\mathbb{R}^2\mid (t A, s A)\in \dom\overline{S}\} \rightarrow U(\mathcal{F})\) for every \(A\in\mathcal{V}\)
satisfying
\begin{align}\label{phi prop1}
\forall (t,s)\in \dom \phi_A: \phi_A(t,s) \text{ is a lift of } S_{tA, sA}\\\label{phi prop2}
\forall (t,s),(s,l),(l,t)\in \dom\phi_A: \phi_A(t,s)\phi_A(s,l)=\phi_A(t,l)\\\label{phi prop3}
\forall t\in\mathbb{R}: \phi_A(t,t)=1\\\label{phi prop4}
\forall s\in\mathbb{R}: \partial_{t} \left.\frac{\phi_A(s,t)}{\overline{S}_{sA,tA}}\right|_{t=s}=0.
\end{align}
\end{Lemma}
\begin{proof}
We first define the phase
\begin{align}\label{def local z}
z:\{(A,B)\in\dom\overline{S}\mid A, B \text{ linearly dependent}\} \rightarrow S^1
\end{align}
by the differential equation
\begin{equation}
\frac{d}{d x} \ln z(t A, x A) = - \left.\left( \frac{d}{dy} \ln \Gamma_{tA,x A, yA}\right)\right|_{y=x}
\end{equation}
and the initial condition 
\begin{equation}\label{z initial}
z(A,A)=1
\end{equation}
 for any \(A\in \mathcal{V}\). The phase \(z\) takes the form
\begin{equation}\label{z solution}
z(tA,xA)=\exp\left(-\int_{t}^x dx' \left.\left( \frac{d}{dx'} \ln \Gamma_{tA,y A, x' A}\right)\right|_{y=x'}\right).
\end{equation}
Please note that both differential equation and initial condition are invariant under rescaling of the potential \(A\), so \(z\) is well defined. 
We will now construct a local solution to \eqref{diff s hat} and define \(\phi_A\) using this solution.
Pick \(A\in \mathcal{V}\) the expression
\begin{equation}\label{loc s hat}
\hat{S}_{0,s A} = \hat{S}_{0, A} \overline{S}_{ A, s A} z( A, s A)
\end{equation}
solves \eqref{diff s hat} locally. Local here means that \(s\) is close enough to \(1\) such that \(( A, s A)\in\dom\overline{S}\).
Calculating the argument of the derivative of \eqref{diff s hat} we find:
\begin{align}
0= \frac{\hat{S}_{0,(s+\varepsilon) A}}{\hat{S}_{0,s A}\overline{S}_{s A, (s+\varepsilon)A}}
= \frac{\hat{S}_{0,A} \overline{S}_{A, (s+\varepsilon)A} z(A, (s+\varepsilon)A)}
{\hat{S}_{0, A} \overline{S}_{ A, s A} \overline{S}_{s A,(s+\varepsilon)}z( A, s A)}\\
\overset{\eqref{gamma attri6}}{=}
 \frac{\hat{S}_{0,A}\overline{S}_{ A, s A} \overline{S}_{s A,(s+\varepsilon)} \Gamma_{A, sA, (s+\varepsilon)A} z(A, (s+\varepsilon)A)}
{\hat{S}_{0, A} \overline{S}_{t A, s A} \overline{S}_{s A,(s+\varepsilon)}z( A, s A)}\\
= \frac{\Gamma_{tA,sA,(s+\varepsilon)A}z(tA, (s+\varepsilon)A)}{z( A, sA)}
\end{align}
Now we take the derivative with respect to \(\varepsilon\) at \(\varepsilon=0\), 
cancel the factor that does not depend on \(\varepsilon\) and relabel \(s=x\) to obtain
\begin{align}
0=\left.\left(\frac{d}{dy}(\Gamma_{ A, x A, y A} ~z( A, y A))\right)\right|_{y=x}\\
\iff \frac{d}{dx}\ln z(tA, xA)=\left.\left(-\frac{d}{dy}\ln \Gamma_{t A, xA, yA}\right)\right|_{y=x},
\end{align}
which is exactly the defining differential equation of \(z\). The initial condition of \(z\) equation \eqref{z initial} 
is necessary to match the initial condition in \eqref{loc s hat} for \(s=1\).
The connection to \(\phi\) from the statement of the lemma can now be made.
We define 
\begin{equation}\label{def phi}
\phi_A(t,s):=z(t A, s A) \overline{S}_{t A, s A},
\end{equation}
for \((tA,sA)\in\dom\overline{S}\). Since \(\overline{S}\) is a lift of \(S\), we see that
 \eqref{phi prop1} holds. Equation \eqref{phi prop3} follows from \eqref{z initial} and 
 \(\overline{S}_{tA, tA}=1\) for general \(t\in\mathbb{R}\).
 Equation \eqref{phi prop4} follows by plugging in 
\eqref{def phi} and using the differential equation for \(z\) \eqref{z initial}:
\begin{align}
\partial_s \left.\frac{\phi_A(t,s)}{\overline{S}_{t A,sA}}\right|_{s=t}
=\partial_s \left.\frac{z(tA,sA)\overline{S}_{t A, sA}}{\overline{S}_{t A, s A}}\right|_{s=t}\\
=\partial_t \left.z(tA, sA)\right|_{t=s}=0.
\end{align}


It remains to see that \eqref{phi prop2}, i.e. that 

\begin{align}\label{consistency phi}
\phi_A(t,s)\phi_A(s,l)=\phi_A(t,l)
\end{align}
holds for \((tA,sA),(sA,lA),(tA,lA)\in\dom\overline{S}\). 
In order to do so we plug in the definition \eqref{def phi} of \(\phi_A\) and obtain
\begin{align}
\phi_A(t,s)\phi_A(s,l)=\phi_A(t,l)\\
\iff z(tA,sA) z(sA,lA)\overline{S}_{tA,sA}\overline{S}_{sA,lA}=z(tA,lA)\overline{S}_{tA,lA}\\
\iff z(tA,sA) z(sA,lA)\overline{S}_{tA,sA}\overline{S}_{sA,lA}\\
=z(tA,lA)\overline{S}_{tA,s A} \overline{S}_{sA,lA}\Gamma_{tA,sA,lA}\\
\iff z(tA,sA) z(sA,lA)z(tA,lA)^{-1}=\Gamma_{tA,sA,lA}.
\end{align}
In order to check the validity of the last equality we plug in the 
ntegral formula \eqref{z solution} for \(z\), we also abbreviate 
\(\frac{d}{d x}=\partial_x\)

\begin{align}
z(tA,sA) z(sA,lA)z(tA,lA)^{-1}\\
=e^{-\int_t^s d x' \left.(\partial_{x'} \ln \Gamma_{tA,yA,x'A} )\right|_{y=x'} -\int_s^l d x' \left.(\partial_{x'} \ln \Gamma_{sA,yA,x'A} )\right|_{y=x'} }\\
\times e^{   +\int_t^l d x' \left.(\partial_{x'} \ln \Gamma_{tA,yA,x'A} )\right|_{y=x'}  }\\
=e^{-\int_l^s d x' \left.(\partial_{x'} \ln \Gamma_{tA,yA,x'A} )\right|_{y=x'}  -\int_s^l d x' \left.(\partial_{x'} \ln \Gamma_{sA,yA,x'A} )\right|_{y=x'} }\\
\overset{\eqref{gamma attri6}}{=}
e^{-\int_l^s d x' \left.(\partial_{x'} \ln \Gamma_{sA,yA,x'A} )\right|_{y=x'} 
-\int_l^s d x' \left.(\partial_{x'} \ln \Gamma_{tA,sA,x'A} )\right|_{y=x'}}\\ 
\times e^{-\int_l^s d x' \left.(\partial_{x'} \ln \Gamma_{tA,yA,sA} )\right|_{y=x'} 
 -\int_s^l d x' \left.(\partial_{x'} \ln \Gamma_{sA,yA,x'A} )\right|_{y=x'} }\\
 =e^{-\int_l^s d x' \left.(\partial_{x'} \ln \Gamma_{tA,sA,x'A} )\right|_{y=x'}}\\
  =e^{-\int_l^s d x'\partial_{x'} \ln \Gamma_{tA,sA,x'A} }\\
  \overset{\eqref{gamma attri4}}{=}\Gamma_{t A, s A, l A},
\end{align}
which proves the validity of the consistency relation 
\eqref{consistency phi}.

In order to prove uniqueness we pick \(A\in \mathcal{V}\) 
and assume there is \(\phi'\) also defined on \(\dom \phi_A\) 
and satisfies \eqref{phi prop1}-\eqref{phi prop4}. Then we may 
use \eqref{phi prop1} to conclude that for any \((t,s)\in\dom\phi_A\) 
there is
\(\gamma(t,s)\in S^1\) such that
\begin{equation}
\phi_A(t,s)=\phi'(t,s) \gamma(t,s)
\end{equation}
holds true. Picking \(l\) such that \((t,s),(s,l),(t,l)\in \dom\phi_A\) and  using \eqref{phi prop2} we find

\begin{align}
\phi'(t,s) \gamma(t,s)=\phi_A(t,s)=\phi_A(t,l)\phi_A(l,s)\\
=\gamma(t,l)\phi'(t,l) \gamma(l,s)\phi'(l,s)=\gamma(t,l) \gamma(l,s)\phi'(t,s) ,
\end{align}
hence we have
\begin{equation}
 \gamma(t,s)=\gamma(t,l) \gamma(l,s).
\end{equation}
From property \eqref{phi prop3} we find
\begin{equation}
\gamma(t,t)=1,
\end{equation}
for any \(t\).
Using equation \eqref{phi prop4} we conclude that 

\begin{align}
0=\partial_t \left.\frac{\phi'(s,t)}{\overline{S}_{sA,tA}}\right|_{t=s}
=\partial_t \left.\frac{\phi_A(s,t) \gamma(s,t)}{\overline{S}_{sA,tA}}\right|_{t=s}\\
=\partial_t \left.\gamma(s,t) \frac{\phi_A(s,t)}{\overline{S}_{sA,tA}}\right|_{t=s}
=\partial_t \left.\gamma(s,t)\right|_{t=s} + \partial_t  \left.\frac{\phi_A(s,t)}{\overline{S}_{sA,tA}}\right|_{t=s}\\
=\partial_t \left.\gamma(s,t)\right|_{t=s}.
\end{align}
Finally we find for general \((s,t)\in \dom\phi_A\):
\begin{equation}
\partial_x \gamma(s,x)|_{x=t}=\partial_x ( \gamma(s,t) \gamma(t,x))|_{x=t}=\gamma(s,t) \partial_x \gamma(t,x)|_{x=t}=0.
\end{equation}
So \(\gamma(t,s)=1\) everywhere. We conclude \(\phi_A=\phi'\).

\end{proof}



\begin{Lemma}[\(\phi\) global existence and uniqueness]\label{lem: phi global}
For any \(A\in\mathcal{V}\) the map \(\phi_A\) constructed in 
lemma \ref{lem phi local} 
can  be uniquely extended to all of \(\mathbb{R}^2\)
keeping its defining properties 
\begin{align}\label{global phi prop1}
\forall (t,s)\in \mathbb{R}^2: \phi_A(t,s) \text{ is a lift of } S_{tA, sA}\\\label{global phi prop2}
\forall (t,s),(s,l),(l,t)\in \mathbb{R}^2: \phi_A(t,s)\phi_A(s,l)=\phi_A(t,l)\\\label{global phi prop3}
\forall t\in\mathbb{R}: \phi_A(t,t)=1\\\label{global phi prop4}
\forall s\in\mathbb{R}: \partial_{t} \left.\frac{\phi_A(s,t)}{\overline{S}_{sA,tA}}\right|_{t=s}=0.
\end{align}
\end{Lemma}
\begin{proof}
Pick \(A\in\mathcal{V}\). For \(x\in\mathbb{R}\) we define the set
\begin{equation}\label{def U x}
U_x:=\{y\in\mathbb{R}\mid (x A, y A)\in \dom\overline{S}\},
\end{equation}
which according to properties 2 and 4 of lemma 
\ref{lem properties of dom s bar} is an open interval and
fulfills that \(\bigcup_{x\in\mathbb{R}} U_x\times U_x\) is an open 
neighbourhood of the diagonal \(\{(x,x)\mid x \in\mathbb{R}\}\).
Therefore \(\phi_A\) is defined for arguments that are
close enough to each other. Since 
properties \eqref{global phi prop4} and \eqref{global phi prop3} 
only concern the behavior of \(\phi_A\) at the diagonal any
extension fulfils them. 

We pick a sequence \((x_k)_{k\in\mathbb{N}}\subset \mathbb{R}\) such
that 
\begin{equation}\label{open cover of R}
\bigcup_{k\in\mathbb{N}_0} U_{x_k} = \mathbb{R}
\end{equation}
holds and 
\begin{equation}
  \forall n \in \mathbb{N}_0: \bigcup_{k=0}^n U_{x_k}=:\dom_n
\end{equation}
is an open interval. Please note that such a sequence always exists. 
We are going to prove that for any \(n\in\mathbb{R}_0\) 
There is a function \(\psi_n:\dom_n\times\dom_n\to U(\mathcal{F})\), 
which satisfies the conditions
\begin{align}\label{induction psi1}
\forall (t,s)\in \dom_n\times \dom_n: \psi_n(t,s) \text{ is a lift of } S_{tA, sA}\\\label{induction psi2}
\forall s,k,l\in\dom_n: \psi_n(k,s)\psi_n(s,l)=\psi_n(k,l)\\\label{induction psi3}
\forall x,y\in\dom_n: (xA, yA)\in\dom\overline{S}\Rightarrow  \psi_n(x,y)=\phi_A(x,y)
\end{align}
and is the unique function to do so, i.e. 
any other function \(\tilde{\psi}_{n}\) fulfilling properties 
\eqref{induction psi1}-\eqref{induction psi3} possibly being defined
on a larger domain coincides with \(\psi_n\) on \(\dom_n\times\dom_n\).

We start with \(\psi_0=\phi_A\)  
restricted to \(U_{x_0} \times U_{x_0}\).
This function is a restriction of \(\phi_A\) and because of 
lemma \ref{lem phi local} it 
fulfils all of the required properties directly. 
 
For the induction step we 
define \(\psi_{n+1}\) on the domain 
\(\dom_{n+1}\times \dom_{n+1}\) by
 \begin{equation}\label{def psi induction}
 \psi_{n+1}(x,y):=\left\{\begin{matrix}
 \psi_n(x,y) \quad &\text{for }x,y\in \dom_n\\
 \phi_A(x,y) \quad &\text{for } x,y\in U_{x_{n+1}}\\
 \psi_n(x,t)\phi_A(t,y)\quad &\text{for } x\in \dom_n, y\in U_{x_{n+1}}\\
 \phi_A(x,t)\psi_n(t,y)\quad &\text{for } y\in \dom_n, x\in U_{x_{n+1}}.
 \end{matrix} \right.
 \end{equation} 
In order to complete the induction step we have to show that 
\(\psi_{n+1}\) is well defined and fulfils properties
\eqref{induction psi1}-\eqref{induction psi3} 
with \(n\) replaced by \(n+1\) and is the unique function 
to do so.
  
To see that \(\psi_{n+1}\) is well defined we have to check 
that the cases in the definition agree when they overlap. 
\begin{enumerate}
  \item If we have \(x,y \in \dom_n\cap U_{x_{n+1}}\) all four cases overlap; however, the 
  alternative definitions all equal \(\phi_A(x,y)\):
\begin{align}\nonumber
  \psi_n(x,y)\overset{\eqref{induction psi3}}{=}\phi_A(x,y)\overset{\eqref{consistency phi}}{=}\phi_A(x,t) \phi_A(t,y)
  \\
\overset{\eqref{induction psi3}}{=}\left\{\begin{matrix}\psi_A(x,t) \phi_n(t,y)\\ \phi_A(x,t) \psi_n(t,y).\end{matrix}\right.
  \end{align}
  \item Furthermore, if we have \(x\in \dom_n\), \(y\in \dom_n\cap U_{x_{n+1}}\) case one 
  and three overlap. Here both calternatives are equal to
  \(\psi_n(x,y)\), since \(x,y\in \dom_n\) and we obtain:
  \begin{equation}
  \psi_n(x,y)\overset{\eqref{induction psi2}}{=}\psi_n(x,t)\psi_n(t,y)\overset{\eqref{induction psi3}}{=}\psi_n(x,t)\phi_A(t,y).
  \end{equation}
  \item Additionally, if \(y\in \dom_n\), \(x\in \dom_n\cap U_{x_{n+1}}\) case one and four overlap. 
  Here they are equal to
  \(\psi_n(x,y)\), since \(x,y\in \dom_n\) a quick calculation yields:
  \begin{equation}
  \psi_n(x,y)\overset{\eqref{induction psi2}}{=}\psi_n(x,t)\psi_n(t,y)\overset{\eqref{induction psi3}}{=}\psi_A(x,t)\psi_n(t,y).
  \end{equation}
  \item Moreover, if we have \(y\in U_{x_{n+1}}\), \(x\in \dom_n\cap U_z\) 
  case two and three overlap. Here both candidate definitions are equal to
  \(\phi_A(x,y)\), since \(x,t\in U_z\) we arrive at:
  \begin{equation}
  \phi_A(x,y)\overset{\eqref{consistency phi}}{=}\phi_A(x,t)\phi_A(t,y)\overset{\eqref{induction psi3}}{=}\psi_n(x,t)\phi_A(t,y).
  \end{equation}
  \item Also, if we have \(x\in U_{x_{n+1}}\), \(y\in \dom_n\cap U_{x_{n+1}}\) 
  case two and four overlap. In this case both alternatives are equal to
  \(\phi_A(x,y)\), since \(y,t\in U_{x_{n+1}}\) we get:
    \begin{equation}
  \phi_A(x,y)\overset{\eqref{consistency phi}}{=}\phi_A(x,t)\phi_A(t,y)\overset{\eqref{induction psi3}}{=}\phi_A(x,t)\psi_n(t,y).
  \end{equation}
  \end{enumerate}
We proceed to show the induction claim, 
starting with \(\eqref{induction psi1}_{n+1}\). By the induction 
hypothesis we know that \(\psi_n(x,y)\) as well as 
\(\phi_A(x,y)\) are lifts of \(S_{xA,yA}\) for any \((x,y)\) in their 
domain of definition. Therefore we have for \(x,y\in \dom_n \cup U_{x_{n+1}}\)
 \begin{equation}\tag{\ref{def psi induction}}
 \psi_{n+1}(x,y)=\left\{\begin{matrix}
 \psi_n(x,y) \quad &\text{for }x,y\in \dom_n,\\
 \phi_A(x,y) \quad &\text{for } x,y\in U_{x_{n+1}},\\
 \psi_n(x,t)\phi_A(t,y)\quad &\text{for } x\in \dom_n, y\in U_{x_{n+1}},\\
 \phi_A(x,t)\psi_n(t,y)\quad &\text{for } y\in \dom_n, x\in U_{x_{n+1}},
 \end{matrix} \right.
 \end{equation} 
where each of the lines is a lift of \(S_{xA,y A}\) whenever the expression is defined.


Equation \(\eqref{induction psi2}_{n+1}\) we will again show in a case 
by case manner depending on the \(s,k\) and \(l\):
\begin{enumerate}
\item \(s,k,l\in \dom_{n}\): 
\(\eqref{induction psi2}_{n+1}\) follows directly from the induction hypothesis;
\item  \(s,k\in \dom_{n}\) and \(l\in U_{x_{n+1}}\):
\begin{align}\nonumber
\psi_{n+1}(s,k)\psi_{n+1}(k,l)=\psi_n(s,k)\psi_n(k,t)\phi_A(t,l)\\
\overset{\eqref{induction psi2}}{=} \psi_n(s,t)\phi_A(t,l)=\psi_{n+1}(s,l),
\end{align}
\item \(s,l\in \dom_n\) and  \(k\in U_{x_{n+1}}\): 
\begin{align*}
\psi_{n+1}(s,k)\psi_{n+1}(k,l)=\psi_{n}(s,t)\phi_A(t,k)\phi_A(t,k)\psi_n(t,l)\\
\overset{\eqref{phi prop3},\eqref{phi prop2}}{=}\psi_{n}(s,t)\psi_n(t,l)\overset{\eqref{induction psi2}}{=}\psi_n(s,l)=\psi_{n+1}(s,l),
\end{align*}
\item  \(s\in \dom_n\) and  \(k,l\in U_{x_{n+1}}\):
\begin{align*}
\psi_{n+1}(s,k)\psi_{n+1}(k,l)=\psi_n(s,t)\phi_A(t,k)\phi_A(k,l)\\
\overset{\eqref{phi prop2}}{=}\psi_n(s,t)\phi_A(t,l)
=\psi_{n+1}(s,l),
\end{align*}
\item \(k,l\in \dom_n\) and  \(s\in U_{x_{n+1}}\): 
\begin{align*}
\psi_{n+1}(s,k)\psi_{n+1}(k,l)=\phi_A(s,t)\psi_n(t,k)\psi_n(k,l)\\
\overset{\eqref{induction psi2}}{=}\phi_A(s,t)\psi_n(t,l)=\psi_{n+1}(s,l),
\end{align*}
\item  \(k\in \dom_n\) and  \(s,l\in U_{x_{n+1}}\):
\begin{align*}
\psi_{n+1}(s,k)\psi_{n+1}(k,l)=\phi_A(s,t)\psi_n(t,k)\psi_n(k,t)\phi_A(t,l)\\
\overset{\eqref{induction psi2}}{=}\phi_A(s,t)\psi(t,t)\phi_A(t,l)
\overset{\eqref{induction psi3},\eqref{phi prop3}}{=}\phi_A(s,t)\phi_A(t,l)\\
\overset{\eqref{phi prop2}}{=} \phi_A(s,l)=\psi_{n+1}(s,l),
\end{align*}
\item \(l\in \dom_n\) and  \(s,k\in U_{x_{n+1}}\):
\begin{align*}
\psi_{n+1}(s,k)\psi_{n+1}(k,l)=\phi_A(s,k)\phi_A(k,t)\psi_n(t,l)\\
\overset{\eqref{phi prop2}}{=}\phi_A(s,t)\psi_n(t,l)=\psi_{n+1}(s,l),
\end{align*}
\item and if \(s,k,l\in U_z\): 
\begin{align*}
\psi_{n+1}(s,k)\psi_{n+1}(k,l)=\phi_A(s,k)\phi_A(k,l)\\
\overset{\eqref{phi prop2}}{=}\phi_A(s,l)=\psi_{n+1}(s,l).
\end{align*}
\end{enumerate}

To see \(\eqref{induction psi3}_{n+1}\), i.e. that  \(\psi_{n+1}\) is 
coincides with \(\phi_A\) where both functions are defined pick 
\(x,y\in \dom_{n+1}\) such that \((xA,yA)\in\dom\overline{S})\).
Recall the definition of \(\psi_{n+1}\)

\begin{equation}\tag{\ref{def psi induction}}
  \psi_{n+1}(x,y)=\left\{\begin{matrix}
  \psi_n(x,y) \quad &\text{for }x,y\in \dom_n,\\
  \phi_A(x,y) \quad &\text{for } x,y\in U_{x_{n+1}},\\
  \psi_n(x,t)\phi_A(t,y)\quad &\text{for } x\in \dom_n, y\in U_{x_{n+1}},\\
  \phi_A(x,t)\psi_n(t,y)\quad &\text{for } y\in \dom_n, x\in U_{x_{n+1}}.
  \end{matrix} \right.
\end{equation} 

Therefore if \(x,y\in\dom_{n+1}\) we may use the induction hypothesis directly
and if \(x,y\in U_{x_{n+1}}\) we also arrived a the claim we want to prove.
Excluding these cases, we are left with rows number three and four of this
definition with the restriction
\begin{enumerate}
  \item[3.] \(x\in \dom_n\backslash~ U_{x_{n+1}}, y\in U_{x_{n+1}}\backslash~ \dom_n \) or
  \item[4.] \(y\in \dom_n\backslash~ U_{x_{n+1}}, x\in U_{x_{n+1}}\backslash~ \dom_n \),
\end{enumerate}
respectively.
Because \(t\) satisfies
\(t\in \dom_n\cap U_{x_{n+1}}\), we have in both cases \(t\in \overline{x~y}\).
By using property 4 of lemma \ref{lem properties of dom s bar} we infer 
from \((xA,yA)\in \dom\overline{S}\) that in both cases
 \((xA, t A),(tA, y A)\in \dom \overline{S}\)
also holds. Hence we may apply the induction hypothesis 
\(\eqref{induction psi3}_{n}\).

It remains to show uniqueness. 
So let \(\tilde{\psi}_{n+1}\) be defined 
on \(\dom_{n+1}\times\dom_{n+1}\) fulfil 
 \begin{align}\label{uniqueness psi1}\tag{\(\ref{induction psi1}_{\tilde{\psi}}\)}
  \forall (t,s)\in \dom_{n+1}\times \dom_{n+1}: \tilde{\psi}(t,s) \text{ is a lift of } S_{tA, sA},\\\label{uniqueness psi2}\tag{\(\ref{induction psi2}_{\tilde{\psi}}\)}
\forall s,k,l\in\mathbb{R}: \tilde{\psi}(k,s)\tilde{\psi}(s,l)=\tilde{\psi}(k,l),\\\label{uniqueness psi3}\tag{\(\ref{induction psi3}_{\tilde{\psi}}\)}
\forall (x,y)\in\dom_{n+1}: (xA, yA)\in\dom\overline{S}\Rightarrow  \tilde{\psi}(x,y)=\phi_A(x,y).
\end{align}
Now pick \(x,y \in (\dom_n\cup U_{x_{n+1}})\). We proceed 
in a case by case manner
\begin{enumerate}
\item If \(x,y\in\dom_n\) holds, then 
\(\psi_{n+1}(x,y)=\tilde{\psi}_{n+1}(x,y)\) follows directly
from the induction hypothesis.
\item Similarly if \(x,y\in U_{x_{n+1}}\) holds, we have
\begin{equation}
  \psi_{n+1}(x,y)=\phi_A(x,y)=\tilde{\psi}_{n+1}(x,y).
\end{equation}
\item Additionally, if 
\(x\in \dom_n, y \in U_{x_{n+1}}\) holds, then 
\begin{align}
\psi_{n+1}(x,y)
\overset{\eqref{induction psi2}}{=}
\psi_{n+1}(x,t)\psi_{n+1}(t,y)\\
\overset{t\in \dom_n\cap U_{x_{n+1}}}{=}\tilde{\psi}_{n+1}(x,t)\tilde{\psi}_{n+1}(t,y)
\overset{\eqref{uniqueness psi2}}{=}
\tilde{\psi}_{n+1}(x,y)
\end{align}
is satisfied. 
\item Conversely, if 
\(y\in \dom_n, x \in U_{x_{n+1}}\) holds, we may 
use the same calculation to obtain 
\begin{align}
\psi_{n+1}(x,y)
\overset{\eqref{induction psi2}}{=}
\psi_{n+1}(x,t)\psi_{n+1}(t,y)\\
\overset{t\in \dom_n\cap U_{x_{n+1}}}{=}\tilde{\psi}_{n+1}(x,t)\tilde{\psi}_{n+1}(t,y)
\overset{\eqref{uniqueness psi2}}{=}
\tilde{\psi}_{n+1}(x,y).
\end{align}
\end{enumerate}

Now we have established an extension \(\psi_n\) of \(\phi_A\)
fulfilling properties \eqref{induction psi1}-\eqref{induction psi3}.

We know that for each \(n\in\mathbb{N}\) the function
\(\psi_{n+1}:\dom_{n+1}^2\rightarrow U(\mathcal{F})\) is 
an extention of 
\(\psi_{n+1}:\dom_{n}^2\rightarrow U(\mathcal{F})\).
Furhtermore, the sets \(\dom_n\) cover 
\(\mathbb{R}\) according 
to equation \eqref{open cover of R}. Consequently 
there is a unique common extension, by small abuse of 
notation again called 
\(\phi_A:\mathbb{R}^2\to U(\mathcal{F})\), 
of all \(\psi_n\). This function fulfills the claim 
\eqref{global phi prop1}-\eqref{global phi prop4}, because 
any \(t,l,s\in\mathbb{R}\) are contained in some \(\dom_n\).



\end{proof}

Lemma \ref{lem: phi global} enables us to define a global lift.
\begin{Def}[global lift]\label{de: s hat}
For any \(A\in\mathcal{V}\) we define 
\begin{equation}
\hat{S}_{0,A}:=\phi_A(1,0).
\end{equation}
\end{Def}
Using lemma  \ref{lem: phi global} we are now in a position
to prove theorem \ref{thm: ex s hat}.
\begin{proof}[proof of theorem \ref{thm: ex s hat}]
The operator \(\hat{S}\) fulfils the claimed differential 
equation \eqref{diff s hat} due to the global 
multiplication property 
\eqref{global phi prop2} and the differential 
equation \eqref{global phi prop4}.
Its uniqueness is inherited from the 
uniqueness of \(\phi_A\) for \(A\in\mathcal{V}\)
from lemma \ref{lem: phi global}.
\end{proof}


%\begin{rmk}
%The lift \(\hat{S}_{0,A}\) can also be calculated differently: pick \(N\in\mathbb{N}\) and a series \((\delta_k)_{k\in\mathbb{N}}\subset \mathbb{R}^+\)
% such that \(\|\mathds{1}-S_{\sum_{k=1}^{n-1}\delta_k A,\sum_{k=1}^{n}\delta_k A}\|<1\)  holds true for all \(k\) and \(\sum_{k=1}^{N}\delta_k=1\). 
%Then 

%\textcolor{red}{is there some kind of direct correspondence between \(\|1-S_{0,\delta_n A}\|\) and \(\|1-S_{\sum_{k=1}^{n-1}\delta_k A,\sum_{k=1}^{n}\delta_k A}\|\)??}
%\begin{equation}
%P^-S_{\sum_{k=1}^{n-1}\delta_k A,\sum_{k=1}^{n}\delta_k A}P^-
%\end{equation}
%is invertible. Now, 
%\begin{equation}\label{hat finite spacing}
%\hat{S}_{0,A}=\prod_{n=0}^{N} \overline{S}_{\sum_{k=1}^{n-1}\delta_k A,\sum_{k=1}^{n}\delta_kA}.
%\end{equation}
%Before proving this claim, we notice the following property of \(\overline{S}\): For any  \(A\in\mathcal{V}\) and \(\alpha, \beta, \gamma>0\)
% such that all factors exist the following identity holds
%\begin{align}
%\overline{S}_{\alpha A, \gamma A} = \overline{S}_{\alpha A, \beta A} \overline{S}_{\beta A, \gamma A}.
%\end{align}
%This property is by \eqref{gamma attri6} equivalent to 
%\begin{equation}\label{gamma equal 1}
%\Gamma_{\alpha A, \beta A, \gamma A}=1.
%\end{equation}
%This claim can be reduced to the one where \(\alpha=1\) by the following renaming scheme
%\begin{align}
%\alpha A&=: \tilde{A}\\
%\beta/\alpha &=: \tilde{\beta}\\
%\gamma/\alpha&=:\tilde{\gamma}.
%\end{align}
%In fact, the claim holds, as
%the following calculation shows:
%\begin{align}
%\ln &\Gamma_{A,\beta A, \gamma A} = \int_1^\beta d \beta' \partial_{\beta'}\ln \Gamma_{A,\beta' A, \gamma A} 
%+ \overbrace{\ln \Gamma_{A,A,\gamma A}}^{\overset{\eqref{gamma attri4}}{=}0}\\
%&= \int_1^\beta d\beta' \left(\int_1^\gamma d\gamma' \partial_{\gamma'}\partial_{\beta'} \ln \Gamma_{A,\beta' A, \gamma' A} 
%+ \partial_{\beta'} \overbrace{\ln \Gamma_{A,\beta' A, A}}^{\overset{\eqref{gamma attri4}}{=}0} \right)\\
%&=\int_1^\beta d\beta' \int_1^\gamma d\gamma' c_A((\beta'-1) A, (\gamma'-1) A)\\
%&=\int_1^\beta d\beta' \int_1^\gamma d\gamma'  (\beta'-1) (\gamma'-1) \overbrace{c_A( A,  A) }^{\overset{\eqref{gamma attri3},\eqref{gamma attri8}}{=}0}=0,
%\end{align}
%where we have used various properties of lemma \ref{gamma attri}.
%
%\textcolor{blue}{generalisation:}
%claim: for all \(A,B\in \mathcal{A}\) and all \(\alpha,\beta\in\mathbb{R}\), \(|\alpha|,|\beta|\) small enough, such that the expression appearing is well defined
%\begin{equation}
%\Gamma_{A,A+\alpha B, A+ \beta B} =1.
%\end{equation}
%proof:
%\begin{align}
%\ln \Gamma_{A,A+\alpha B, A+ \beta B} = \overbrace{\ln \Gamma_{A,A+\alpha B, A}}^{=0}+ \int_0^{\beta} d\beta' \partial_{\beta'} \ln \Gamma_{A,A+\alpha B, A+ \beta' B}\\
%=\int_0^{\beta} d\beta' \partial_{\beta'} \left(\overbrace{\ln \Gamma_{A,A, A+ \beta' B}}^{=0}+ \int_0^{\alpha}d\alpha' \partial_{\alpha'} \ln \Gamma_{A,A+\alpha' B, A+ \beta' B}\right)\\
%=\int_0^{\beta} d\beta'  \int_0^{\alpha}d\alpha' \partial_{\beta'} \partial_{\alpha'} \ln \Gamma_{A,A+\alpha' B, A+ \beta' B}\\
%=\int_0^{\beta} d\beta'  \int_0^{\alpha}d\alpha' c_A(\alpha' B, \beta' B) 
%=\int_0^{\beta} d\beta'  \int_0^{\alpha}d\alpha' \alpha' \beta' \overbrace{ c_A( B,  B)}^{=0}.
%\end{align}


%Because of \eqref{gamma equal 1} we see that \eqref{hat finite spacing} is actually independent of the choice of sequence \((\delta_k)_{k}\).
%The right side of equation \eqref{hat finite spacing} satisfies the initial condition of the ordinary differential equation, so we only need to check the differential equation itself.
%Next we reformulate the ordinary differential equation \eqref{diff s hat}, pick \(B=t A\) then we have
%\begin{equation}
%0=\partial_B \frac{\hat{S}_{0,A+B}}{\hat{S}_{0,A}\overline{S}_{A,A+B}}
%=\partial_\varepsilon \frac{\hat{S}_{0,A(1+\varepsilon t)}}{\hat{S}_{0,A}\overline{S}_{A,A(1+\varepsilon t)}}
%=t\partial_\varepsilon \frac{\hat{S}_{0,A(1+\varepsilon)}}{\hat{S}_{0,A}\overline{S}_{A,A(1+\varepsilon)}}
%\end{equation}


%What is more, we can pick \((\delta_k)_k\) such that \(\sum_{k=1}^N \delta_k=1\) and \(\delta_{N+1}=\varepsilon\) then we have

%\begin{align}
%\hat{S}_{0,A(1+\varepsilon)}=\prod_{n=0}^{N+1} \overline{S}_{\sum_{k=1}^{n-1}\delta_k A,\sum_{k=1}^{n}\delta_kA}
%=\prod_{n=0}^{N} \overline{S}_{\sum_{k=1}^{n-1}\delta_k A,\sum_{k=1}^{n}\delta_kA}~ \overline{S}_{A,A(1+\varepsilon)}\\
%=\hat{S}_{0,A}\overline{S}_{A,A+B},
%\end{align}

%or in other words

%\begin{equation}
%\frac{\hat{S}_{0,A+B}}{\hat{S}_{0,A}\overline{S}_{A,A+B}}=1.
%\end{equation}
%If \(A=0\) holds, we see directly that 

%\begin{equation}
%\frac{\hat{S}_{0,B}}{\hat{S}_{0,0}\overline{S}_{0,B}}= 1,
%\end{equation}

%so in both cases the ordinary differential equation is satisfied. 
%\end{rmk}

\begin{Def}[relative phase]\label{def relative phase}
Let \((A,B) \in\dom\overline{S}\), we define \(z(A,B)\in S^1\) by
\begin{equation}\label{def z}
z(A,B):=\frac{\hat{S}_{0,B}}{\hat{S}_{0,A}\overline{S}_{A,B}}.
\end{equation}
Please note that for such \(A,B\) the lift \(\bar{S}_{A,B}\) 
is well defined. This means that the product in the 
denominator is a lift of \(S_{0,B}\)
and according to definition \ref{de: s hat} the 
ratio is well defined. 
\end{Def}

\begin{Remark}
The global function \(z\) defined here is an extension 
of the funtion \(z\) appearing locally 
in the 
the proof of lemma \ref{lem phi local},
cf. formula \eqref{def local z}.

Please note that \(z\) is smooth
when restricted to  \(\mathcal{W}^2\cap \dom\overline{S}\) 
for any finite dimensional subspace
 \(\mathcal{W}\subseteq \mathcal{V}\).
\end{Remark}


\begin{Lemma}[properties of the relative phase]
For all \\
\((A,F),(F,G),(G,A)\in\dom\overline{S}\), as well as 
or all \(H,K\in \mathcal{V}\), we have
\begin{align}\label{z antisym}
z(A,F)&=z(F,A)^{-1}\\\label{z gamma}
z(F,A)z(A,G)z(G,F)&=\Gamma_{F,A,G}\\\label{z c}
\partial_{H}\partial_{K}\ln  z(A+ H,A+ K)&=c_A(H,K).
\end{align}
\end{Lemma}
\begin{proof}
Pick \(A,F,G\in\mathcal{V}\) as in the lemma. We start off by analysing
\begin{align}
&\hat{S}_{0,F}\overline{S}_{F,G}\overset{\eqref{def z}}{=}z(A,F)\hat{S}_{0,A}\overline{S}_{A,F}\overline{S}_{F,G}\\\label{phase comparison}
&\overset{\eqref{gamma attri6}}{=}z(A,F) \Gamma_{A,F,G}^{-1} \hat{S}_{0,A} \overline{S}_{A,G}.
\end{align}
Exchanging  \(A\) and \(F\) in this equation yields
\begin{equation}
\hat{S}_{0,A}\overline{S}_{A,G}=z(F,A) \Gamma_{F,A,G}^{-1} \hat{S}_{0,F} \overline{S}_{F,G}.
\end{equation}
This is equivalent to
\begin{equation}
\hat{S}_{0,F} \overline{S}_{F,G}=z(F,A)^{-1} \Gamma_{F,A,G} \hat{S}_{0,A}\overline{S}_{A,G}~.
\end{equation}
Comparing the last equation with 
formula \eqref{phase comparison} 
and taking the permutation properties 
 \eqref{gamma attri3} of \(\Gamma\) into 
 account this implies that 
\begin{equation}
z(A,F)=z(F,A)^{-1}
\end{equation}
holds true. Equation \eqref{phase comparison} solved for \(\hat{S}_{0,A}\overline{S}_{A,G}\) also gives us

\begin{align}
\hat{S}_{0,G}\overset{\eqref{def z}}{=}z(A,G)\hat{S}_{0,A}\overline{S}_{A,G}\\
\overset{\eqref{phase comparison}}{=}z(A,G)z(A,F)^{-1}\Gamma_{A,F,G}\hat{S}_{0,F}\overline{S}_{F,G}.
\end{align}
The latter equation compared with 
\begin{equation}
\hat{S}_{0,G}\overset{\eqref{def z}}{=}z(F,G)\hat{S}_{0,F}\overline{S}_{F,G},
\end{equation}
yields  a direct connection between \(\Gamma\) and \(z\):
\begin{equation}
\frac{z(A,G)}{z(A,F)}\Gamma_{A,F,G}=z(F,G),
\end{equation}
which we rewrite using the antisymmetry 
\eqref{z antisym} of \(z\)  as
\begin{equation}
\Gamma_{A,F,G}=z(F,G)z(A,F)z(G,A).
\end{equation}
Finally, in this equation, we substitute 
\(F=A+\varepsilon_1 H\) as well as  
\(G=A+\varepsilon_2 K\), where 
\(\varepsilon_1,\varepsilon_2\) is small enough so that 
\(z\) and \(\Gamma\) are still well defined. Then we 
take the second logarithmic derivative to find
\begin{align}\notag
 \partial_{\varepsilon_1}\partial_{\varepsilon_2}\ln  z(A+\varepsilon_1 H,A+\varepsilon_2 K)=\partial_{\varepsilon_1}\partial_{\varepsilon_2}\ln\Gamma_{A,A+\varepsilon_1 H,A+\varepsilon_2 K}\\
 \overset{\eqref{gamma attri8}}{=}c_A(H,K). 
\end{align}

\end{proof}

So we find that \(c_A\) is the second mixed logarithmic 
derivative of \(z\). In the following we will 
characterise \(z\) more thoroughly by \(c\) and \(c^+\).
\begin{Def}[\(p\)-forms of four potentials, phase integral]
For \(p\in\mathbb{N}\), we introduce the set \(\Omega^p\) 
of \(p\)-forms 
to consist of all maps 
\(\omega: \mathcal{V}\times \mathcal{V}^{p}\rightarrow \mathbb{C}\)
such that \(\omega\) is linear and antisymmetric in its 
\(p\) last arguments and smooth in its first argument
when restricted to any finite
dimensional subspace of \(\mathcal{V}\).

Additionally, we define the \(1\)-form 
\(\chi\in \Omega^1(\mathcal{V})\) by
\begin{equation}\label{de chi}
\chi_A(B):=\partial_B\ln z(A,A+B)
\end{equation}
for all \(A,B\in\mathcal{V}\).
Furthermore, for \(p\in\mathbb{N}\) and any differential 
form \(\omega\in \Omega^p(\mathcal{V})\), 
we define its exterior derivative, 
\(d \omega\in\Omega^{p+1}(\mathcal{V})\) by
\begin{equation}
(d\omega)_A(B_1,\dots, B_{p+1}):=\sum_{k=1}^{p+1} (-1)^{k+1} 
\partial_{B_k}\omega_{A+B_k}(B_1,\dots , \widehat{B_k},\dots, B_{p+1}),
\end{equation}
for \(A,B_1,\dots, B_{p+1}\in\mathcal{V}\), where 
the notation \(\widehat{B_k}\) denotes that \(B_k\) is  
dropped as an argument.

\end{Def}

\begin{Lemma}[connection between \(c\) and the relative phase]\label{connection between c and the relative phase}
The differential form \(\chi\) fulfils 
\begin{equation}
(d\chi)_A(F,G)=2 c_A(F,G)
\end{equation}
for all \(A,F,G\in\mathcal{V}\).
\end{Lemma}
\begin{proof}
Pick \(A,F,G\in \mathcal{V}\), we calculate
\begin{align}\nonumber
&(d\chi)_A(F,G)=\partial_F\partial_G \ln z(A+F,A+F+G)\\
&\hspace{3cm}-\partial_F \partial_G \ln z(A+G,A+F+G)\\
&=\partial_F\partial_G (\ln  z(A,A+F+G)+\ln z(A+F,A+G))\\
 &\hspace{1cm} - \partial_F\partial_G(  \ln z(A,A+F+G)+\ln z(A+G,A+F))\\
&\overset{\eqref{z antisym}}{=} 2 \partial_F\partial_G \ln z(A+F,A+G)\overset{\eqref{z c}}{=}2 c_A(F,G).
\end{align}
\end{proof}

Now since \(d c=0\), we might use Poincaré's lemma as a method independent of \(z\) to construct a differential form \(\omega\) such that \(d\omega=c\). 
In order to execute this plan, we first need to prove Poincaré's lemma for our setting:

\begin{Lemma}[Poincaré]\label{lem poincare}
Let \(\omega\in \Omega^p(\mathcal{V})\) for 
\(p\in\mathbb{N}\) be closed, i.e. \(d \omega =0\). 
Then \(\omega\) is also exact, more precisely we have
\begin{equation}
\omega=d \int_{0}^1 \iota^*_t i_X f^* \omega dt,
\end{equation}
where \(X\), \(\iota_t\) for \(t\in\mathbb{R}\) and \(f\) are given by
 \begin{align}
 &X: \mathbb{R}\times\mathcal{V}\rightarrow \mathbb{R}\times\mathcal{V},\\
 &\hspace{2cm} (t,B)\mapsto (1,0) \\
&\forall t \in \mathbb{R}: \iota_t: \mathcal{V}\rightarrow \mathbb{R}\times\mathcal{V},\\
&\hspace{2cm} B\mapsto (t,B)\\
&f:\mathbb{R}\times \mathcal{V}\mapsto \mathcal{V},\\
&\hspace{2cm} (t,B) \mapsto t B\\
&i_X: \Omega^p(\mathcal{V})\rightarrow \Omega^{p-1}(\mathcal{V}),\\
&\hspace{2cm} \omega \mapsto ((A;Y_1,\dots, Y_{p-1})\mapsto \omega_A(X,Y_1,\dots,Y_{p-1}))
 \end{align}
\end{Lemma}
\begin{proof}
  \todo{put proof into appendix}
Pick some \(\omega \in \Omega^p(\mathcal{V})\).
We will first show the more general formula 
\begin{equation}\label{poincare more general}
f^*_b\omega-f^*_a \omega=d \int_{a}^b \iota_t^* i_X f^* \omega ~dt+ \int_{a}^b \iota_t^* i_X f^* d \omega dt,
\end{equation}
where \(f_t\) is defined as
\begin{equation}
\forall t \in \mathbb{R}: f_t:=f(t,\cdot).
\end{equation}
The lemma follows then by \(b=1, a= 0\), \(f^*_1\omega=\omega, f^*_0 \omega=0\) and \(d \omega=0\) for a closed \(\omega\). 
We begin by rewriting the right hand side of \eqref{poincare more general}:
\begin{align}\nonumber
d \int_{a}^b \iota_t^* i_X f^* \omega ~dt+ \int_{a}^b \iota_t^* i_X f^* d \omega dt\\\label{poincare 1 manipulation}
=\int_a^b (d\iota_t^* i_X f^* \omega+ \iota_t^* i_X f^* d \omega )dt.
\end{align}
Next we look at both of these terms separately. Let therefore \(p\in \mathbb{N}\), \(t, s_k\in \mathbb{R}\) and \(A,B_k\in \mathcal{V}\) for each \(p+1\ge k\in\mathbb{N}\).
First, we calculate \(d \iota^*_t i_X f^* \omega\):
\begin{align}
&(f^*\omega)_{(t,A)}((s_1,B_1),\dots, (s_p,B_p))\\\nonumber
&\hspace{3cm}=\omega_{tA}(s_1A+tB_1,\dots, s_p A+t B_p)\\[0.3cm]
&\Rightarrow (i_X f^* \omega)_{(t,A)}((s_1,B_1),\dots, (s_{p-1},B_{p-1}))\\\nonumber
&\hspace{3cm}=\omega_{tA}(A,s_1A+tB_1,\dots, s_{p-1}A + t B_{p-1})\\[0.3cm]
&\Rightarrow (\iota^*_t i_X f^* \omega)_{A}(B_1,\dots, B_{p-1})= t^{p-1} \omega_{t A}(A,B_1,\dots, B_{p-1})\\[0.3cm]\notag
&\Rightarrow (d\iota^*_t i_X f^* \omega)_{A}(B_1,\dots, B_{p}) \\
&\quad=\partial_\varepsilon |_{\varepsilon=0} \sum_{k=1}^p (-1)^{k+1} t^{p-1} \omega_{t A + \varepsilon t B_k}(A,B_1,\dots, \widehat{B_k},\dots, B_p)\\
&\quad+ \partial_\varepsilon|_{\varepsilon=0} \sum_{k=1}^p (-1)^{k+1} t^{p-1} \omega_{tA}(A+\varepsilon B_k,B_1,\dots, \widehat{B_k},\dots, B_p)\\\notag
&=\partial_{\varepsilon}|_{\varepsilon=0}\sum_{k=1}^p t^p (-1)^{k+1} \omega_{tA+\varepsilon B_k}(A,B_1,\dots, \widehat{B_k},\dots, B_p)\\\label{li derivative 1}
&\hspace{6cm}+p t^{p-1}\omega_{tA}(B_1,\dots, B_p).
\end{align}


Now, we calculate \(\iota^*_t i_X f^* d \omega\):
\begin{align}\notag
&(d\omega)_A(B_1,\cdots, B_{p+1})\\
&\hspace{2cm}=\partial_\varepsilon |_{\varepsilon=0} \sum_{k=1}^{p+1} (-1)^{k+1} \omega_{A+\varepsilon B_k}(B_1,\dots , \widehat{B_k}, \dots, B_{p+1})\\[0.3cm]
&(f^* d \omega){(t,A)}((s_1,B_1),\dots , (s_{p+1},B_{p+1}))\\\notag
&= (d\omega)_{tA}(s_1A + t B_1, \dots, s_{p+1}A+t B_{p+1})\\
&=\partial_{\varepsilon}|_{\varepsilon =0} \sum_{k=1}^{p+1}(-1)^{k+1}\\\notag
&\quad \times\omega_{tA + \varepsilon(s_kA + t B_k)}(s_1A+tB_1, \dots,  \widehat{s_k A + t B_k},\dots , s_p A  + t B_p )\\[0.4cm]
&(i_X f^* d \omega)_{(t,A)}((s_1,B_1),\dots , (s_p, B_p))\\\notag
&\quad= \partial_{\varepsilon} |_{\varepsilon =0} \omega_{(t+\varepsilon)A}(s_1A+tB_1, \dots, s_pA+t B_p)\\\notag
&+\partial_{\varepsilon}|_{\varepsilon=0} \sum_{k=1}^p (-1)^k\\\notag
&\times\omega_{tA + \varepsilon(s_k A + t B_k)} (A,s_1A+t B_1, \dots, \widehat{s_k A + t B_k},\dots, s_p A + t B_p)\\
&= t^p \partial_{\varepsilon}|_{\varepsilon=0}\omega_{(t+\varepsilon)A }(B_1,\dots, ,\widehat{B_k},\dots, B_p))\\
&\hspace{2cm}+ \sum_{k=1}^p s_k t^{p-1} (-1)^{k+1} \partial_{\varepsilon}|_{\varepsilon=0}\omega_{(t+\varepsilon)A}(A,B_1,\dots, B_p) \\
&+\partial_{\varepsilon}|_{\varepsilon=0}\sum_{k=1}^p(-1)^k t^{p-1}(\omega_{(t+s_k\varepsilon)A}(A,B_1,\dots, \widehat{B_k},\dots, B_p) \\
&\hspace{2cm} + \omega_{tA + \varepsilon t B_k}(A,B_1, \dots, \widehat{B_k},\dots, B_p))\\
&=t^p\partial_{\varepsilon}|_{\varepsilon=0} \Big(\omega_{(t+\varepsilon)A}(B_1,\dots, B_p)\\\notag
&\hspace{2cm} +\sum_{k=1}^p (-1)^k \omega_{tA+\varepsilon B_k}(A,B_1,\dots, \widehat{B_k},\dots, B_p)\Big)\\\label{li derivative 2}
&(\iota_t^* i_X f^* d \omega)_{A}(B_1,\dots ,  B_p)=t^p\partial_{\varepsilon}|_{\varepsilon=0} \Big(\omega_{(t+\varepsilon)A}(B_1,\dots, B_p)\\\notag
&\hspace{2cm}+\sum_{k=1}^p (-1)^k \omega_{tA+\varepsilon B_k}(A,B_1,\dots, \widehat{B_k},\dots, B_p)\Big)
\end{align}

Adding \eqref{li derivative 1} and \eqref{li derivative 2} we find for \eqref{poincare 1 manipulation}:

\begin{align}
\int_a^b (d\iota_t^* i_X f^* \omega+ \iota_t^* i_X f^* d \omega )dt=\\
\int_a^b  \Big( t^p\partial_{\varepsilon}|_{\varepsilon=0} \omega_{(t+\varepsilon)A}(B_1,\dots, B_p)
+p t^{p-1}\omega_{tA}(B_1,\dots, B_p)\Big) dt\\
=\int_a^b  \frac{d}{dt} (t^p \omega_{tA}(B_1,\dots, B_p))dt =\int_a^b  \frac{d}{dt} (f^*_t \omega)_{A}(B_1,\dots, B_p)dt\\
=(f^*_b\omega)_A(B_1,\dots,B_p)-(f^*_a\omega)_A(B_1,\dots,B_p).
\end{align}

\end{proof}

\begin{Def}[antiderivative of a closed \(p\) form]\label{antiderivative}
For a closed exterior form \(\omega\in\Omega^{p}(\mathcal{V})\) we define the form \(\Pi [\omega]\)
\begin{equation}
\Pi\![\omega]:=\int_{0}^1 \iota^*_t i_X f^* \omega dt.
\end{equation}
For \(A,B_1,\dots , B_{p-1}\in\mathcal{V}\) it takes the form 
\begin{equation}
\Pi\![\omega]_A(B_1,\dots, B_{p-1})=\int_0^1 t^{p-1} \omega_{tA}(A,B_1,\dots, B_{p-1})dt.
\end{equation}
By lemma \ref{lem poincare} we know \(d\Pi [\omega]=\omega\) if \(d\omega=0\).
\end{Def}

Now we found two one forms each produces \(c\) when the exterior derivative is taken. The next lemma informs us about their relationship.

\begin{Lemma}[inversion of lemma \ref{connection between c and the relative phase}]
The following equality holds
\begin{equation}
\chi=2 \Pi\![c].
\end{equation}
\end{Lemma}
\begin{proof}
By definition \ref{antiderivative} of \(\Pi\) and 
lemma \ref{connection between c and the relative phase}
we have \(d(\chi-2 \Pi\![c])=0\). Hence, 
by the Poincaré lemma 
\ref{lem poincare}, we know that there is 
\(v:\mathcal{V}\rightarrow \mathbb{R}\) such that
\begin{equation}
dv=\chi-2 \Pi\![c]
\end{equation}%{def relative phase}
holds. Using the definition \ref{def relative phase} of \(z\), 
 the parallel transport equation \eqref{diff s hat} 
 translates into the 
following ODE for \(z\):
\begin{equation}
\partial_B \ln z(0,B)=0, \quad \partial_\varepsilon \ln z(A,(1+\varepsilon)A)|_{\varepsilon =0}=0
\end{equation}
for all \(A,B\in\mathcal{V}\). Therefore have
\begin{equation}
\chi_0(B)=0=\Pi\![c]_0(B), \quad \chi_{A}(A)=0=\Pi\![c]_A(A),
\end{equation}
which implies
\begin{equation}
\partial_\varepsilon v_{\varepsilon A}|_{\varepsilon=0}=0, 
\quad \partial_\varepsilon v_{A+\varepsilon A}|_{\varepsilon=0}=0.
\end{equation}
In conclusion, \(v\) is constant.
\end{proof}


From this point on we will assume the existence of  a function \(c^+\) fulfilling \eqref{c+ 1},\eqref{c+ 2} and \eqref{c+ 3}.
Recall property \eqref{c+ 2}: 
\begin{equation}
\forall A,F,G,H: \partial_H c_{A+H}^+(F,G)=\partial_G c^+_{A+G}(F,H).
\end{equation}

For a fixed \(F\in\mathcal{V}\), this condition can be read as \(d( c^+_{\cdot} (F,\cdot))=0\). As a consequence we can apply lemma \ref{lem poincare} to define a one form.

\begin{Def}[integral of the causal splitting]
For any \(A,F\in\mathcal{V}\), we define

\begin{equation}
\beta_A(F):=2 \Pi\![c^+_{\cdot}(F,\cdot)]_A.
\end{equation}
\end{Def}

\begin{Lemma}[relation between the integral of the causal splitting and the phase integral]
The following two equations hold:
\begin{align}\label{beta c}
&d \beta=-2 c,\\
&d(\beta +\chi)=0.
\end{align}
\end{Lemma}
\begin{proof}
We start with the exterior derivative of \(\beta\). Pick \(A,F,G\in\mathcal{V}\):
\begin{align}
d\beta_A(F,G)=\partial_F \beta_{A+F}(G)-\partial_G \beta_{A+G}(F)\\
=d\Big(\Pi\![c^+_\cdot(G,\cdot)]\Big)_A(F)-d\Big(\Pi\![c^+_\cdot(F,\cdot)]\Big)_A(G)\\
=2 c^+_A(G,F)-2 c_A^+(F,G)\overset{\eqref{c+ 1}}{=}-2 c_A(F,G).
\end{align}
This proves the first equality. The second equality follows directly by \(d \chi=2 c\).
\end{proof}

\begin{Def}[corrected lift]
Since \(\beta+\chi\) is closed, we may use lemma \ref{lem poincare} again to define the phase
\begin{equation}\label{def alpha}
\alpha:=\Pi\![\beta+\chi].
\end{equation}
Furthermore, for all \(A,B\in\mathcal{V}\) we define the corrected second quantised scattering operator 
\begin{align}\label{eq def tilde S}
&\tilde{S}_{0,A}:=e^{-\alpha_A} \hat{S}_{0,A},\\
&\tilde{S}_{A,B}:=\tilde{S}^{-1}_{0,A}\tilde{S}_{0,B}.
\end{align}
\end{Def}

Using this definition one immediately gets:
\begin{Corollary}[group structure of the corrected lift]
We have\\
 \(\tilde{S}_{A,B} \tilde{S}_{B,C}=\tilde{S}_{A,C}\) for all \(A,B,C\in\mathcal{V}\).
\end{Corollary}



\begin{Thm}[causality of the corrected lift]
The corrected second quantised scattering operator fulfils the following causality condition for all \(A,F,G\in \mathcal{V}\) such that \(F\prec G\):
\begin{equation}
\tilde{S}_{A,A+F}=\tilde{S}_{A+G,A+G+F}.
\end{equation}
\end{Thm}
\begin{proof}
Let \(A,F,G\in\mathcal{V}\) such that \(F\prec G\). For the first quantised scattering operator we have
\begin{equation}
S_{A+G,A+G+F}=S_{A,A+F},
\end{equation}
so that by definition of \(\overline{S}\) we obtain
\begin{equation}\label{s bar causal}
\overline{S}_{A+G,A+G+F}=\overline{S}_{A,A+F}.
\end{equation}
So for any lift this equality is true up to a phase, meaning that 

\begin{equation}\label{f causal}
f(A,F,G):=\frac{\tilde{S}_{A+G,A+G+F}}{\tilde{S}_{A,A+F}}
\end{equation}
is well defined. We see immediately
\begin{equation}\label{vanish at axis}
f(A,0,G)=1=f(A,F,0).
\end{equation}

Pick \(F_1,F_2\prec G_1,G_2\). We abbreviate \(F=F_1+F_2, G=G_1+G_2\) and we calculate
\begin{align}
&f(A,F,G)=\frac{\tilde{S}_{A+G,A+F+G}}{\tilde{S}_{A,A+F}}\\
&=\frac{\tilde{S}_{A+G,A+F+G}}{\tilde{S}_{A+G_1,A+G_1+F}}\frac{\tilde{S}_{A+G_1,A+G_1+F}}{\tilde{S}_{A,A+F}}\\
&=\frac{\tilde{S}_{A+G,A+G+F_1} \tilde{S}_{A+G+F_1,A+F+G}}{\tilde{S}_{ A+G_1,A+F_1+G_1} \tilde{S}_{ A+G_1+F_1,A+G_1+F}}  \frac{\tilde{S}_{A+G_1,A+G_1+F}}{\tilde{S}_{A,A+F}}\\
&=\frac{\tilde{S}_{A+G,A+G+F_1}}{\tilde{S}_{A+G_1,A+F_1+G_1}} \frac{\tilde{S}_{A+G+F_1,A+F+G}}{\tilde{S}_{A+G_1+F_1,A+G_1+F}}  f(A,G_1,F_1+F_2)\\
&=f(A+G_1,F_1,G_2)f(A+G_1+F_1,G_2,F_2)
 f(A,G_1,F_1+F_2).
\end{align}
Taking the logarithmic derivative we find:
\begin{equation}\label{shift to small G,F}
\partial_{F_2}\partial_{G_2}\ln f(A,F_1+F_2,G_1+G_2)=\partial_{F_2}\partial_{G_2}\ln f(A+F_1+G_1,F_2,G_2).
\end{equation}
Next we pick \(F_2=\alpha_1 F_1\) and \(G_2=\alpha_2 G_1\) for \(\alpha_1,\alpha_2\in\mathbb{R}^+\) small enough so that

 \begin{align}
 \|1-S_{A+F_1+F_2+G_1+G_2,A+F_1+G_1}\|<1\\
 \|1-S_{A+F_1+F_2+G_1+G_2,A+F_1+G_1+G_2}\|<1\\
 \|1-S_{A+F_1+F_2+G_1+G_2,A+F_1+F_2+G_1}\|<1
 \end{align} 
 hold. We abbreviate \(A'=A+G_1+F_1\), use \eqref{def z} and compute
 
 \begin{align}\notag
 &f(A',F_2,G_2)\\\notag
 &\overset{\eqref{eq def tilde S}}{=}
 \exp(-\alpha_{A'+F_2+G_2}+\alpha_{A'+G_2}+\alpha_{A'+F_2}-\alpha_{A'})\\
 &\hspace{2cm}\times\frac{\hat{S}_{0,A'+G_2}^{-1}\hat{S}_{0,A'+G_2+F_2}}{\hat{S}_{0,A'}^{-1}\hat{S}_{0,A'+F_2}}
 \\\notag
 &\overset{\eqref{def z}}{=}
 \exp(-\alpha_{A'+F_2+G_2}+\alpha_{A'+G_2}+\alpha_{A'+F_2}-\alpha_{A'})\\
 &\hspace{2cm}\times \frac{z(A'+G_2,A'+G_2+F_2) }{z(A',A'+F_2)} \frac{\overline{S}_{A'+G_2,A'+G_2+F_2}}{\overline{S}_{A',A'+F_2}}\\\notag
 &\overset{F_2\prec G_2 }{=}\exp(-\alpha_{A'+F_2+G_2}+\alpha_{A'+G_2}+\alpha_{A'+F_2}-\alpha_{A'})\\
 &\hspace{2cm}\times \frac{z(A'+G_2,A'+G_2+F_2) }{z(A',A'+F_2)}
 \end{align}

Most of the factors do not depend on \(F_2\) and \(G_2\), so taking the mixed logarithmic derivative things simplify:
\begin{align}\notag
&\partial_{G_2}\partial_{F_2} \ln f(A',F_2,G_2)= \\
&\partial_{G_2}\partial_{F_2} ( -\alpha_{A'+F_2+G_2} + \ln z(A'+G_2,A'+G_2+F_2))\\
&\overset{\eqref{def alpha}, \eqref{de chi}}{=}  \partial_{G_2} (-\beta_{A'+G_2}(F_2)-\chi_{A'+G_2}(F_2) + \chi_{A'+G_2}(F_2))\\
&\overset{\eqref{beta c}}{=}2 c^+_{A'}(F_2,G_2)\overset{F_2\prec G_2, \eqref{c+ 3}}{=}0.
\end{align}
So by \eqref{shift to small G,F} we also have
\begin{align}
\partial_{F_2}\partial_{G_2}\ln f(A,F_1+F_2,G_1+G_2)=0\\
=\partial_{\alpha_1}\partial_{\alpha_2}\ln f(A,F_1(1+\alpha_1),G_1(1+\alpha_2)).
\end{align}
Using this then we can integrate and obtain
\begin{align}
&0=\int_{-1}^0d \alpha_1 \int_{-1}^0d \alpha_2  \partial_{\alpha_1}\partial_{\alpha_2}\ln f(A,F_1(1+\alpha_1),G_1(1+\alpha_2))\\
&=\ln f(A,F_1,G_1)-\ln f(A,0,G_1)-\ln f(A,F_1,0)\\\notag 
&\hspace{3cm}+ \ln f(A,0,0)\\
&\overset{\eqref{vanish at axis}}{=}\ln f(A,F_1,G_1).
\end{align}
remembering equation \eqref{f causal}, the definition of \(f\),  this ends our proof.
\end{proof}

Using \(\tilde{S}\) we introduce the current associated to it.

%\begin{Def}
%Let \(A,F\in\mathcal{V}\), define
%\begin{equation}
%j_A(F):=i\partial_F \left\langle\Omega, \tilde{S}_{A,A+F}\Omega\right\rangle = i \partial_F \ln \left\langle\Omega, \tilde{S}_{A,A+F}\Omega\right\rangle.
%\end{equation}
%\end{Def}

\begin{Thm}[evaluation of the current of the corrected lift]
For general \(A,F\in\mathcal{V}\) we have
\begin{equation}
j_A(F)=-i\beta_A(F).
\end{equation}
So in particular for \(G\in\mathcal{V}\)
\begin{equation}
\partial_G j_{A+G}(F)=-2i c^+_A(F,G).
\end{equation}
holds.
\end{Thm}
\begin{proof}
Pick \(A,F\in\mathcal{V}\) as in the theorem. We calculate
\begin{align}
i\partial_F \ln \left\langle\Omega, \tilde{S}_{A,A+F}\Omega\right\rangle\\
\overset{\eqref{eq def tilde S}}{=}i\partial_F\left( -\alpha_{A+F}-\alpha_A + \ln \left\langle\Omega, \hat{S}_{0,A}^{-1} \hat{S}_{0,A+F}\Omega\right\rangle\right)\\
\overset{\eqref{def z}}{=}i\partial_F\left( -\alpha_{A+F}+\ln z(A,A+F) + \ln \left\langle\Omega, \overline{S}_{A,A+F}\Omega\right\rangle\right)
\end{align}
The last summand vanishes, as can be seen by the following calculation
\begin{align}
\partial_{F} \ln \left\langle \Omega, \overline{S}_{A,A+F}\Omega\right\rangle\\
=i\partial_F\ln \det_{\mathcal{H}^-} (P^-S_{A,A+F}P^-\AG(P^-S_{A,A+F}P^-)^{-1})\\
\overset{\eqref{def AG}}{=}i\partial_F\ln \det_{\mathcal{H}^-} |P^-S_{A,A+F}P^-|\\
=\frac{i}{2} \partial_F \ln \det_{\mathcal{H}^-} ((P^- S_{A,A+F}P^-)^*P^- S_{A,A+F}P^-)\\
=\frac{i}{2} \partial_F \det_{\mathcal{H}^-} (P^- S_{A+F,A}P^-S_{A,A+F}P^-)\\
=\frac{i}{2}\tr(\partial_F P^-S_{A+F,A}P^-S_{A,A+F}P^-)\\
=\frac{i}{2}\tr(\partial_F P^-S_{A,A+F}P^-+\partial_F P^-S_{A+F,A}P^-)=0\end{align}
where we made use of \eqref{diff det}.
Where after the substitution \(P^-=1-P^+\) we may use 
theorem \ref{thm smoothness of S} 
to see that the derivatives are well-defined.
So we are left with
\begin{align}
j_A(F)=i\partial_F (-\alpha_{A+F}+\ln z(A,A+F))\\
=i(-\beta_A(F)-\chi_A(F)+\chi_A(F))=-i\beta_A(F).
\end{align}

Finally by taking the derivative with respect to \(G\in\mathcal{V}\) and using the definition of \(\beta\) we find
\begin{equation}
\partial_G j_{A+G}(F)=-2i c_A^+(F,G).
\end{equation}


\end{proof}



\section{Simple Formula for the Scattering Operator}

\subsection{Defining One-Particle Scattering-Matrix}\label{sec:one-particle}


%The goal of this and the next section is to introduce some results, which are
%known to the community and form the basis of my work. 
In order to introduce the one-particle dynamics I introduce Diracs 
equation \eqref{dirac} and reformulate it in integral form in 
equation \eqref{dirac_integral}. By iterating this equation we 
will naturally be led to the informal series expansion of the 
scattering operator equation \eqref{DefU}, whose convergence is 
discussed in the next section. 
%\Dirk{ordne es anders: wir wollen $S$, dazu Zeitentwicklung $U$, dazu Dirac
%Gleichung, Integralversion, formale Iteration, Konvergenz}
%\Markus{So? Oder wolltest du, dass ich auch die Mathematik umordne?} Gut so.

Throughout this thesis I will consider four-potentials $A, F$ or \(G\) to be smooth functions
in \(C_{c}^\infty(\mathbb{R}^4)\otimes \mathbb{C}^4\), where the index \(c\)
denotes that the elements have compact support. The Dirac
equation for a wave function \(\phi \in L^2(\mathbb{R}^3)\otimes \mathbb{C}^4\)
is
\begin{equation}\label{dirac}
0= (i\slashed{\partial}-e\slashed{A}-m \id) \phi,
\end{equation}
where \(m\) is the mass of the electron, \(\id: \mathbb{C}^4\rightarrow \mathbb{C}^4\) is the identity  and crossed out letters mean that their four-index is contracted with Dirac matrices
\begin{equation}
\slashed{A}:= A_\alpha \gamma^\alpha,
\end{equation}
where Einstein's summation convention is used. These matrices fulfil the anti-commutation relation
\begin{equation}
\forall \alpha, \beta \in \{0,1,2,3\}:\{\gamma^\alpha, \gamma^\beta\}:= \gamma^\alpha \gamma^\beta+ \gamma^\beta \gamma^\alpha= g^{\alpha \beta},
\end{equation}
where \(g\) is the Minkowski metric. I work with the \(+---\) metric signature and the Dirac representation of this algebra. Squared four dimensional objects always refer to the Minkowski square, meaning for all \(a\in \mathbb{C}^4\), \(a^2:= a^{\alpha} a_{\alpha}\). 

In order to define Lorentz invariant measures for four dimensional integrals I employ the same notation as in \cite{ivp1}. The standard volume form over \(\mathbb{R}^4\) is denoted by \(\mathrm{d}^4 x= \mathrm{d}x^0 \mathrm{d}x^1\mathrm{d}x^2 \mathrm{d}x^3\), the product of forms is understood as the wedge product. The symbol \(\mathrm{d}^3x\)  means the 3-form \(\mathrm{d}^3x= \mathrm{d}x^1\mathrm{d}x^2\mathrm{d}x^3\) on \(\mathbb{R}^4\). Contraction of a form \(\omega\) with a vector \(v\) is denoted by \(\mathfrak{i}_v(\omega)\). The notation \(\mathfrak{i}_v (\omega)\) is also used for the spinor matrix valued vector \(\gamma=(\gamma^0,\gamma^1,\gamma^2,\gamma^3)=\gamma^\alpha e_\alpha\):
\begin{equation}
\mathfrak{i}_\gamma (\mathrm{d}^4x) := \gamma^\alpha \mathfrak{i}_{e_\alpha}(\mathrm{d}^4 x),
\end{equation} 

with \((e_\alpha)_{\alpha}\) being the canonical basis of \(\mathbb{C}^4\). Let \(\mathcal{C}_A\) be the space of solutions to \eqref{dirac} which have compact support on any spacelike hyperplane \(\Sigma\). Let \(\phi, \psi\) be in \(\mathcal{C_A}\), the scalar product \(\langle \cdot, \cdot\rangle\) of elements of \(\mathcal{C}_A\) is defined as


%\begin{equation}
%\langle \phi, \psi \rangle := \int_{\Sigma} \overline{\phi (x)} \mathfrak{i}_{\gamma} (\mathrm{d}^4x) \psi (x)=: \int_{\Sigma} \phi^\dagger (x) \gamma^0 \mathfrak{i}_{\gamma} (\mathrm{d}^4x) \psi (x) .
%\end{equation}
%Furthermore define \(\mathcal{H}\) to be \(\mathcal{H}:=\overline{\mathcal{C}_A}^{\langle \cdot, \cdot \rangle}\). The mas-shell \(\mathcal{M}\subset \mathbb{R}^4\) is given by
%\begin{equation}
%\mathcal{M}=\{p\in \mathbb{R}^4\mid p^2=m^2\}.
%\end{equation}
%The subset \(\mathcal{M}^+\) of \(\mathcal{M}\) is defined to be \(\mathcal{M}^+:=\{p\in\mathcal{M}\mid p^0>0\}\). The image  of \(\mathcal{H}\) by the projector \(1_{\mathcal{M^+}}\), given in momentum space representation, is denoted by \(\mathcal{H}^+\) and its orthogonal complement by \(\mathcal{H}^-\). 
%I introduce a family of Cauchy hypersurfaces \((\Sigma_t)_{t\in\mathbb{R}}\) governed by a family of normal vector fields \((\left.v_t n\right|_{\Sigma_t})\), where \(n: \mathbb{R}^4 \times \mathbb{R} \rightarrow \mathbb{R}^4\) and \(v: \mathbb{R}^4 \times \mathbb{R} \rightarrow \mathbb{R}\) are smooth functions. For \(x\in\Sigma_t\) the vector \(n_t(x)\) denotes the future directed unit-normal vector to \(\Sigma_t\) at \(x\) and \(v_t(x)\) the corresponding normal velocity of the flow of the Cauchy surfaces. 

%Now we have the tools to recast the Dirac equation into an integral version
%which will allow me to define the scattering operator. 
%Let \(\psi \in \mathcal{C}_A\), for any \(t\in\mathbb{R}\) I denote by \(\phi_t\) the solution to the free Dirac equation, that is equation \eqref{dirac} with \(A=0\), with \(\left.\psi\right|_{\Sigma_t}\) as initial condition on \(\Sigma_t\). Let \(t_0\in\mathbb{R}\) have some fixed value, equation \eqref{dirac} can be reformulated, c.f. theorem 2.23 of \cite{ivp1}, as
%\begin{multline}\label{dirac_integral}
%\phi_t(y)=\phi_{t_0}(y)
%-i \int_{t_0}^t \text{d}s \int_{\Sigma_s}\int_{\mathcal{M}}\frac{\slashed{p}+m}{2m^2}e^{ip(x-y)}\mathfrak{i}_p(\text{d}^4p) \frac{\mathfrak{i}_{\gamma}(\text{d}^4x)}{(2\pi)^{3}}\\
%v_s(x)\slashed{n}_s(x) \slashed{A}(x)\phi_s(x),
%\end{multline}
%which holds for any \(t\in\mathbb{R}\). Employing the following rewriting of integrals

%\begin{equation}
%\int_{\mathcal{M}}\frac{\slashed{p}+m}{2 m^2} f(p) \mathfrak{i}_p(\text{d}^4p)=\frac{1}{2\pi i}   \left( \int_{\mathbb{R}^4-i \epsilon e_0}-\int_{\mathbb{R}^4+i \epsilon e_0} \right) (\slashed{p}-m)^{-1} f(p)  \text{d}^4p,
%\end{equation}
%which is due to the theorem of residues, equation \eqref{dirac_integral} assumes the form

%\begin{multline}
%\phi_t(y)=\phi_{t_0}(y)
%- \int_{[t_0,t]\times\mathbb{R}^3}  \left( \int_{\mathbb{R}^4-i \epsilon e_0}-\int_{\mathbb{R}^4+i \epsilon e_0} \right)\\
% (\slashed{p}-m)^{-1} e^{ip(x-y)}  \text{d}^4p  \frac{\text{d}^4x}{(2\pi)^{4}}\slashed{A}(x)\phi_s(x). 
%\end{multline} 
%In the last expression I picked all hypersurfaces \(\Sigma_s\) to be equal time
%hyperplanes such that \(v_s=1\) and \(\slashed{n}_s=\gamma^0 e_0\). We identify
%the advanced and retarded Greens functions of the Dirac equation:
%\begin{equation}
%\Delta^\pm (x):= \frac{-1}{(2\pi)^4} \int_{\mathbb{R}^4\pm i \varepsilon e_0} \frac{\slashed{p}+ m}{p^2-m^2} e^{-ipx} d^4 p,
%\end{equation}
%yielding
%\begin{multline}\label{dirac integral split}
%\phi_t(y)=\phi_{t_0}(y)
%+ \int_{[t_0,t]\times\mathbb{R}^3}  (\Delta^--\Delta^+)(y-x)  \text{d}^4x \slashed{A}(x)\phi_s(x). 
%\end{multline} 

% Iteratingequation \eqref{dirac integral split} and picking \(t\) in the future of
%\(\supp A\) and \(t_0\) in the past of it, denoting them by \(\pm \infty\) since 
%their exact value is no longer important, the following series expansion is 
%obtained informally

%\begin{equation}\label{DefU}
%\phi_\infty(y)=U^A \phi_{-\infty} :=\sum_{k=0}^\infty Z_k(A) \phi_{-\infty},
%\end{equation} 
%with \(Z_0=\id \), the identity on \(\mathbb{C}^4\), and where for arbitrary \(\phi\in \mathcal{H}\), \(Z_k \) is defined as
%\begin{align*}
%&Z_k(A)\phi(y):= \int_{\mathbb{R}^4}  (\Delta^--\Delta^+)(y-x_1)  \text{d}^4x_1 \slashed{A}(x_1) \\
%&\prod_{l=2}^k \left[\int_{[-\infty,x^0_{l-1}]\times\mathbb{R}^3}  (\Delta^--\Delta^+)(x_{l-1}-x_l)   \slashed{A}(x_l)\text{d}^4x_l\right]
% \phi(x_k).
%\end{align*}
%Now since the integration variables are time ordered and \(\supp \Delta^\pm \subseteq \text{Cau}^\pm\) 
%in every one but the first factor the contribution of \(\Delta^-\) vanishes. Therefore we can simply \noch{führe Cau als kausale Menge ein}
%drop it. Furthermore we may continue the integration domain to all of \(\mathbb{R}^4\), since
%there \(\Delta^+\) gives no contribution, giving
%\begin{multline}\label{Z_kDelta}
%Z_k(A)\phi(y)= (-1)^{k-1}\int_{\mathbb{R}^4}\text{d}^4x_1  (\Delta^--\Delta^+)(y-x_1)   \slashed{A}(x_1) \\
%\prod_{l=2}^k \left[\int_{\mathbb{R}^4}\text{d}^4x_l \Delta^+(x_{l-1}-x_l)   \slashed{A}(x_l)\right]
% \phi(x_k).
%\end{multline}


% This is convenient, because we may now use the spacetime
%integration with the exponential factor of the definition of \(\Delta^-\) as a Fourier transform
%acting on the four-potentials and the wave function. 
%Undoing the substitutions again for the first factor and executing the just mentioned Fourier
%transforms using the convolution theorem inductively results in

%\begin{multline}\label{explicit_Zk}
%Z_k(A)\phi(y) =- i  \int_{\mathcal{M}}\frac{\mathfrak{i}_p(\text{d}^4p_1)}{(2\pi)^{3}} \frac{\slashed{p}_1+m}{2m} e^{-ip_1y}  \\
%  \prod_{l=2}^{k} \left[ \int_{\mathbb{R}^4+i \epsilon e_0}\frac{\text{d}^4p_l}{(2\pi)^{4}} \slashed{A}(p_{l-1}-p_l)  (\slashed{p}_l-m)^{-1}  
% \right]\\
% \int_{\mathcal{M}}  \mathfrak{i}_p(\text{d}^4p_{k+1})\slashed{A}(p_{k}-p_{k+1})\hat{\phi}(p_{k+1}).
%\end{multline}

%Due to the representation \eqref{Z_kDelta} one may also represent \(Z_k\) in terms of The operators
%\begin{align}
%\Delta^0:= \Delta^+-\Delta^-\\
%L_A^{\pm,0}:= \Delta^{\pm,0} \ast \slashed{A}
%\end{align}
%in this manner
%\begin{equation}
%Z_k(A)\phi(y)= (-1)^{k} L^0_A \left({L_A^+}^{k-1}( \phi)\right) (y),
%\end{equation}
%where the upper right index for an operator means iterative application of said operator.

\noch{delete derivation; simply state series representation and convergence result citing something}

\subsection{Construction of the Second Quantised Scattering-Matrix}\label{sec:second quant}
 In the following I outline how the construction of the
second quantised scattering operator is to be carried out, we will naturally be
led to an informal power series representation for the scattering operator \(S\). 

First I fix some more notation in agreement with \cite{ivp2}. Using a general Hilbertspace \(\mathcal{H}\) as a one-particle Hilbertspace. 
A closed subspace \(\mathcal{H}^+\) of \(\mathcal{H}\) is called polarisation if both \(\mathcal{H}^+\) and \(\mathcal{H}^-:=\left(\mathcal{H}^+\right)^\perp\)
are infinite dimensional, where by \(\perp\) I denote the orthogonal complement. With a polarisation \(\mathcal{H}^+\) comes also the orthogonal
projection operator \(P^+\) onto the subspace \(\mathcal{H}^+\) and its complement \(P^-=1-P^+\). For one particle operators \(C\) we 
introduce the notation \(C_{\#\ddag}:= P^\# C P^\ddag\), where \(\#,\ddag\in\{+,-\}\).
%\Dirk{am besten schon oben einführen}
One constructs the Fock space associated with \(\mathcal{H}\) and a polarisation \(\mathcal{H}^+\) of \(\mathcal{H}\) in the following way.
We define \(\overline{\mathcal{H}^-}\) identical with \(\mathcal{H}^-\) as a set, but scalar multiplication as 
\(\mathbb{C}\times \overline{\mathcal{H}} \ni (a, \psi)\mapsto \overline{a} \psi\) where the bar denotes complex conjugation of complex numbers.
A wedge \(\wedge\) in the exponent denotes that only elements which are antisymmetric with respect to permutations
are allowed. This antisymmetric product as well as the tensor product are to be understood in the Hilbert space sense. 
The Factor \(\left(\mathcal{H}^{\pm}\right)^0\) is understood as
\(\mathbb{C}\). We now define Fock space as

\begin{equation}
\mathcal{F}:=\bigoplus_{m,p=0}^\infty \left(\mathcal{H}^+ \right)^{\wedge m} \otimes \left(\overline{\mathcal{H}^- }\right)^{\wedge p}.
\end{equation}

I will denote the sectors of Fock space of fixed particle
numbers by \(\mathcal{F}_{m,p}\). The element of
\(\mathcal{F}_{0,0}\) of norm \(1\) will be denoted by \(\Omega\).
The simplest and yet interesting example of this construction is
the Fock space constructed on a hyperplane prior to the support of an external field,
in this case \(\mathcal{H}=L^2(\mathbb{R}^3,\mathbb{C}^4)\) and \(\mathcal{H}^+\)
consists of the wavefunctions that can be constructed from the generalised eigenfunctions
of positive energy with respect to the free Dirac Hamiltonian.

The
annihilation operator \(a\) acts on an arbitrary sector of Fock space
\(\mathcal{F}_{m,p}\), for any \(m,p\in\mathbb{N}_0\) with either of the operator types


\begin{align}
a: &\overline{\mathcal{H}}\otimes \mathcal{F}_{m,p} \rightarrow \mathcal{F}_{m-1,p}\oplus \mathcal{F}_{m,p+1}\\
a: &\overline{\mathcal{H}}\times \mathcal{F}_{m,p} \rightarrow \mathcal{F}_{m-1,p}\oplus \mathcal{F}_{m,p+1}
\end{align}
regardless of the exact type of the annihilation operator I will denote it by \(a\). Also here the tensor product is 
understood in the algebraic sense.
I start out by defining \(a\) on elements of 
\(\{\bigwedge_{l=1}^m \varphi_l \otimes \bigwedge_{c=1}^p \phi_c  \mid \forall c:  \varphi_c \in \mathcal{H}^+, \phi_c \in \mathcal{H}^-   \}\)
which spans a dense subset of \(\mathcal{F}_{m,p}\), then one continues this operator uniquely by 
linearity and finally by the bounded linear extension theorem to all of \(\mathcal{F}_{m,p}\) and then 
again by linearity to all of \(\overline{\mathcal{H}}\otimes \mathcal{F}_{m,p}\).
\begin{align}
&a\left(\phi \otimes \bigwedge_{l=1}^m \varphi_l \otimes \bigwedge_{c=1}^p \phi_c\right)=a\left(\phi,\bigwedge_{l=1}^m \varphi_l \otimes \bigwedge_{c=1}^p \phi_c\right)\\
&= \sum_{k=1}^m (-1)^{1+k} \langle P^+ \phi, \varphi_k\rangle \bigwedge_{\overset{l=1}{l\neq k}}^m \varphi_l \otimes \bigwedge_{c=1}^p \phi_c + \bigwedge_{l=1}^m \varphi_l \otimes P^- \phi \wedge \bigwedge_{c=1}^p \phi_c
\end{align}


where \(\langle, \rangle\) denotes that the scalar product of \(\mathcal{H}\). The first summand on the right hand side is taken to vanish for \(m=0\). 
For \(\varphi\in \mathcal{H}\) I will also use the abbreviation \(a(\varphi):=a(\varphi,\cdot)\).

Now we turn to the construction of the \(S\)-matrix, the second quantised analogue of \(U^A\). This construction is carried out axiomatically. The first axiom makes sure that the following diagram, and the analogue for the adjoint of the annihilation operator commute.
\begin{equation}
\begin{CD}								%heuristics with infinite wedge space?
\mathcal{F}     @>S^A>>  \mathcal{F}\\
@AAaA        @AAaA\\
\overline{\mathcal{H}}\otimes \mathcal{F}     @>U^A\otimes S^A>>  \overline{\mathcal{H}}\otimes \mathcal{F} 
\end{CD}
\end{equation}
\begin{axiom}
The \(S\) operator fulfils the ``lift condition''.
\begin{align}\label{lift_condition1}
\forall \phi\in \mathcal{H}:& \hspace{0.5cm} S^A \circ a(\phi)=a\left( U^A \phi \right)  \circ S^A,\tag{lift condition}\\
\label{lift_condition2}
\forall \phi\in \mathcal{H}:& \hspace{0.5cm} S^A \circ a^*(\phi)=a^*\left( U^A \phi \right)  \circ S^A,\tag{adjoint lift condition}
\end{align}
where \(a^*\) is the adjoint of the annihilation operator, the creation operator. 
\end{axiom}

There is a convergent power series of the one-particle scattering operator \(U^A\):
\begin{equation}\label{U_expansion}
U^A = \sum_{k=0}^\infty \frac{1}{k!} Z_k(A),
\end{equation}
where \(Z_k(A)\) are bounded operators on \(\mathcal{H}\), which are homogeneous of degree \(k\) in \(A\).
We try an analogous formal power series ansatz for the second quantised scattering operator \(S^A\)

\begin{equation}\label{S_expansion}
S^A=\sum_{k=0}^\infty \frac{1}{k!} T_k(A).
\end{equation}
Here \(T_k\) are assumed to be homogeneous of degree \(k\) in \(A\); however, they will only turn out to be bounded on
fixed particle number subspaces \(\mathcal{F}_{m,p}\) of Fock space. It is the goal of the following  sections to
show that this ansatz indeed works. That is, we can identify operators \(T_k\) such that \eqref{S_expansion}
holds up to a global phase and furthermore the question of convergence can be settled if one assumes that 
the phase is analytic in the external field \(A\). %In constructing a guess for the scattering operator \(S^A\) we will assume \eqref{S_expansion} to converge absolutely when applied to an element of Fock space with finitely many particles.
In order to fully characterise \(S^A\) it is enough to characterise all of the \(T_k\) operators. 
Using the \eqref{lift_condition1} one can derive commutation relations for the operators 
\(T_k\) by plugging in \eqref{U_expansion} and \eqref{S_expansion} into \eqref{lift_condition1} and \eqref{lift_condition2}
and collecting all terms with the same degree of homogeneity. They are given by

\begin{equation}\label{logarithmic lift condition}
\left[T_m(A) , a^\# (\phi)\right]= \sum_{j=1}^{m} \begin{pmatrix} m \\ j \end{pmatrix} a^\# \left(Z_j (A) \phi \right) T_{m-j}(A), 
\end{equation}
where \(a^\#\) is either \(a\) or \(a^*\). Together \(T_k\) and \(\langle T_k\rangle\) characterise the operator \(T_k\) on the whole algebraic
direct sum, it can then be further extended to all of Fock space.

Before we go on to construct a concrete form of the scattering operator, we will first define a certain kind of unitary operator on Fock space.

\subsection{Differential second quantisation}

Let \(B:\mathcal{H}\rightarrow\mathcal{H}\) be a bounded
operator on \(\mathcal{H}\), such that \(i B\) is self adjoint and \(B_{+-}\) is a Hilbert-Schmidt operator. 
We would like to construct a version \(\mathrm{d}\Gamma(B)\) of \(B\) that acts on Fock space and also is skew adjoint.
The strategy of this section is to construct an operator in two steps that is essentially self adjoint of the Fock space of 
finitely many particles, a dense subset of Fock space. It is denoted by

\begin{Def}
\begin{equation}
\mathcal{F}':=\bigobot_{m,p=0}^\infty \mathcal{F}_{m,p},
\end{equation}
where \(\bigobot\) refers to the algebraic direct sum.
\end{Def}
Because \(B_{-+}:\mathcal{H}^+\rightarrow \mathcal{H}^-\) is compact, there is an ONB \((\varphi_n)_{n\in\mathbb{N}}\) 
of \(\mathcal{H}^+\) and likewise an ONB \((\varphi_{-n})_{n\in\mathbb{N}}\) of \(\mathcal{H}^-\) such that it takes the canonical form 
of compact operators

\begin{equation}
B_{-+} = \sum_{n\in\mathbb{N}} \lambda_n |\varphi_{-n}\rangle \langle \varphi_{n}|, \quad \lambda_n \ge 0.
\end{equation}
Here the numbers \(\lambda_n\) fulfil \(\sum_{k=1}^\infty \lambda_k^2 = \|B_{-+}\|_{\text{HS}}<\infty\). As a consequence we have

\begin{equation}
B_{+-} = -\sum_{n\in\mathbb{N}} \lambda_n |\varphi_{n}\rangle \langle \varphi_{-n}|.
\end{equation}

With respect to this basis we define the set of finite linear combinations of product states of finitely many particles

\begin{Def}
We define
\begin{equation}
\mathcal{F}^0\!\!:=\! \mathrm{span}\!\! \left\{\prod_{k=1}^m \!a^*\!(\varphi_{L_k}\!)\!\prod_{c=1}^p\!\! a(\varphi_{-C_c})\Omega\mid m,p\!\in\!\!\mathbb{N}, (L_k)_k,\!(C_c)_c\!\subset\!\mathbb{N} \!\right\}\!,
\end{equation}
we will refer to a subset of this set for fixed values of \(m\) and \(p\) by \(\mathcal{F}^0_{m,p}\).
\end{Def}

In order to do so, the following splitting turns out to be advantageous. 

\begin{Def}
We define the following operators of type \(\mathcal{F}^0\rightarrow \mathcal{F}\)

\begin{align}\label{predefdGamma}
\mathrm{d}\Gamma(B_{++})&:= \sum_{n\in\mathbb{N}}  a^*(B_{++} \varphi_n) a(\varphi_n) \\
\mathrm{d}\Gamma(B_{--})&:= -\sum_{n\in\mathbb{N}}   a(\varphi_{-n})a^*(B_{--} \varphi_{-n}) \\
\mathrm{d}\Gamma(B_{-+})&:= \sum_{n\in\mathbb{N}}  a^*(B_{-+}\varphi_{n}) a(\varphi_n)
\end{align}
where the sum converges in the strong operator topology and \((\varphi_n)_n , (\varphi_{-n})_n\) are arbitrary ONBs of \(\mathcal{H}^+\) and \(\mathcal{H}^-\).
\end{Def}



\begin{Lemma}
The operators \(\mathrm{d}\Gamma(B_{++}),\mathrm{d}\Gamma(B_{--})\) and \(\mathrm{d}\Gamma(B_{-+})\) 
restricted to \(|_{\mathcal{F}^0_{m,p}}\) they have the following type
\begin{align}
\mathrm{d}\Gamma(B_{++})|_{\mathcal{F}^0_{m,p}}&: \quad \mathcal{F}^0_{m,p} \rightarrow \mathcal{F}_{m,p}\\
\mathrm{d}\Gamma(B_{--})|_{\mathcal{F}^0_{m,p}}&: \quad \mathcal{F}^0_{m,p} \rightarrow \mathcal{F}_{m,p}\\
\mathrm{d}\Gamma(B_{-+})|_{\mathcal{F}^0_{m,p}}&: \quad \mathcal{F}^0_{m,p} \rightarrow \mathcal{F}_{m-1,p-1}
\end{align}
 and fulfil
the following bounds for all \(m,p\)
\begin{align}
\|\mathrm{d}\Gamma(B_{++})|_{\mathcal{F}^0_{m,p}}\|&\le (m+1)\|B_{++}\|\\
\|\mathrm{d}\Gamma(B_{--})|_{\mathcal{F}^0_{m,p}}\|&\le (p+1)\|B_{--}\|\\
\|\mathrm{d}\Gamma(B_{-+})|_{\mathcal{F}^0_{m,p}}\|&\le \|B_{-+}\|_{\text{HS}}.
\end{align}
\end{Lemma}
\begin{proof}
Pick \(\alpha\in\mathcal{F}^0_{m,p}\) for \(m,p\in\mathbb{N}_0\), \(\alpha\) can be expressed in terms of a general ONB 
\((\tilde{\varphi}_k)_{k\in\mathbb{N}}\) of \(\mathcal{H}^+\) and \((\tilde{\varphi}_{-k})_{k\in\mathbb{N}}\) of \(\mathcal{H}^-\)
\begin{equation}
\alpha=\sum_{\overset{L,C\subset \mathbb{N}}{|L|=m,|C|=p}}\!\!\!\!\!\!\alpha_{L,C} \prod_{l=1}^m a^*(\tilde{\varphi}_{L_l}) \prod_{c=1}^p a(\tilde{\varphi}_{-C_c}) \Omega.
\end{equation}
In this expansion only finitely many coefficients \(\alpha_{\cdot, \cdot}\) are nonzero. Our operators all map the vacuum onto the zero vector, so commuting them 
through the products of of creation and annihilation operators in the expansion of \(\alpha\) we can make the action of them more explicit:
\begin{align}\nonumber
\mathrm{d}\Gamma(B_{++}) \alpha =\hspace{-0.5cm} \sum_{\overset{L,C\subset \mathbb{N}}{|L|=m,|C|=p}}\!\!\!\!\!\!\alpha_{L,C} \sum_{b=1}^m 
\prod_{l=1}^{b-1} a^*(\tilde{\varphi}_{L_l})  \sum_{n\in\mathbb{N}} a^*(B_{++}\varphi_n) \langle \varphi_n, \tilde{\varphi}_{L_b}\rangle \\
\prod_{l=b+1}^m a^*(\tilde{\varphi}_{l}) \prod_{c=1}^p a(\tilde{\varphi}_{-C_c}) \Omega\\
=\sum_{\overset{L,C\subset \mathbb{N}}{|L|=m,|C|=p}}\!\!\!\!\!\!\alpha_{L,C} \sum_{b=1}^m 
\prod_{l=1}^{b-1} a^*(\tilde{\varphi}_{L_l})  ~~ a^*(B_{++} \tilde{\varphi}_{L_b})  \!\!\!
\prod_{l=b+1}^m a^*(\tilde{\varphi}_{l}) \prod_{c=1}^p a(\tilde{\varphi}_{-C_c}) \Omega.
\end{align}
We notice, that \(\mathrm{d}\Gamma(B_{++})\alpha \in \mathcal{F}_{m,p}\) holds. What is left to show for the first operator is therefore
its norm. For estimating this we see that \(B_{++}\) in the last line can be replaced by 
\begin{equation}
B^L_{L_b}:=\left(1-\sum_{\overset{l=1}{l\neq b}}^m |\tilde{\varphi}_{L_l}\rangle \langle \tilde{\varphi}_{L_l}|\right) B_{++},
\end{equation}
due to the antisymmetry of fermions. Expanding 
\begin{align}\nonumber
&\|\mathrm{d}\Gamma(B_{++})\alpha\|^2 = \langle \mathrm{d}\Gamma(B_{++})\alpha, \mathrm{d}\Gamma(B_{++})\alpha \rangle \\\nonumber
&= \hspace{-1cm}\sum_{\overset{L,C, L',C'\subset \mathbb{N}}{|L'|=|L|=m,|C'|=|C|=p}}\hspace{-1cm}\overline{\alpha_{L,C}}\alpha_{L',C'} \sum_{b,b'=1}^m 
\left\langle \prod_{l=1}^{b-1} a^*(\tilde{\varphi}_{L_l})  ~~ a^*(B^{L}_{L_b} \tilde{\varphi}_{L_b})\right.  \\\nonumber
&\prod_{l=b+1}^m a^*(\tilde{\varphi}_{L_l}) \prod_{c=1}^p a(\tilde{\varphi}_{-C_c}) \Omega, 
\prod_{l=1}^{b'-1} a^*(\tilde{\varphi}_{L'_l})  ~~ a^*(B_{L'_{b'}}^{L'} \tilde{\varphi}_{L'_b})  \\
&\left.\prod_{l=b'+1}^m a^*(\tilde{\varphi}_{L'_l}) \prod_{c=1}^p a(\tilde{\varphi}_{-C'_c}) \Omega\right\rangle
\end{align}
we see that in fact \(C\) and \(C'\) need to agree, because we can just commute the corresponding annihilation operators from one end of the 
scalar product to the other. Furthermore only a single wavefunction on each side of the scalar product is modified, this implies that in order for the
scalar product not to vanish \(|L\cap L'|\ge m-2\) has to hold. If \(L\neq L'\) the double sum over \(n,n'\) has only the contribution where 
\(b=L_l \not\in L'\) and \(b'=L'_{l'}\not\in L\) are selected. Otherwise the full sum contributes, yielding
\begin{align}\nonumber
&\|\mathrm{d}\Gamma(B_{++})\alpha\|^2 = \\\nonumber
&= \hspace{-0.4cm}\sum_{\overset{L,C\subset \mathbb{N}}{\overset{|C|=p}{|L|=m-1}}}\hspace{-0.2cm}\sum_{n\neq n'\in\mathbb{N}\backslash L }
\hspace{-0.4cm}\overline{\alpha_{L\cup\{n\},C}}\alpha_{L\cup\{n'\},C}
 \langle B^{L\cup\{n\}}_n \tilde{\varphi}_{n}, B^{L\cup\{n'\}}_{n'}\tilde{\varphi}_{n'}\rangle (-1)^{g(L,n)+g(L,n')}\\\label{B++estimate}
&+\sum_{\overset{L,C\subset \mathbb{N}}{|L|=m,|C|=p}}\hspace{-0.4cm} |\alpha_{L,C}|^2
 \sum_{b,b'=1}^m 
\left\langle \prod_{l=1}^{b-1} a^*(\tilde{\varphi}_{L_l})  ~~ a^*(B^{L}_{L_b} \tilde{\varphi}_{L_b})\right.  \\\nonumber
&\prod_{l=b+1}^m a^*(\tilde{\varphi}_{L_l}) \Omega, 
\prod_{l=1}^{b'-1} a^*(\tilde{\varphi}_{L_l})  ~~ a^*(B_{L_{b'}}^{L} \tilde{\varphi}_{L_b})  \left.\prod_{l=b'+1}^m a^*(\tilde{\varphi}_{L_l}) ) \Omega\right\rangle,
\end{align}
where \(g(L,n):=|\{l\in L \mid l<n\}|\) keeps track of the number of anti commutations. In the first sum we add and subtract the terms 
where \(n=n'\). The enlarged sum can then be reformulated

\begin{align}\nonumber
&\sum_{\overset{L,C\subset \mathbb{N}}{\overset{|L|=m-1}{|C|=p}}}\hspace{-0.1cm}\sum_{n,n'\in\mathbb{N}\backslash L} \hspace{-0.3cm}\overline{\alpha_{L\cup\{n\},C}}\alpha_{L\cup\{n'\},C} 
 \langle B^{L\cup\{n\}}_n \tilde{\varphi}_{n}, B^{L\cup\{n'\}}_{n'}\tilde{\varphi}_{n'}\rangle (-1)^{g(L,n)+g(L,n')}\\\nonumber
 &=\sum_{\overset{L,C\subset \mathbb{N}}{|L|=m-1,|C|=p}}\left\|\sum_{n\in\mathbb{N}\backslash L}\alpha_{L\cup\{n\},C}  B^{L\cup\{n\}}_n \tilde{\varphi}_{n} (-1)^{g(L,n)} \right\|^2\\\label{B++ first term}
  &=\sum_{\overset{L,C\subset \mathbb{N}}{\overset{|L|=m-1}{|C|=p}}}\left\|\left(1-\sum_{l\in L}|\tilde{\varphi}_l\rangle\langle \tilde{\varphi}_l | \right)B_{++}\hspace{-0.2cm}\sum_{n\in\mathbb{N}\backslash L}\hspace{-0.2cm}\alpha_{L\cup\{n\},C}  \tilde{\varphi}_{n} (-1)^{g(L,n)} \right\|^2
\end{align}
Now the operator product inside the norm has operator norm \(\|B_{++}\|\) and so we can estimate the whole object by
\begin{equation}\label{B++estimatefinal1}
\eqref{B++ first term}\le \|\alpha\|^2 \|B_{++}\|^2.
\end{equation}
Now for the first term in \eqref{B++estimate} we need to estimate the term we added to complete the norm square, this is done as follows
\begin{align}\nonumber
&\sum_{\overset{L,C\subset\mathbb{N}}{|L|=m-1,|C|=p}} \hspace{-0.4cm}\sum_{n\in\mathbb{N}\backslash L} |\alpha_{L\cup\{n\},C}|^2 \|B_n^{L\cup\{n\}} \tilde{\varphi}_{n}\|^2\\\label{B++estimatefinal2}
&\le \hspace{-0.6cm}\sum_{\overset{L,C\subset\mathbb{N}}{|L|=m,|C|=p}}\hspace{-0.5cm} \|B_{++}\|^2|\alpha_{L,C}|^2 =\|\alpha\|^2 \|B_{++}\|^2.
\end{align}
What remains is the second sum in \eqref{B++estimate}, for this term there are two cases.  If \(b=b'\) then the scalar product is 
equal to \(\langle B_{L_b}^L \tilde{\varphi}_b, B_{L_{b}}^L \tilde{\varphi}_{b}\rangle\). If \(b\neq b'\) the scalar product is, up to a sign,
equal to \(\langle B_{L_b}^L \tilde{\varphi}_b,\tilde{\varphi}_{b}\rangle \langle \tilde{\varphi}_{b'}, B_{L_{b'}}^L \tilde{\varphi}_{b'}\rangle\).
However both of these terms can be estimated by \(\|B_{++}\|^2\). So all \(m^2\) summands of this sum contribute \(\|B_{++}\|^2\). Overall 
this estimate yields
\begin{align}\nonumber
&\|\mathrm{d}\Gamma(B_{++})\alpha\|^2 \le \eqref{B++estimatefinal1}+\eqref{B++estimatefinal2} + \|\alpha\|^2 m^2 \|B_{++}\|^2 \\\nonumber
&= \|\alpha\|^2 (2+m^2) \|B_{++}\|^2.
\end{align}
For convenience of notation the estimate can be weakened to 
\begin{equation}
\|\mathrm{d}\Gamma(B_{++})\alpha\| \le (m+1) \|B_{++}\|,
\end{equation}
because for all \(m\neq0\) 
this estimate is and upper bound on what we found, but for \(m=0\) the operator \(\mathrm{d}\Gamma(B_{++})\) is actually the zero operator.
A completely analogous argument works for \(\mathrm{d}\Gamma(B_{--})\). 

So lets move on to \(\mathrm{d}\Gamma(B_{-+})\). Applying it to the same \(\alpha\in \mathcal{F}^0_{m,p}\) again we permute all the operators
to the right, where they annihilate the vacuum. The remaining terms are

\begin{align}\nonumber
\sum_{n\in\mathbb{N}} a^*(B_{-+}\varphi_n)a(\varphi_{n})\hspace{-0.5cm}\sum_{\overset{L,C\subset \mathbb{N}}{|L|=m,|C|=p}}  \hspace{-0.5cm}\alpha_{L,C} 
\prod_{l=1}^m a^*(\tilde{\varphi}_{L_l}) \prod_{c=1}^p a(\tilde{\varphi}_{-C_c}) \Omega  \\\nonumber
=\sum_{\overset{L,C\subset \mathbb{N}}{|L|=m,|C|=p}}\hspace{-0.5cm} \alpha_{L,C} \sum_{b=1}^m \sum_{d=1}^p (-1)^{m-1+b+d} \langle B_{+-} \tilde{\varphi}_{-C_d},\tilde{\varphi}_{L_b}\rangle \\
\prod_{\overset{l=1}{l\neq b}}^m a^*(\tilde{\varphi}_{L_l})
\prod_{\overset{c=1}{c\neq d}}^p a(\tilde{\varphi}_{-C_c})\Omega.
\end{align}
By counting the remaining creation and annihilation operators we immediately see that \(\mathrm{d}\Gamma(B_{-+})\alpha\in \mathcal{F}_{m-1,p-1}\). 
For estimating the norm of this vector, we switch basis from \((\tilde{\varphi}_{\pm n})_n\) to \((\varphi_{\pm n}')_n\), the basis where 
\(B_{-+}\) takes its canonical form. Then the scalar product involving \(B_{-+}\) reduces to \(\lambda_{L_b}\delta_{L_b,C_d}\). We estimate

\begin{align}\nonumber
&\|\mathrm{d}\Gamma(B_{-+})\alpha\|^2= \hspace{-0.5cm}\sum_{\overset{L,L',C,C'\subset\mathbb{N}}{\overset{|L|=|L'|=m}{|C|=|C'|=p}}} ~\sum_{a,a'=1}^m \sum_{b,b'=1}^p
\overline{\alpha}_{L,C} \alpha_{L',C'}(-1)^{b+d+b'+d'}\lambda_{L_b} \lambda_{L_{b'}'} \\
&\delta_{L_b,C_d} \delta_{L_{b'}',C_{d'}'} \left\langle \prod_{\overset{l=1}{l\neq b}}^m a^*(\varphi_{L_l}')\prod_{\overset{c=1}{c\neq d}}^p 
a(\varphi_{-C_c}')\Omega, \prod_{\overset{l=1}{l\neq b'}}^m a^*(\varphi_{L_l'}')
\prod_{\overset{c=1}{c\neq d'}}^p a(\varphi_{-C_c'}')\Omega\right\rangle.
\end{align}

The scalar product in the second line tells us that \(L\backslash \{L_b\}=L'\backslash \{L_{b'}'\}\) and \(C\backslash \{C_{d}\}=C'\backslash \{C_{d'}'\}\) have
to hold in order for the term not to vanish. So this means that \(L\) and \(L'\) as well as \(C\) and \(C'\) can respectively differ at most by one element
which then has to be in the intersection \(L\cap C\). Because this sum is really just a finite sum, we can reorder it in the following way

\begin{align}\nonumber
\|\mathrm{d}\Gamma(B_{-+})\alpha\|^2
&=\hspace{-0.4cm}\sum_{\overset{L,C\subset\mathbb{N}}{\overset{|L|=m-1}{|C|=p-1}}} \sum_{b,b'\in \mathbb{N}\backslash (L\cup C)}\hspace{-0.3cm} \lambda_{b} \lambda_{b'} 
\overline{\alpha}_{L\cup\{b\},C\cup \{b\}} \alpha_{L\cup\{b'\},C\cup\{b'\}}\\
&(-1)^{g(L,b)+g(C,b)+g(L,b')+g(C,b')},
\end{align}
where \(g(L,b)=|\{l\in L \mid l<b\}|\) as before. This expression can be rewritten in terms of a scalar product in \(\ell^2(\mathbb{N})\)

\begin{align}\nonumber
\|\mathrm{d}\Gamma(B_{-+})\alpha\|^2
&=\hspace{-0.4cm}\sum_{\overset{L,C\subset\mathbb{N}}{\overset{|L|=m-1}{|C|=p-1}}} \left|\left\langle 1_{(L\cup C)^c} \alpha_{L\cup \{\cdot \}, C\cup \{\cdot\}} (-1)^{g(L,\cdot)+g(C,\cdot)},\lambda_\cdot \right\rangle_{\ell^2}  \right|^2 \\
&\le \hspace{-0.4cm}\sum_{\overset{L,C\subset\mathbb{N}}{\overset{|L|=m-1}{|C|=p-1}}} \sum_{b\in \mathbb{N}} 1_{(L\cup C)^c}(b) |\alpha_{L\cup\{b\},C\cup\{b\}}|^2 \sum_{d\in\mathbb{N}}\lambda_d^2\\
&\le \|\alpha\|^2 \|B_{-+}\|^2_{\text{HS}}.
\end{align}

\end{proof}

\begin{Corollary}
The operators \(\mathrm{d}\Gamma(B_{--})\) and \(\mathrm{d}\Gamma(B_{++})\) can be extended by continuity on \(\mathcal{F}^0_{m,p}\) to unbounded operators on all of \(\mathcal{F}'\).
The operator \(\mathrm{d}\Gamma(B_{-+})\) can be continuously extended to all of \(\mathcal{F}\).
\end{Corollary}

\begin{Lemma}
The operator \(\left(\mathrm{d}\Gamma(B_{-+})\right)^*\)  acts on elements of \(\mathcal{F}^0\) as

\begin{equation}
- \sum_{n\in\mathbb{N}} a^*(B_{+-}\varphi_{-n}) a(\varphi_{-n})=:-\mathrm{d}\Gamma(B_{+-}).
\end{equation}
So also \(\mathrm{d}\Gamma(B_{+-}):\mathcal{F}^0\rightarrow \mathcal{F}\) can be extended continuously to all of \(\mathcal{F}\).
Moreover \(\mathrm{d}\Gamma(B_{-+})+\mathrm{d}\Gamma(B_{+-})\) is skew-adjoint.
\end{Lemma}
\begin{proof}
Pick \(\beta,\alpha \in \mathcal{F}^0\). We expand those states with respect to the basis \((\varphi_k')_{k\in\mathbb{Z}\backslash \{0\}}\). Consider

\begin{align}\nonumber
&\langle \beta, \mathrm{d}\Gamma(B_{-+}) \alpha \rangle=\left\langle \beta, \sum_{n\in\mathbb{N}} a^*(B_{-+}\varphi_n)a(\varphi_{n}) \alpha \right\rangle\\\nonumber
&=\sum_{n\in\mathbb{N}} \langle \beta , a^*(B_{-+}\varphi_n)a(\varphi_{n}) \alpha\rangle
=\sum_{n\in\mathbb{N}}  \langle a^*(\varphi_{n})  a(B_{-+}\varphi_n)\beta , \alpha\rangle\\\nonumber
&=\left\langle \sum_{n\in\mathbb{N}}a^*(\varphi_{n})  a(B_{-+}\varphi_n) \beta, \alpha \right\rangle
=\left\langle \sum_{n\in\mathbb{N}} \lambda_n a^*(\varphi_{n})  a(\varphi_{-n}) \beta, \alpha \right\rangle\\
&=\left\langle \sum_{n\in\mathbb{N}}  a^*(- B_{+-}\varphi_{-n})  a(\varphi_{-n}) \beta, \alpha \right\rangle
=-\langle \mathrm{d}\Gamma(B_{+-})  \beta, \alpha \rangle,
\end{align}
 So we see that \(\mathrm{d}\Gamma(B_{+-})\)
and \(\mathrm{d}\Gamma(B_{-+})^*\) agree on \(\mathcal{F}^0\) which is dense. So they are the same bounded and continuous operator on all of
Fock space. 
\end{proof}

\begin{Lemma}
The operator \(\mathrm{d}\Gamma(B):\mathcal{F}'\rightarrow \mathcal{F}\),
\begin{equation}
\mathrm{d}\Gamma(B):=\mathrm{d}\Gamma(B_{++})+\mathrm{d}\Gamma(B_{+-})+\mathrm{d}\Gamma(B_{-+})+\mathrm{d}\Gamma(B_{--})
\end{equation}
is skew symmetric.
\end{Lemma}
\begin{proof}
Since the sum of skew symmetric operators is skew symmetric, it suffices to show skew symmetry of \(\mathrm{d}\Gamma(B_{++})\) and \(\mathrm{d}\Gamma(B_{--})\).
Moreover since both of these operators are extended versions of operators of the same name of type \(\mathcal{F}^0\rightarrow \mathcal{F}\) it suffices to show skew symmetry
on this domain. We will only do the calculation for \(\mathrm{d}\Gamma(B_{++})\), the other calculation is analogous. 
First we notice that
\begin{equation}
\mathrm{d}\Gamma(B_{++})=\sum_{n\in\mathbb{N}} a^*(B_{++}\varphi_{n})a(\varphi_n)
=\sum_{n\in\mathbb{N}} \sum_{m\in\mathbb{N}}  \langle \varphi_m, B_{++}\varphi_n\rangle a^*(\varphi_m) a(\varphi_n)
\end{equation}
holds.  Pick \(\alpha,\beta \in \mathcal{F}^0\). Consider
\begin{align}\nonumber
\left\langle \beta, \mathrm{d}\Gamma(B_{++}) \alpha \right\rangle
= \sum_{L,L',C,C'\subset \mathbb{N}} \overline{\beta}_{L',C'}\alpha_{L,C} 
\left\langle \prod_{l=1}^{|L'|}a^*(\varphi_{L_l'})\prod_{c=1}^{|C'|} a(\varphi_{-C_c'})\Omega,\right. \\ \nonumber
\left. \sum_{n\in\mathbb{N}} a^*(B_{++}\varphi_{n})a(\varphi_n) \prod_{l=1}^{|L|}a^*(\varphi_{L_l})\prod_{c=1}^{|C|} a(\varphi_{-C_c})\Omega\right\rangle\\\nonumber
= \sum_{L,L',C,C'\subset \mathbb{N}} \overline{\beta}_{L',C'}\alpha_{L,C} \sum_{n\in\mathbb{N}}
\left\langle \prod_{l=1}^{|L'|}a^*(\varphi_{L_l'})\prod_{c=1}^{|C'|} a(\varphi_{-C_c'})\Omega,\right. \\ \nonumber
\left. a^*(B_{++}\varphi_{n})a(\varphi_n) \prod_{l=1}^{|L|}a^*(\varphi_{L_l})\prod_{c=1}^{|C|} a(\varphi_{-C_c})\Omega\right\rangle\\\nonumber
= \sum_{L,L',C,C'\subset \mathbb{N}} \overline{\beta}_{L',C'}\alpha_{L,C} \sum_{n\in\mathbb{N}}
\left\langle a^*(\varphi_n) a(B_{++}\varphi_{n})\prod_{l=1}^{|L'|}a^*(\varphi_{L_l'})\prod_{c=1}^{|C'|} a(\varphi_{-C_c'})\Omega,\right. \\ \nonumber
\left.  \prod_{l=1}^{|L|}a^*(\varphi_{L_l})\prod_{c=1}^{|C|} a(\varphi_{-C_c})\Omega\right\rangle\\\nonumber
= \sum_{L,L',C,C'\subset \mathbb{N}} \overline{\beta}_{L',C'}\alpha_{L,C} 
\left\langle \sum_{n\in\mathbb{N}} \sum_{m\in\mathbb{N}} \langle B_{++}\varphi_n,\varphi_m\rangle a^*(\varphi_n) a(\varphi_{m})\right. \\ \nonumber
\left. \prod_{l=1}^{|L'|}a^*(\varphi_{L_l'})\prod_{c=1}^{|C'|} a(\varphi_{-C_c'})\Omega, \prod_{l=1}^{|L|}a^*(\varphi_{L_l})\prod_{c=1}^{|C|} a(\varphi_{-C_c})\Omega\right\rangle.
\end{align}
Now because \(B_{++}^*=-B_{++}\) we see that 
\begin{equation}
\left\langle \beta, \mathrm{d}\Gamma(B_{++})\alpha\right\rangle =- \left\langle \mathrm{d}\Gamma(B_{++})\beta, \alpha\right\rangle 
\end{equation}
holds.
\end{proof}



Now we would like to define \(e^{\mathrm{d}\Gamma(B)}\), in order to do so, we will show that \(\mathrm{d}\Gamma(B)\) is essentially skew-adjoint. 
One way of doing so is by Nelson's analytic vector theorem. 

\begin{Thm}[Nelson's analytic vector theorem]
Let \(C\) be a symmetric operator on a Hilbert space \(\mathscr{H}\). If \(D(C)\) contains a total set 
\(S\subset \bigcap_{n=1}^\infty D(C^n)\) of analytic vectors, then \(C\) is essentially self adjoint. 
A vector \(\phi\in \bigcap_{n=1}^\infty D(C^n)\) is called analytic if there is \(t>0\) such that
\(\sum_{k=0}^\infty \frac{\|C^n \phi\|}{n!} t^n<\infty\) holds. A set \(S\) is said to be total if \(\overline{\text{span}(S)}=\mathscr{H}\)
\end{Thm}
For a proof see e.g. \cite{SimonReed2}.

\begin{Lemma}\label{Gamma exponential bound}
For any \( \alpha \in \mathcal{F}', t>0\) the operator \(\mathrm{d}\Gamma(B):\mathcal{F}'\rightarrow \mathcal{F}\) satisfies
\begin{equation}
\sum_{k=0}^\infty \frac{\|\mathrm{d}\Gamma(B)^k \alpha\|}{k!} t^k <\infty.
\end{equation}
\end{Lemma}
\begin{proof}
 By definition of \(\mathcal{F}'\) there are \(m,p\in\mathbb{N}\) such that \(\alpha \in \bigobot_{l=0}^m\bigobot_{c=0}^p \mathcal{F}_{l,p}\). Fix \(t>0\). 
We dissect \(\alpha\) into its parts of fixed particle numbers:
\begin{equation}
\sum_{k=0}^\infty \frac{\|\mathrm{d}\Gamma(B)^k \alpha\|}{k!} t^k \le \sum_{l=0}^m\sum_{c=0}^p \sum_{k=0}^\infty \frac{\|\mathrm{d}\Gamma(B)^k \alpha_{l,c}\|}{k!} t^k .
\end{equation}
 Using the following abbreviations
\begin{align}
&\Gamma_{-1}:=\mathrm{d}\Gamma(B)_{-+}\\
&\Gamma_0:=\mathrm{d}\Gamma(B)_{++}+\mathrm{d}\Gamma(B)_{--}\\
&\Gamma_{+1}:=\mathrm{d}\Gamma(B)_{+-}\\
&\beta:=\max\{\|B_{++}\|+\|B_{--}\|,\|B_{-+},\|_{\mathrm{HS}}\}
\end{align}
we estimate
\begin{align}\nonumber
\|\mathrm{d}\Gamma(B)^k \alpha_{l,c}\|\le \sum_{x\in \{-1,0,+1\}^k} \left\| \prod_{b=1}^k \Gamma_{x_b}\alpha_{l,c}\right\|\\\label{essentialSelfadjointnessOperatorProduct}
\le  \sum_{x\in \{-1,0,+1\}^k} \prod_{b=1}^k \left\| \Gamma_{x_b}|_{\mathcal{F}_{l+\sum_{d=1}^{b-1} x_d,c+\sum_{d=1}^{b-1} x_d}}\right\| \|\alpha_{l,c}\|\\
\le 3^k \|\alpha\| \max_{x\in \{-1,0,+1\}^k} \prod_{b=1}^k\left\| \Gamma_{x_b}|_{\mathcal{F}_{l+\sum_{d=1}^{b-1} x_d,c+\sum_{d=1}^{b-1} x_d}}\right\|  .
\end{align}
At this point the factors only depend on the number of particles the Fock space vector attains as we act on it with the operators \(\Gamma_{\cdot}\). 
As these bounds increase with the particle number we can restrict the set \(\{-1,0,+1\}\) in the last line to \(\{0,+1\}\).
We notice that the bound in \eqref{essentialSelfadjointnessOperatorProduct} will only increase if  we exchange each pair 
\(x_{i}=1, x_{h}=0\) by the pair \(x_{\max\{i,h\}}=1, x_{\min\{i,h\}}=0\) so that the norm of the operator that acts like
a particle number operator is taken after the particle number is increased. The maximum therefore has the form \((c+l+2+2d)^{k-d}\), which we bound by
\(2^k (c/2+d/2+1+d)^{k-d}\).
For maximising this object we treat \(d\) as a continuous variable take the derivative and set it to zero. From the form of the function to be maximised
it is clear that it is equal to \(1\) for \(d=k\) and at \(d=-c/2-l/2\), but for \(k\) large it will be bigger in between. We abbreviate \(y=c/2+l/2+1\).
\begin{align}
0= (y+d)^{k-d} (-\ln(y+d) + \frac{k-d}{y+d})\\
\iff \frac{k-d}{y+d}= \ln (y+d)\\
\iff \frac{k+y}{y+d}  -1 = -1 + \ln (e (y+d))\\
\iff e(k+y)=e(y+d)\ln(e(y+d))\\
\iff e(k+y)=\ln(e(y+d)) e^{\ln(e(y+d))}\\
\iff W_0(e(k+y))=\ln(e(y+d))\\
\iff e^{W_0(e(k+y))-1}-y=d,
\end{align}

where we made use of the Lambert W function, which is the inverse function of \(x\mapsto x e^x\) and has multiple branches; however as \(e(y+d)>0\) 
\(W_0\) is the only real branch which is applicable here, it corresponds to the inverse of \(x\mapsto x e^x\) for \(x>-1\). 
From the form of the maximising value we see, that it is always bigger than \(-y\). Plugging this back onto our
function we find its maximum

\begin{align}\nonumber
&\max_{d\in ]-y,\infty[} (y+d)^{k-d}=e^{(W_0(e(k+y))-1) (k+y)- (W_0(e(k+y))-1)e^{W_0(e(k+y))-1}}\\\nonumber
&=e^{-(k+y) + (k+y)W_0(e(k+y)) + e^{W_0(e(k+y))-1} -((k+y)e)/e}\\\nonumber
&=e^{-2(k+y) + (k+y)W_0((k+y)e)+ \frac{e(k+y)}{eW_0((k+y)e)}}\\
&=e^{(k+y)(-2+W_0((k+y)e) + W_0((k+y)e)^{-1})},
\end{align}
where we repeatedly used \(W_0(x)e^{W_0(x)}=x\). Putting things together we find

\begin{align}
\|\Gamma(B)^k\alpha_{l,c}\|\le (6\beta )^k \|\alpha\| e^{(k+y)(-2+W_0((k+y)e) + W_0((k+y)e)^{-1})}.
\end{align}
Dividing this by \(k!\) and using the lower bound given by Sterling's formula we would like to prove that 

\begin{align}\label{essentialSelfadjointnessSimplifiedSum}
\sum_{k=1}^\infty (6\beta t )^k   e^{k(1-\ln(k))-\frac{1}{2}\ln(k) +(k+y)(-2+W_0((k+y)e) + W_0((k+y)e)^{-1})}<\infty
\end{align}
holds, where we neglected constant factors and the summand \(k=0\) which do not matter for the task at hand. Next we are going
to use an inequality about the growth of \(W_0\) proven in \cite{hoorfar2008inequalities}. For any \(x\ge e\) 
\begin{equation}
W_0(x)\le \ln(x)-\ln( \ln (x)) + \frac{e}{e-1}\frac{\ln (\ln (x))}{\ln (x)}
\end{equation}
holds true. Plugging this into our sum the exponent is bounded from above by

\begin{align}\nonumber
&k(1-\ln(k))-\frac{1}{2}\ln(k) +(k+y)\Big[-1+\ln(k+y) - \ln(1+ \ln (k+y))\\ \nonumber
&\left.+\frac{e}{e-1} \frac{\ln(1+\ln(k+y))}{1+\ln(k+y)} + W_0((k+y)e)^{-1}\right]\\\nonumber
&=-y + k \ln\left(1+\frac{y}{k}\right) + y \ln (k+y) -\frac{1}{2}\ln(k) +\\\nonumber
&(k+y)\left[ \ln (1+\ln(k+y))\frac{1-(e-1)\ln(k+y)}{(e-1)(1+\ln(k+y))} + W_0((k+y)e)^{-1}\right]\\\label{essentialSelfadjointnessExponent}
&\le y \ln (k+y) -\frac{1}{2}\ln(k) + (k+y)W_0((k+y)e)^{-1} +\\\nonumber
&(k+y) \ln (1+\ln(k+y))\frac{1-(e-1)\ln(k+y)}{(e-1)(1+\ln(k+y))} .
\end{align}
Now it is important to notice that the only remaining term that grows faster than linearly in magnitude is the last summand.
This term; however, is negative for large \(k\), as the fraction converges to \(-(e-1)\) for large \(k\), while the double logarithm
in front grows without bounds. So there is a \(k^*\) big enough such that  for all \(k>k^*\) \eqref{essentialSelfadjointnessExponent} 
is smaller than \(- k (\ln(6\beta t) + 1)\), proving that \eqref{essentialSelfadjointnessSimplifiedSum} in fact holds.

\end{proof}



\begin{Thm}\label{Gamma essential selfadjointness}
The operator \(\mathrm{d}\Gamma(B):\mathcal{F}'\rightarrow \mathcal{F}\) is essentially skew adjoint and hence by Stones theorem
 generates a strongly continuous unitary group \(\left( e^{t ~\widehat{\mathrm{d}\Gamma(B)}}\right)_t\), where \(\widehat{\mathrm{d}\Gamma(B)}\) is the closure of 
 \(\mathrm{d}\Gamma(B)\).
\end{Thm}
\begin{proof}
In order to apply Nelson's analytic vector theorem we pick \(C=\mathcal{F}'\). Pick \(\alpha \in\mathcal{F}'\). We need to show that there is \(t>0\) such that
\begin{equation}
\sum_{k=0}^\infty \frac{\|\mathrm{d}\Gamma(B)^k \alpha\|}{k!} t^k <\infty
\end{equation}
holds. This is guaranteed by the last lemma.
\end{proof}

Lastly in this chapter, we will investigate the commutation properties of \(\mathrm{d}\Gamma(B)\) with general creation and annihilation operators.
These properties are the reason we are interested in this operator, they will prove to be very useful in the next chapter.

\begin{Thm}\label{Commutation Gamma}
For \(\psi \in \mathcal{H}\) we have
\begin{equation}
[\mathrm{d}\Gamma(B),a^\# (\psi)]=a^\#(B\psi),
\end{equation}
where \(a^\#\) can be either \(a\) or \(a^*\).
\end{Thm}
\begin{proof}
Because \(\mathrm{d}\Gamma(B)\) is defined as the extension of an operator on \(\mathcal{F}^0\) it suffices to show the desired identity on this space. In order to do so
we first restrict \(\psi\in \text{span}\{\varphi_{n}| n \in \mathbb{Z}\backslash\{0\}\}\). We will first cover the case \(a(\psi)\).  
As a first step we decompose \(\mathrm{d}\Gamma(B)\) into its four parts

\begin{align}
\left[ \mathrm{d}\Gamma(B),a(\psi) \right] = \left[ \mathrm{d}\Gamma(B_{++})
+\mathrm{d}\Gamma(B_{-+})+\mathrm{d}\Gamma(B_{-+})+\mathrm{d}\Gamma(B_{--}),a(\psi) \right],
\end{align}
each of those parts is evaluated directly. We begin with the \(B_{++}\) part, this can be expressed as

\begin{align}
\left[ \mathrm{d}\Gamma(B_{++}),a(\psi) \right] \\
= \sum_{n\in\mathbb{N}} a^*(B_{++}\varphi_n)a(\varphi_n) a(\psi) - \sum_{n\in\mathbb{N}} a(\psi) a^*(B_{++}\varphi_n)a(\varphi_n) \\
=\sum_{n\in\mathbb{N}} \left[ -\langle \psi, B_{++} \varphi_n\rangle a(\varphi_n) +a(\psi) a^*(B_{++}\varphi_n)a(\varphi_n) \right]  \\
- \sum_{n\in\mathbb{N}} a(\psi) a^*(B_{++}\varphi_n)a(\varphi_n).
\end{align}

Let \(\alpha \in \mathcal{F}^0\). Now applying the expression in the last two lines to \(\alpha\) online finitely many elements of the sum in fact contribute.
So we may split the firs sum into two and observe the cancellation between the last two terms. Continuing we find

\begin{align}
\left[ \mathrm{d}\Gamma(B_{++}),a(\psi) \right] \alpha
=-\sum_{n\in\mathbb{N}}  \langle \psi, B_{++} \varphi_n\rangle a(\varphi_n)\alpha\\
=-a\left(\sum_{n\in\mathbb{N}} \langle B_{++}\varphi_n, \psi\rangle  \varphi_n  \right)\alpha
=a\left(\sum_{n\in\mathbb{N}} \langle \varphi_n, B_{++}\psi\rangle  \varphi_n  \right)\alpha\\
=a(B_{++}\psi)\alpha,
\end{align}

where we used \(B^*=-B^*\). Now for a general \(\psi\in\mathcal{H}\) we pick a sequence 
\((\psi_k)_{k\in\mathbb{N}}\subset \text{span}\{\varphi_{n}| n \in \mathbb{Z}\backslash\{0\}\}\)
such that \(\lim_{k\rightarrow \infty} \psi_k = \psi\). Now because of the calculation we have the equality

\begin{equation}
\left[ \mathrm{d}\Gamma(B_{++}),a(\psi_k) \right] \alpha =a(B_{++}\psi_k)\alpha
\end{equation}

for each \(k\in\mathbb{N}\) and hence if a limit exists it also holds in the limit. Now on the right hand side, because
\(a\) is a bounded operator on all of \(\mathcal{F}\) clearly the limit exists and is equal to \(a(B_{++}\psi)\alpha\).
On the left hand side we know \(\mathrm{d}\Gamma(B)\) to be bounded and hence continuous on every \(\mathcal{F}_{m,p}\)
for every \(m,p\in\mathbb{N}\).  Furhtermore since \(\alpha\in\mathcal{F}^0\) there is are \(m',p'\in\mathbb{N}\) such that 
\(\alpha \in \bigobot_{m=0}^{m'}\bigobot_{p=0}^{p'} \mathcal{F}_{m,p}\) holds and hence we can exchange the limit also 
with \(\mathrm{d}\Gamma(B)\) and find

\begin{equation}
\left[ \mathrm{d}\Gamma(B_{++}),a(\psi) \right] \alpha =a(B_{++}\psi)\alpha
\end{equation}

for general \(\psi\in\mathcal{H}\). The final extension of this equation to all \(\alpha \in \mathcal{F}'\) happens via the continuous linear
extension theorem on \(\mathcal{F}_{m,p}\) for each \(m,p\in\mathbb{N}\). The proof in all seven other cases are completely analogous.
Putting thins together again we obtain

\begin{align}
\left[ \mathrm{d}\Gamma(B_{++}),a(\psi) \right] +\left[ \mathrm{d}\Gamma(B_{-+}),a(\psi) \right] \\
+\left[ \mathrm{d}\Gamma(B_{+-}),a(\psi) \right] +\left[ \mathrm{d}\Gamma(B_{--}),a(\psi) \right]  =\\
a(B_{++}\psi)+a(B_{+-}\psi)+a(B_{-+}\psi)+a(B_{--}\psi)\iff \\
\left[ \mathrm{d}\Gamma(B),a(\psi) \right] =a(B\psi)
\end{align}
on all of \(\mathcal{F}'\).

\end{proof}

 

\subsection{Presentation and Proof of the Formula}\label{sec:proof simple formula}

In this chapter we verify the formula for the \(S\)-matrix directly.
For a heuristic derivation in the appendix in section\ref{sec:heuristic construction}


\begin{Thm}\label{sleek_second_quantised_scattering_operator}
For \(A\) such that 
\begin{equation}
\|\id-U^A\|<1.
\end{equation}\label{eq:sleek_scattering}
The second quantized scattering operator fulfils
\begin{equation}\label{eq:sleek_second_quantised_scattering_operator}
S^A= e^{i \varphi^A} e^{\mathrm{d}\Gamma(\ln (U^A))}
\end{equation}
for some phase \(\varphi^A\in\mathbb{R}\), that may depend on the external field \(A\).

\end{Thm}
\begin{proof}
In order to establish this theorem we need to verify that the expression given in \eqref{eq:sleek_scattering} for the scattering operator
 is a well defined object
and fulfils \eqref{lift_condition1} and \eqref{lift_condition2}. Because these conditions uniquely fix the Operator \(S^A\)  up to a phase this suffices as a proof.

Well definedness is established, by theorem \ref{Gamma essential selfadjointness}, because for unitary \(U^A\)
with \(\|1-U^A\|<1\) the power series of the logarithm converges and fulfils
\begin{align}
\|\ln(U^A)\|=\|\ln (1-(1-U^A))\|= \left\| -\sum_{k=1}^\infty \frac{(1-U^A)^k}{k} \right\|\\
 \le   \sum_{k=1}^\infty \frac{\|1-U^A\|^k}{k}=-\ln(1-\|1-U^A\|)
\end{align}
implying that the power series of the logarithm around the identity is a well defined map from the one particle operators of norm less than
one to the bounded one particle operators. Moreover this operator fulfils \([\ln (U^A)]^*=\ln (U^A)^*= \ln (U^A)^{-1}=-\ln (U^A)\), so 
\(\mathrm{d}\Gamma(\ln U^A)\) is a well defined unbounded operator that is essentially self adjoint on the finite particle sector of Fockspace \(\mathcal{F}'\).


Let \(\varphi\in \mathcal{H}\), for any \(k\in \mathbb{N}_0\) we see applying the commutation relation of \(\mathrm{d}\Gamma\):
\begin{multline*}
\mathrm{d}\Gamma(\ln U) \sum_{l=0}^k \binom{k}{l} a^\# \left(\left(\ln U\right)^l \varphi \right) \left(\mathrm{d}\Gamma(\ln U)\right)^{k-l} = \\
\sum_{l=0}^k \binom{k}{l} a^\# \left(\left(\ln U\right)^{l+1} \varphi \right) \left(\mathrm{d}\Gamma(\ln U)\right)^{k-l}\\
+\sum_{l=0}^k \binom{k}{l} a^\# \left(\left(\ln U\right)^l \varphi \right) \left(\mathrm{d}\Gamma(\ln U)\right)^{k-l+1}\\
= \sum_{b=0}^{k+1} \left( \binom{k}{b-1} + \binom{k}{b}\right) a^\#\left( (\ln U)^b \varphi\right) \left( \mathrm{d}\Gamma(\ln U)\right)^{k+1-b}\\
=\sum_{b=0}^{k+1}  \binom{k+1}{b}  a^\#\left( (\ln U)^b \varphi\right) \left( \mathrm{d}\Gamma(\ln U)\right)^{k+1-b},
\end{multline*}
so we see that for \(k\in\mathbb{N}_0\)
\begin{equation}
\left(\mathrm{d}\Gamma(\ln U)\right)^k a^\# (\varphi) = \sum_{b=0}^{k}  \binom{k}{b}  a^\#\left( (\ln U)^b \varphi\right) \left( \mathrm{d}\Gamma(\ln U)\right)^{k-b}
\end{equation}
holds. Let \(\alpha \in \mathcal{F}'\). Using what we just obtained, we conclude
\begin{multline*}
e^{\mathrm{d}\Gamma(\ln U)}a^\#(\varphi) = \sum_{k=0}^\infty \frac{1}{k!} \left(\mathrm{d}\Gamma(\ln U)\right)^k a^\# (\varphi)\alpha\\
=\sum_{k=0}^\infty \frac{1}{k!}  \sum_{b=0}^{k}  \binom{k}{b}  a^\#\left( (\ln U)^b \varphi \right) \left( \mathrm{d}\Gamma(\ln U)\right)^{k-b}\alpha\\
\overset{*}{=}\sum_{c=0}^\infty \sum_{l=0}^\infty \frac{1}{c! l!} a^\#\left( (\ln U)^c \varphi \right) \left( \mathrm{d}\Gamma(\ln U)\right)^{l}\alpha\\
=a^\#\left( e^{\ln U} \varphi \right) e^{\mathrm{d}\Gamma( \ln U)}\alpha
=a^\#\left( U \varphi \right) e^{\mathrm{d}\Gamma( \ln U)}\alpha.
\end{multline*}
For the marked equality changing oder of summation is justified, because by the bounds
 \(\|a^\#((\ln U)^c \varphi)\|\le \|\ln U\|^c\) and  lemma \ref{Gamma exponential bound} the sum obtained by changing the order
 of summands converges absolutely.
Clearly multiplying the second quantised operator by an additional phase as in 
\eqref{sleek_second_quantised_scattering_operator} does not influence this calculation.
\end{proof}

\noch{decide whether to use or delete this part.}
As a preparation for calculating the vacuum polarisation current we proof the following 
\begin{Lemma}\label{G_kommutator}
Let \(P_k,P_l\in Q\) then the following holds
\begin{equation}
\left[G(P_k),G(P_l)\right]= 
\tr\left(P_{\stackrel{k}{-+}}P_{\stackrel{l}{+-}}\right)
-\tr\left(P_{\stackrel{l}{-+}}P_{\stackrel{k}{+-}}\right) 
+G\left(\left[P_k,P_l\right]\right)
.\end{equation}
\end{Lemma}
For a proof of this lemma let \(P_k,P_l \in Q\), we compute
\begin{align*}
&\left[G(P_k),G(P_l)\right]\stackrel{\eqref{G commutator}}{=}\\
&=\sum_{n,b\in\mathbb{N}} \left[ a^*\left(P_k\varphi_n\right) a(\varphi_n), a^*\left(P_l\varphi_b\right)a(\varphi_n)\right]\\
&-\sum_{-b,n\in\mathbb{N}}\left[a^*\left(P_k\varphi_n\right)a(\varphi_n), a(\varphi_b) a^*\left(P_l\varphi_b\right)\right]\\
&-\sum_{-n,b\in\mathbb{N}}\left[a(\varphi_n)a^*\left(P_k\varphi_n\right),a^*\left(P_l\varphi_b\right)a(\varphi_b)\right]\\
&+\sum_{n,b\in\mathbb{N}}\left[ a(\varphi_n)a^*\left(P_k\varphi_n\right), a(\varphi_b)a^*\left(P_l\varphi_b\right)\right]\\
&=\sum_{n,b\in\mathbb{N}} \left(a^*\left(P_k\varphi_n\right)\left<\varphi_n,P_l\varphi_b\right>a(\varphi_b)-a^*\left(P_l\varphi_b\right) \left<\varphi_b,P_k\varphi_n\right>a(\varphi_n) \right)\\
&-\sum_{-b,n\in\mathbb{N}}\left( - \left<\varphi_b,P_k \varphi_n\right>a(\varphi_n)a^*\left(P_l\varphi_b\right) + a(\varphi_b) a^*\left(P_k\varphi_n\right)\left<\varphi_n,P_l\varphi_b\right>\right)\\
&-\sum_{-n,b\in\mathbb{N}} \left( - \left< \varphi_n,P_l\varphi_b\right> a^*\left(P_k\varphi_n\right) a(\varphi_b) + a^*\left(P_l\varphi_b\right)a(\varphi_n)\left<\varphi_b,P_k\varphi_n\right>\right)\\
&+\sum_{n,b\in -\mathbb{N}} \left(a(\varphi_n) \left< \varphi_b,P_k\varphi_n\right>a^*\left(P_l\varphi_b\right)-a(\varphi_b)\left<\varphi_n,P_l\varphi_b\right> a^*\left(P_k\varphi_n\right)\right)\\
&=\sum_{b\in\mathbb{N}} a^*\left(P_k P_{\stackrel{l}{++}}\varphi_b\right)a(\varphi_b) - \sum_{n\in\mathbb{N}}a^*\left(P_l P_{\stackrel{k}{++}}\varphi_n\right)a(\varphi_n)\\
&+\sum_{n\in\mathbb{N}}a(\varphi_n)a^*\left(P_l P_{\stackrel{k}{-+}}\varphi_n\right) - \sum_{-b\in\mathbb{N}} a(\varphi_b)a^*\left(P_k P_{\stackrel{l}{+-}}\varphi_b\right)\\
&+\sum_{b\in\mathbb{N}}a^*\left(P_k P_{\stackrel{l}{-+}}\varphi_b\right)a(\varphi_b)-\sum_{-n\in\mathbb{N}}a^*\left(P_l P_{\stackrel{k}{+-}}\varphi_n\right) a(\varphi_n)\\
&+\sum_{-n\in\mathbb{N}}a(\varphi_n)a^*\left(P_l P_{\stackrel{k}{--}}\varphi_n\right) - \sum_{-b \in \mathbb{N}} a(\varphi_b)a^*\left(P_k P_{\stackrel{l}{--}}\varphi_b\right)\\
=&\sum_{n\in\mathbb{N}} a^*\left(P_k P_l \varphi_n \right) a(\varphi_n) - \sum_{n\in\mathbb{N}} a^*\left(P_l P_{\stackrel{k}{++}} \varphi_n \right) a(\varphi_n)\\
&+\tr \left( P_{\stackrel{l}{+-}} P_{\stackrel{k}{-+}}\right) - \sum_{n\in\mathbb{N}} a^*\left( P_l P_{\stackrel{k}{-+}} \varphi_n\right)a(\varphi_n)\\
&-\tr \left( P_{\stackrel{l}{-+}} P_{\stackrel{k}{+-}}\right) + \sum_{-b\in\mathbb{N}} a(\varphi_b) a^*\left(P_l P_{\stackrel{k}{+-}} \varphi_b\right)\\
&+\sum_{-b\in\mathbb{N}} a(\varphi_b) a^*\left( P_l P_{\stackrel{k}{--}}\varphi_b\right) - \sum_{-b\in\mathbb{N}} a(\varphi_b) a^*\left( P_k P_l \varphi_b\right)\\
&=\tr \left( P_{\stackrel{l}{+-}} P_{\stackrel{k}{-+}}\right)
-\tr \left( P_{\stackrel{l}{-+}} P_{\stackrel{k}{+-}}\right)\\
&+\sum_{n\in\mathbb{N}} a^*\left(\left[P_k ,P_l\right] \varphi_n \right) a(\varphi_n)
+\sum_{-b\in\mathbb{N}} a(\varphi_b)a^*\left(\left[P_l ,P_k\right] \varphi_b \right) \\
&=\tr \left( P_{\stackrel{l}{+-}} P_{\stackrel{k}{-+}}\right)
-\tr \left( P_{\stackrel{l}{-+}} P_{\stackrel{k}{+-}}\right)
+G\left(\left[P_k,P_l\right]\right)
\end{align*}
\qed

\begin{Def}
For \(k\in\mathbb{N}_0\), \(X,Y\in \mathcal{B}(\mathcal{H})\) the nested commutator \([X,Y]_k\) is defined inductively as
\begin{align*}
[X,Y]_0&:= Y\\
[X,Y]_{k+1}&:=[X,[X,Y]_{k}] \quad \forall k\in\mathbb{N}_0.
\end{align*}
\end{Def}

\begin{Lemma}\label{nested_kommuted_G}
For \(m\in\mathbb{N}\) and \(B,C \in Q\) the following holds
\begin{multline}
\left[ G(B),G(C)\right]_m=  \tr\left(P_-BP_+[B,C]_{m-1}\right) - \tr\left(P_+BP_-[B,C]_{m-1}\right)\\
+G\left([B,C]_m\right) .
\end{multline}
\end{Lemma}
\textbf{Proof:} Proof by Induction is the first thing that comes to mind, looking at the claim. Indeed, \(m=1\) is the consequence
of the lemma \ref{G_kommutator}. For \(m\) general we have

\begin{multline}
\left[ G(B),G(C)\right]_{m+1}
=\left[ G(B),\left[ G(B),G(C)\right]_{m}\right]\\
\stackrel{\text{ind.hyp.}}{=}\left[ G(B), \tr\left(P_-BP_+[B,C]_{m-1}\right) - \tr\left(P_+BP_-[B,C]_{m-1}\right)
+G\left([B,C]_m\right) \right]\\
=\left[ G(B),G\left([B,C]_m\right) \right]\\
\stackrel{\text{lemma } \ref{G_kommutator}}{=} 
\tr\left(P_- B P_+ [B,C]_m\right)
-\tr\left(P_+BP_- [B,C]_m \right) 
+G\left(\left[B,[B,C]_m\right]\right)\\
=
\tr\left(P_- B P_+ [B,C]_m\right)
-\tr\left(P_+BP_- [B,C]_m \right) 
+G\left([B,C]_{m+1}\right)
\end{multline}
\qed 

\begin{Lemma}\label{lemma:derivativeJ}
For external potentials \(A, X\) small enough the derivatives of the scattering operator can be computed to fulfil
\begin{align}\label{eq:derivativeS1}
\partial_\varepsilon |_{\varepsilon=0} e^{G \ln U^{A+\varepsilon X}} &= e^{G \ln U^A} j_A^0(X) + e^{G \ln U^A} G( (U^A)^{-1} \partial_{\varepsilon} U^{A+\varepsilon X})\\
\partial_\varepsilon |_{\varepsilon=0} e^{-G \ln U^{A+\varepsilon X}} &=- e^{-G \ln U^A} j_A^0(X) + G( \partial_{\varepsilon} \left(U^{A+\varepsilon X}\right)^{-1} U^A ) e^{-G \ln U^A},
\end{align}
with 
\begin{multline}\label{def:vacuumExpectationCurrent}
j^0_A(X):= \sum_{l\in\mathbb{N}_0} \frac{(-1)^{l+1}}{(l+2)!} \left(\tr P_- \ln U^A P_+ \left[ \ln U^A, \partial_\varepsilon \ln U^{A+\varepsilon X}\right]_{l}\right.\\
\left. - \tr P_+ \ln U^A P_- \left[ \ln U^A, \partial_\varepsilon \ln U^{A+\varepsilon X}\right]_{l} \right).
\end{multline}
\end{Lemma}
\textbf{Proof:} We start out by employing Duhamel's and Hadamard's formulas. \todo{ref!! + restrictions, something better than 
 \href{https://s3.amazonaws.com/academia.edu.documents/46627661/CBH.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1517999839&Signature=m\%2FGNTMS\%2FNAH7ioWII76ZlULcCRY\%3D&response-content-disposition=inline\%3B\%20filename\%3DCrib_Notes_on_Campbell-Baker-Hausdorff_e.pdf}{this}}
These are

\begin{equation}\label{Duhamel}
\partial_{\alpha}\left. e^{Y+\alpha X}\right|_{\alpha=0} = \int_0^1 d t e^{(1-t) Y} X e^{t Y}\tag{Duhamel's formula}
\end{equation}

and
\begin{equation}\tag{Hadamard's formula}\label{Hadamard}
e^{X}Ye^{-X}=\sum_{k=0}^\infty \frac{1}{k!} [X,Y]_k.
\end{equation}

So one gets

\begin{align}\label{proof:duhamelApplied}
\partial_\varepsilon |_{\varepsilon=0} e^{G \ln U^{A+\varepsilon X}} &= \int_0^1 d z e^{(1-z) G \ln U^A} \partial_\varepsilon |_{\varepsilon=0} G \ln U^{A+\varepsilon X} e^{z G \ln U^A}\\\notag
&=e^{G \ln U^A} \int_0^1 d z \sum_{l\in\mathbb{N}_0} \frac{1}{l!} \left[ - z G \ln U^A, \partial_\varepsilon |_{\varepsilon=0} G \ln U^{A+\varepsilon X} \right]_l\\\notag
&=e^{G \ln U^A} \int_0^1 d z \sum_{l\in\mathbb{N}_0} \frac{(-z)^l}{l!} \partial_\varepsilon |_{\varepsilon=0} \left[  G \ln U^A,  G \ln U^{A+\varepsilon X} \right]_l.
\end{align}
At this point we see that for \(l=0\) the summand vanishes. For all other values of \(l\) we use lemma \ref{nested_kommuted_G}, yielding
\begin{multline}
\partial_\varepsilon |_{\varepsilon=0} e^{G \ln U^{A+\varepsilon X}}= 
e^{G \ln U^A} \int_0^1 d z \sum_{l\in\mathbb{N}} \frac{(-z)^l}{l!} \partial_\varepsilon |_{\varepsilon=0} \left( G\left(\left[ \ln U^A, \ln U^{A+\varepsilon X}\right]\right) \right.\\
+\tr P_- \ln U^A P_+ \left[ \ln U^A, \partial_\varepsilon |_{\varepsilon=0} \ln U^{A+\varepsilon X}\right]_{l-1}\\
\left. - \tr P_+ \ln U^A P_- \left[ \ln U^A, \partial_\varepsilon |_{\varepsilon=0} \ln U^{A+\varepsilon X}\right]_{l-1}\right).
\end{multline}

The last two terms together result in the first term of \eqref{eq:derivativeS1} after performing the integration and shifting the summation
index. For the first term we will use linearity and continuity of \(G\)\todo{contiuity of G!!}   ~and use the same identities backwards to give

\begin{multline}
e^{G \ln U^A} \int_0^1 d z \sum_{l\in\mathbb{N}} \frac{(-z)^l}{l!} \partial_\varepsilon |_{\varepsilon=0} G\left(\left[ \ln U^A, \ln U^{A+\varepsilon X}\right]\right) \\
=e^{G \ln U^A}  G\left( \int_0^1 d z \sum_{l\in\mathbb{N}} \frac{1}{l!}  \left[ -z \ln U^A,\partial_\varepsilon|_{\varepsilon=0} \ln U^{A+\varepsilon X}\right]\right) \\
=e^{G \ln U^A}  G\left(e^{- \ln U^A} \int_0^1 d z e^{ \ln U^A} e^{-z \ln U^A}\partial_\varepsilon|_{\varepsilon=0} \ln U^{A+\varepsilon X} e^{z \ln U^A}\right) \\
=e^{G \ln U^A}  G\left(e^{ \ln \left(U^A\right)^{-1}} \partial_\varepsilon|_{\varepsilon=0} e^{ \ln U^{A+\varepsilon X}} \right) \\
=e^{G \ln U^A}  G\left(\left(U^A\right)^{-1} \partial_\varepsilon|_{\varepsilon=0}U^{A+\varepsilon X} \right).
\end{multline}
Putting things together results in the first equality we wanted to prove. For the second one the computation is completely analogous, except for after applying Duhamel's formula as in \eqref{proof:duhamelApplied} we substitute \(u=1-z\). The minus sign in front of the first term then arises by the chain rule, where as the second term does not share the sign change with the first, since we have to revert the use of the chain rule in the second half of the calculation when we apply  \eqref{Duhamel} backwards. \qed

\begin{Def}
We use Bogoliubov's formula to define the vacuum expectation value of the current \todo{ref!!}
\begin{equation}
j_A(F) = i \partial_{\varepsilon}\left. \langle \Omega, {S^{A} }^* S^{A+\varepsilon F}\Omega \rangle \right|_{\varepsilon=0}.
\end{equation}
\end{Def}



\begin{Thm}\label{thm:CurrentExact}
The vacuum expectation value of the current of the scattering operator takes the form
\begin{align*}
j_A(F)&=- \partial_{\varepsilon} \left. \varphi(A+\varepsilon F) \right|_{\varepsilon=0}\\
-2\int_0^1 &d z (1-z)  \Im \tr\left(P_+ \ln U^A P_- e^{ -z \ln U^A} \partial_{\varepsilon} \left. \ln U^{A+\varepsilon F}\right|_{\varepsilon=0} e^{ z \ln U^A}\right)\\
&=- \partial_{\varepsilon} \left.\varphi(A+\varepsilon F) \right|_{\varepsilon=0}\\
  - 2\Im \sum_{k=0}^\infty&\frac{(-1)^k}{(k+2)!}  \tr \left( P_+ \ln U^A P_-\left[\ln U^A,\left.\partial_{\varepsilon}\ln U^{A+\varepsilon F} \right|_{\varepsilon=0}\right]_{k}\right) 
\end{align*}
\end{Thm}
\textbf{Proof:} By theorem \ref{sleek_second_quantised_scattering_operator} and abbreviating
\(\varphi(A)= \sum_{n\in\mathbb{N}} \frac{C_n(A)}{n!} \)we see that the current can be written in the form

\begin{multline}\label{sleek_current_calc1}
j_A(F) = i \partial_{\varepsilon}\left. \langle \Omega, {S^{A} }^* S^{A+\varepsilon F}\Omega \rangle \right|_{\varepsilon=0}\\
=i \partial_{\varepsilon}\left. \langle \Omega, e^{-i\varphi(A)} e^{-G(\ln (U^A))} 
e^{i\varphi(A+\varepsilon F)}  e^{G(\ln (U^{A+\varepsilon F}))} \Omega\rangle \right|_{\varepsilon=0}\\
=- \partial_{\varepsilon} \left. \varphi(A+\varepsilon F) \right|_{\varepsilon=0}
+i\langle \Omega,\partial_{\varepsilon}\left.  e^{-G(\ln (U^A))} 
e^{G(\ln (U^{A+\varepsilon F}))} \Omega\rangle \right|_{\varepsilon=0},
\end{multline}
so the first summand works out just as claimed. For the second summand we employ lemma \ref{lemma:derivativeJ} and note that the vacuum expectation value of \(G\) vanishes no matter its argument.

\begin{multline}\label{sleek_current_calc3}
i\langle \Omega,\partial_{\varepsilon}\left.  e^{-G(\ln (U^A))} 
e^{G(\ln (U^{A+\varepsilon F}))} \Omega\rangle \right|_{\varepsilon=0}\\
=-i  \partial_{\varepsilon} \left.\sum_{k=0}^\infty \frac{(-1)^k}{(k+2)!} 
 \tr\left(P_- \ln U^A P_+[\ln U^A ,\ln U^{A+\varepsilon F} ]_{k}\right) \right|_{\varepsilon=0}\\
 +i  \partial_{\varepsilon} \left.\sum_{k=0}^\infty \frac{(-1)^k}{(k+2)!}  \tr\left(P_+ \ln U^A P_-[\ln U^A,\ln U^{A+\varepsilon F} ]_{k}\right) \right|_{\varepsilon=0}\\ 
\end{multline}

In order to apply Hadamard's formula once again in the opposite direction, we introduce two auxiliary integrals.
The second term then becomes

\begin{multline}\label{sleek_current_calc4}
 i  \partial_{\varepsilon} \left.\sum_{k=0}^\infty \frac{(-1)^k}{(k+2)!}  \tr\left(P_+ \ln U^A P_-[\ln U^A,\ln U^{A+\varepsilon F} ]_{k}\right) \right|_{\varepsilon=0}\\
= i  \partial_{\varepsilon} \left.\sum_{k=0}^\infty \frac{(-1)^k}{k!} \int_0^1 d t \int_0^1 s^k t^{k+1}
\tr\left(P_+ \ln U^A P_-[\ln U^A,\ln U^{A+\varepsilon F} ]_{k}\right) \right|_{\varepsilon=0}\\
= i  \partial_{\varepsilon} \left.\sum_{k=0}^\infty \frac{1}{k!} \int_0^1 d t \int_0^1 ds\ t
\tr\left(P_+ \ln U^A P_-[ -t s \ln U^A,\ln U^{A+\varepsilon F} ]_{k}\right) \right|_{\varepsilon=0}\\
= i  \partial_{\varepsilon} \left.\sum_{k=0}^\infty \frac{1}{k!} \int_0^1 d z \int_z^1 ds\ 
\tr\left(P_+ \ln U^A P_-[ -z \ln U^A,\ln U^{A+\varepsilon F} ]_{k}\right) \right|_{\varepsilon=0}\\
= i  \partial_{\varepsilon} \left.\sum_{k=0}^\infty \frac{1}{k!} \int_0^1 d z (1-z)
\tr\left(P_+ \ln U^A P_-[ -z \ln U^A,\ln U^{A+\varepsilon F} ]_{k}\right) \right|_{\varepsilon=0}\\
= i  \partial_{\varepsilon} \left. \int_0^1 d z (1-z) 
\tr\left(P_+ \ln U^A P_- \sum_{k=0}^\infty \frac{1}{k!} [ -z \ln U^A,\ln U^{A+\varepsilon F} ]_{k}\right) \right|_{\varepsilon=0}\\
\stackrel{\eqref{Hadamard}}{=} i  \partial_{\varepsilon} \left. \int_0^1 d z (1-z) 
\tr\left(P_+ \ln U^A P_- e^{ -z \ln U^A}\ln U^{A+\varepsilon F} e^{ z \ln U^A}\right) \right|_{\varepsilon=0}\\
=i  \int_0^1 d z (1-z) 
\tr\left(P_+ \ln U^A P_- e^{ -z \ln U^A} \partial_{\varepsilon} \left. \ln U^{A+\varepsilon F}\right|_{\varepsilon=0} e^{ z \ln U^A}\right) .
\end{multline}

The calculation for the first term of \eqref{sleek_current_calc3} is identical.
At this point we notice that \eqref{sleek_current_calc4} and the term where the 
projectors are exchanged are complex conjugates of one another. So summarising we find
\begin{multline*}
j_A(F)=- \partial_{\varepsilon} \left. \varphi (A+\varepsilon F) \right|_{\varepsilon=0}\\
-2\int_0^1 d z (1-z)  \Im \tr\left(P_+ \ln U^A P_- e^{ -z \ln U^A} \partial_{\varepsilon} \left. \ln U^{A+\varepsilon F}\right|_{\varepsilon=0} e^{ z \ln U^A}\right).
\end{multline*}
\qed

\begin{Thm}
Independent of the phase that is used to correct the scattering operator the following formula holds true for any 
 four potentials \(A,F,H\), with \(A\) small enough so that the relevant series converge.
\begin{multline}
\partial_\varepsilon |_{\varepsilon=0} ( j_{A+\varepsilon H} (F)-j_{A+\varepsilon F}(H)) = \\
2 \Im \tr \left(P_+ \left(U^A\right)^{-1} \partial_{\varepsilon}|_{\varepsilon=0} U^{A+\varepsilon F} P_-\left(U^A\right)^{-1} \partial_{\delta}|_{\delta=0} U^{A+\delta H}\right)
\end{multline}
\end{Thm}
\textbf{Proof:} We compute \(\partial_\varepsilon |_{\varepsilon=0} j_{A+\varepsilon F}(H) \). 

\begin{align*}
&-i\partial_\varepsilon |_{\varepsilon=0} j_{A+\varepsilon H}(F)=\\
&\partial_\varepsilon |_{\varepsilon=0} \partial_\delta |_{\delta=0} \langle \Omega, e^{i\varphi(A+\varepsilon H + \delta F)-i\varphi(A+\varepsilon H)} e^{-G \ln U^{A+\varepsilon H}} e^{G \ln U^{A+\varepsilon H + \delta F}}\Omega \rangle\\
\end{align*}

We first act with the derivative with respect to \(H\), fixing \(F\).

\begin{align*}
&-i\partial_\varepsilon |_{\varepsilon=0} j_{A+\varepsilon H}(F)=\\
&\partial_\delta |_{\delta=0}   i(\partial_\varepsilon |_{\varepsilon=0}\varphi(A+\varepsilon H + \delta F)-\partial_\varepsilon |_{\varepsilon=0}\varphi(A+\varepsilon H ))e^{i\varphi(A + \delta F)-i\varphi(A)} \\
&\langle \Omega, e^{-G \ln U^{A}} e^{G \ln U^{A + \delta F}}\Omega \rangle\\
&+\partial_\delta |_{\delta=0} e^{i\varphi(A + \delta F)-i\varphi(A)} \langle \Omega
\partial_\varepsilon |_{\varepsilon=0} e^{-G \ln U^{A+\varepsilon H}} e^{G \ln U^{A + \delta F}}\Omega \rangle\\
&+\partial_\delta |_{\delta=0} e^{i\varphi(A + \delta F)-i\varphi(A)} 
 \langle \Omega e^{-G \ln U^{A}} \partial_\varepsilon |_{\varepsilon=0}e^{G \ln U^{A+\varepsilon H + \delta F}}\Omega \rangle
\end{align*}

In computing further one can notice a few cancellations. For the first summand the first factor vanishes if \(\delta\) is set to zero, so the only 
the first summand in the product rule will not vanish. For the second and third summand we will use lemma \ref{lemma:derivativeJ},
giving

\begin{align*}
&-i\partial_\varepsilon |_{\varepsilon=0} j_{A+\varepsilon H}(F)=\\
&\partial_\delta |_{\delta=0}   i\partial_\varepsilon |_{\varepsilon=0}\varphi(A+\varepsilon H + \delta F)\\
&-\partial_\delta |_{\delta=0} e^{i\varphi(A + \delta F)-i\varphi(A)}  j^0_{A}(H) 
\langle \Omega, e^{-G \ln U^{A}} e^{G \ln U^{A + \delta F}}\Omega \rangle \\
&+\partial_\delta |_{\delta=0} e^{i\varphi(A + \delta F)-i\varphi(A)} \langle \Omega,
 G \left(\partial_\varepsilon |_{\varepsilon=0} \left(U^{A+\varepsilon H}\right)^{-1} U^A\right) e^{-G \ln U^{A}} e^{G \ln U^{A + \delta F}}  \Omega \rangle\\
&+\partial_\delta |_{\delta=0} e^{i\varphi(A + \delta F)-i\varphi(A)} j^0_{A+\delta F}(H)
\langle \Omega, e^{-G \ln U^{A}} e^{G \ln U^{A+ \delta F}}\Omega \rangle\\
&+\partial_\delta |_{\delta=0} e^{i\varphi(A + \delta F)-i\varphi(A)} 
\langle \Omega, e^{-G \ln U^{A}} e^{G \ln U^{A+ \delta F}}G\left( \left(U^{A+\delta F}\right)^{-1}\partial_\varepsilon |_{\varepsilon=0} U^{A+\varepsilon H + \delta F} \right) \Omega \rangle.
\end{align*}

Now there are a few further simplifications to appreciate: since \(\langle \Omega, G \Omega \rangle=0\), in the third and last summand only the derivatives with respect to \(\delta\) which produce by lemma \ref{lemma:derivativeJ} another factor of \(G\)  will contribute to the sum.
 For the other summands except for the first we can spot the appearance of \(j^0\). Respecting all this results in

\begin{align*}
&-i\partial_\varepsilon |_{\varepsilon=0} j_{A+\varepsilon H}(F)=
i\partial_\delta |_{\delta=0}   \partial_\varepsilon |_{\varepsilon=0}\varphi(A+\varepsilon H + \delta F)\\
&-i\partial_\delta |_{\delta=0}\varphi(A + \delta F)j^0_{A}(H) -  j^0_{A}(H) j^0_A(F)\\
&+\langle \Omega,
 G \left(\partial_\varepsilon |_{\varepsilon=0} \left(U^{A+\varepsilon H}\right)^{-1} U^A\right) G\left( \left(U^{A}\right)^{-1}\partial_\delta |_{\delta=0} U^{A +\delta F} \right)  \Omega \rangle\\
&+i\partial_\delta |_{\delta=0} \varphi(A + \delta F) j^0_{A}(H)
+\partial_\delta |_{\delta=0}  j^0_{A+\delta F}(H)
+ j^0_{A}(H)j^0_A(F)\\
&+\langle \Omega, G\left( \left(U^{A}\right)^{-1}\partial_\delta |_{\delta=0} U^{A + \delta F} \right) G\left( \left(U^{A}\right)^{-1}\partial_\varepsilon |_{\varepsilon=0} U^{A+\varepsilon H} \right) \Omega \rangle.
\end{align*}

A few more terms cancel in the second and fourth line, also since \(\partial_\varepsilon |_{\varepsilon=0} \left(U^{A+\varepsilon H}\right)^{-1} U^{A+\varepsilon H}=0\) we can combine the two products of \(G\) into a commutator:

\begin{align*}
&-i\partial_\varepsilon |_{\varepsilon=0} j_{A+\varepsilon H}(F)=
i\partial_\delta |_{\delta=0}   \partial_\varepsilon |_{\varepsilon=0}\varphi(A+\varepsilon H + \delta F)\\
&+\partial_\delta |_{\delta=0}  j^0_{A+\delta F}(H)\\
&+\langle \Omega, \left[G\left( \left(U^{A}\right)^{-1}\partial_\delta |_{\delta=0} U^{A + \delta F} \right), G\left( \left(U^{A}\right)^{-1}\partial_\varepsilon |_{\varepsilon=0} U^{A+\varepsilon H} \right)\right] \Omega \rangle.
\end{align*}

So we can once again apply lemma \ref{G_kommutator}, which results in exactly right hand side of the equation
we claimed to produce in the statement of this theorem. So all that is left is to recognise that one can combine
the first two summands into \(-i \partial_{\varepsilon} j_{A+\varepsilon H} (F)\), which is a direct consequence
of theorem \ref{thm:CurrentExact}.\qed

\subsection{Quantitative Estimates}
\noch{probably this part cannot be made rigorous. Decide whether to keep it as heuristics}
Since we do not only want to give an expression for the time evolution operator, but also give bounds on the numerical errors
which are due to truncate the occurring series we need to look at these series a little closer. The series involve powers of the 
second quantisation operator \(G\), so we start by examining these in greater depth. In order to do so we define an object 
closely related to \(G\).

\begin{Def}
\begin{multline}\label{def:L}
L: \{M\subset B(\mathcal{H})\mid |M|<\infty\} \times \{M\subset B(\mathcal{H})\mid |M|<\infty\} \rightarrow B(\mathcal{F})\\
L(\{A_1,\dots, A_c\},\{ B_1,\dots ,B_m\}):= \prod_{l=1}^m a(\varphi_{-k_l}) \\
\prod_{l=1}^c a^*(A_l \varphi_{n_l}) \prod_{l=1}^m a^*(B_l \varphi_{-k_l}) \prod_{l=1}^c a(\varphi_{n_l}),
\end{multline}
where for notational reasons we chose to list the occurring one-particle operators in a specific order; however, 
the order does not matter, since commutation of the relevant creation and annihilation operators always results an 
overall factor of one.
\end{Def}

Since this operator \(L\) occurs when computing powers of \(G\) we compute is product with some \(G\) with 
the following 

\begin{Lemma}\label{lem:Ntimes1} For any \(a,b,\in\mathbb{N}_0\) and apropriate one particle operators \(A_k, B_l, C\) for \(1\le k\le a\), \(1\le l\le b\) we have the following equality
\begin{align}
&L\Big(\bigcup_{l=1}^a \{A_l\}; \bigcup_{l=1}^b \{B_l\}\Big)G(C) =\\\label{Ntimes1:simplyAdd1}
 &(-1)^{a+b} L\Big(\bigcup_{l=1}^a \{A_l\}\cup \{C\}; \bigcup_{l=1}^b \{B_l\}\Big) \\\label{Ntimes1:simplyAdd2}
 &+(-1)^{a+b+1} L\Big(\bigcup_{l=1}^a \{A_l\}; \bigcup_{l=1}^b \{B_l\}\cup \{C\} \Big) \\\label{Ntimes1:n+1atEnd1}
&+ \sum_{f=1}^a  L\Big(\bigcup_{\stackrel{l=1}{l\neq f}}^a \{A_l\}\cup \{A_fP_+C \}; \bigcup_{l=1}^b \{B_l\}\Big) \\\label{Ntimes1:n+1atBeginning1}
&+\sum_{f=1}^a
 L\Big(\bigcup_{\stackrel{l=1}{f\neq l}}^a \{A_l\}\cup \{- CP_- A_f\}; \bigcup_{l=1}^b \{B_l\}\Big)\\\label{Ntimes1:n+1atEnd2}
& -\sum_{f=1}^a L\Big(\bigcup_{\stackrel{l=1}{f\neq l}}^a \{A_l\}; \bigcup_{l=1}^b \{B_l\}\cup \{A_f P_+ C\}\Big)\\\label{Ntimes1:n+1atBeginning2}
&+ \sum_{f=1}^b L\Big(\bigcup_{l=1}^a \{A_l\}; \bigcup_{\stackrel{l=1}{l\neq f}}^b \{B_l\}\cup \{-CP_- B_f\}\Big)\\\label{Ntimes1:TrTerm}
&+ (-1)^{a+b+1} \sum_{f=1}^a \tr \Big(P_+ C P_- A_f\Big) L\Big(\bigcup_{\stackrel{l=1}{l \neq f}}^a \{A_l\}; \bigcup_{l=1}^b \{B_l\}\Big)\\\label{Ntimes1:middle1}
&+ (-1)^{a+b+1} \sum_{\stackrel{f_1,f_2=1}{f_1\neq f_2}}^a L\Big(\bigcup_{\stackrel{l=1}{l\neq f_1,f_2}}^a \{A_l\}\cup \{-A_{f_2} P_+ C P_- A_{f_1}\}; \bigcup_{l=1}^b \{B_l\}\Big)\\\label{Ntimes1:middle2}
&+(-1)^{a+b+1} \sum_{f=1}^b \sum_{g=1}^a L\Big(\bigcup_{\stackrel{l=1}{l\neq g}}^a \{A_l\}; \bigcup_{\stackrel{l=1}{l\neq f}}^b \{B_l\}\cup \{-A_g P_+ C P_- B_f\}\Big).
\end{align}
\end{Lemma}
{\bfseries Proof:}  The proof of this equality is a rather long calculation, where \eqref{def:L} is used repeatedly. We break up the calculation into 
several parts. Let us start with

\begin{multline}
L\left(\bigcup_{l=1}^a \{A_l\}; \bigcup_{l=1}^b \{B_l\}\right) L(C;)=\\
\prod_{l=1}^b a(\varphi_{-k_l}) \prod_{l=1}^a a^*(A_l \varphi_{n_l}) \prod_{l=1}^b a^*(B_l \varphi_{-k_l}) \prod_{l=1}^a a(\varphi_{n_l}) a^*(C \varphi_m) a(\varphi_m).
\end{multline}

We (anti)commute the creation operator involving \(C\) to its place at the end of the second product, after that the term
will be normally ordered and can be rephrased in terms of \(L\)s. During the commutation the creation operator
in question can be picked up by any of the annihilation operators in the rightmost product. For each term where that happens
we can perform the sum over the basis of \(\mathcal{H}^-\) related to the annihilation operator whose anticommutator triggered.
After this sum the corresponding term is also normally ordered and can be rephrased in terms of an \(L\) after some 
reshuffling which may only produce signs. So performing these steps we get

\begin{multline}
L\left(\bigcup_{l=1}^a \{A_l\}; \bigcup_{l=1}^b \{B_l\}\right) L(C;)=\\
\sum_{f=a}^1 (-1)^{a-f} \prod_{l=1}^b a(\varphi_{-k_l}) \prod_{l=1}^{f-1} a^*(A_l \varphi_{n_l}) a^*(A_fP_+C\varphi_m)\\
 \prod_{l=f+1}^a a^*(A_l \varphi_{n_l}) \prod_{l=1}^b a^*(B_l \varphi_{-k_l}) \prod_{\stackrel{l=1}{l\neq f}}^a a(\varphi_{n_l}) a(\varphi_m)\\
+ L\left(\bigcup_{l=1}^a \{A_l\} \cup \{C\}; \bigcup_{l=1}^b \{B_l\}\right)\\
=\sum_{f=1}^a  L\left(\bigcup_{\stackrel{l=1}{l \neq f}}^a \{A_l\} \cup \{A_f P_+ C\}; \bigcup_{l=1}^b \{B_l\}\right)\\
+ L\left(\bigcup_{l=1}^a \{A_l\} \cup \{C\}; \bigcup_{l=1}^b \{B_l\}\right).
\end{multline}

Now the remaining case is more laborious, that is why we will split off and treat some of the appearing terms separately. 
We start off analogous to before 

\begin{multline}
L\left(\bigcup_{l=1}^a \{A_l\}; \bigcup_{l=1}^b \{B_l\}\right) L(;C)=\\
\prod_{l=1}^b a(\varphi_{-k_l}) \prod_{l=1}^a a^*(A_l \varphi_{n_l}) \prod_{l=1}^b a^*(B_l \varphi_{-k_l}) \prod_{l=1}^a a(\varphi_{n_l})a(\varphi_{-m}) a^*(C \varphi_{-m}).
\end{multline}

This time we need to (anti)commute the rightmost annihilation operator all the way to the end of the first product and the creation operator to the end of the second but last product. So there will be several qualitatively different terms. From the first step alone we get
\begin{align}\notag
L\left(\bigcup_{l=1}^a \{A_l\}; \bigcup_{l=1}^b \{B_l\}\right) L(;C)&=\\\notag
(-1)^{a}\sum_{f=b}^1 (-1)^{b-f} \prod_{l=1}^b a(\varphi_{-k_l}) &\prod_{l=1}^a a^*(A_l \varphi_{n_l}) \prod_{\stackrel{l=1}{l\neq f}}^b a^*(B_l \varphi_{-k_l})\\ \label{Ntimes1 term 1}
& \prod_{l=1}^a a(\varphi_{n_l}) a^*(C P_- B_f \varphi_{-k_f})\\\notag
+(-1)^{a+b}\sum_{f=a}^1 (-1)^{b-f} \prod_{l=1}^b a(\varphi_{-k_l}) &\prod_{\stackrel{l=1}{l\neq f}}^a a^*(A_l \varphi_{n_l})\\\label{Ntimes1 term 2}
& \prod_{l=1}^b a^*(B_l \varphi_{-k_l}) \prod_{l=1}^a a(\varphi_{n_l}) a^*(CP_- \varphi_{n_f})\\\notag
+(-1)^b\prod_{l=1}^b a(\varphi_{-k_l}) a(\varphi_{-m}) &\prod_{l=1}^a a^*(A_l \varphi_{n_l}) \\ \label{Ntimes1 term 3}
&\prod_{l=1}^b a^*(B_l \varphi_{-k_l}) \prod_{l=1}^a a(\varphi_{n_l})a^*(C \varphi_{-m}).
\end{align}

We will discuss terms \eqref{Ntimes1 term 1}, \eqref{Ntimes1 term 2} and \eqref{Ntimes1 term 3} separately. 
In Term \eqref{Ntimes1 term 1} we need to commute the last creation operator into its place in the third product,
it can be picked up by one of the annihilation operators of the last product, but after performing the sum over
the corresponding basis the resulting term can be rephrased in terms of an \(L\) operator by commuting
only creation operators of the second and third product. Performing these steps yields the identity

\begin{multline}
\eqref{Ntimes1 term 1}=\sum_{f=1}^b L\left( \bigcup_{l=1}^a \{A_l\}; \bigcup_{\stackrel{l=1}{l\neq f}}^b \{B_l\}\cup  \{CP_- B_f \}\right)\\
+(-1)^{a+b+1}\sum_{f=1}^b \sum_{g=1}^a L\left( \bigcup_{\stackrel{l=1}{l\neq g}}^a \{A_l\}; \bigcup_{\stackrel{l=1}{l\neq f}}^b \{B_l\} \{A_gP_+CP_- B_f \}\right).
\end{multline}

For \eqref{Ntimes1 term 2} the last creation operator needs to be commuted to the end of the second product. It can be picked up by 
one of the annihilation operators of the last product, but here we have to distinguish between two cases. If the index of this
annihilation operator equals \(f\) the resulting commutator will be \(\tr P_+ C P_- A_f \) otherwise one can again perform the sum
over the corresponding index and express the whole Product in terms of an \(L\) operator. All this results in 

\begin{multline}
\eqref{Ntimes1 term 2}=\sum_{f=1}^a L\left( \bigcup_{\stackrel{l=1}{l\neq f}}^a \{A_l\}\cup \{C P_- A_f\}; \bigcup_{l=1}^b \{B_l\}\right)\\
+(-1)^{a+b}\sum_{f=1}^a  L\left( \bigcup_{\stackrel{l=1}{l\neq f}}^a \{A_l\}; \bigcup_{l=1}^b \{B_l\} \right) \tr (P_+ C P_- A_f )\\
+(-1)^{a+b+1}\sum_{\stackrel{f_1,f_2=1}{f_1\neq f_2}}^a L\left( \bigcup_{\stackrel{l=1}{l\neq f_1,f_2}}^a \{A_l\} \cup \{A_{f_2} P_+CP_- A_{f_1} \}; \bigcup_{l=1}^b \{B_l\} \right).
\end{multline}

For \eqref{Ntimes1 term 3} the procedure is basically the same as for \eqref{Ntimes1 term 1}, it results in

\begin{multline}
\eqref{Ntimes1 term 3}= (-1)^{a+b} L\left( \bigcup_{l=1}^a \{A_l\}; \bigcup_{l=1}^b \{B_l\}\right)\\
+ \sum_{f=1}^a L\left( \bigcup_{\stackrel{l=1}{l \neq f}}^a \{A_l\}\cup \{C P_- A_f\}; \bigcup_{l=1}^b \{B_l\}\cup \{A_f P_+ C\}\right).
\end{multline}

Putting the results of the calculation together results in the claimed equation, after pulling in some factors of \(-1\) into \(L\). \qed

We carry on with defining the important quantities for powers of \(G\). First we introduce for each \(k \in \mathbb{N}\) a linear
bounded operator on \(\mathcal{H}\), \(X_k\) which fulfils \(\tr P_+ X_k P_- X_k<\infty\wedge \tr P_- X_k P_+ X_k<\infty\). 

\begin{Def}
Let 
\begin{equation*}
Y:=\{X_k\mid k\in\mathbb{N}\}.
\end{equation*}
Let for \(n\in\mathbb{N}\)
\begin{equation*}
\langle n\rangle := \{X_l\mid l\in\mathbb{N}, l\le n\}.
\end{equation*}
\end{Def}

\begin{Def}
Let for  \(b\subset Y\), such that \(|b|<\infty\)
\begin{align}\notag
f_b:\{l\in\mathbb{N}\mid l\le |b|\}\rightarrow b\\
\forall k<|b| : f_b(k)=X_l\wedge f_b(k+1)=X_m\rightarrow l<m
\end{align}
\end{Def}

\begin{Def}
For any set \(b\), we denote by \(S(b)\) the symmetric group (group of permutations) over \(b\).
\end{Def}

\begin{Def}
Let for  \(b\subset Y\), such that \(|b|<\infty\) and \(\sigma_b \in S(b)\)
\begin{align*}
\text{VZ}^b_{\sigma_b}: \{k\in\mathbb{N}\mid k<|b|\} \rightarrow \{-1,1\}\\
\text{VZ}^b_{\sigma_b}(k):=\text{sgn}\big[f_b^{-1}(\sigma_b(f_b(k+1))) - f_b^{-1}(\sigma_b(f_b(k)))  \big]
\end{align*}
\end{Def}

In what is to follow the order of one particle operators will be changed in all possible ways, to keep track of this by
use of a compact notation we introduce
\begin{Def}
\begin{align*}
W: \{(b,\sigma_b) \mid b\subseteq Y \wedge |b|<\infty \wedge \sigma_b \in S(b) \} \rightarrow B(\mathcal{H})\\
W(b,\sigma_b):= \left( \prod_{k=1}^{|b|-1} \sigma_{b}(f_b(k)) P_{\text{VZ}_{\sigma_b}^b(k)} \text{VZ}_{\sigma_b}^b(k) \right) \sigma_b (f_b(|b|))
\end{align*}
\end{Def}

\begin{Def} Let \(l\) be any finite subset of \(Y\). Denote by \(X^l_{\text{max}}\) the operator \(X_k\in l \)
such that for any \(X_c\in l \) the relation \(k\ge c\) is fulfilled. Furthermore define
\begin{align*}
&\text{PT}: \{T\subset \mathcal{P}(Y)\mid |T|<\infty, \forall b\in T: |b|<\infty\}\rightarrow \mathbb{C}\\
&\text{for: } T=\emptyset: \text{PT}(T)=1, \text{ otherwise: }\\
&\text{PT}(T)=\sum_{\stackrel{\forall l \in T:}{\sigma_l \in S(l \backslash \{X_{\text{max}}^l\})}} \prod_{l\in T} 
\tr [P_+ X^l_{\text{max}} P_- W(l, \sigma_l)]
\end{align*}
\end{Def}

There is one more function left to define
\begin{Def}
\begin{align*}
\text{Op}:  \{R\in \mathcal{P}(Y)\mid |R|<\infty\} \times \{D\subset \mathcal{P}(Y)\mid |T|<\infty\}\rightarrow \mathcal{B}(\mathcal{F})\\
\text{Op}(R,D)=\sum_{\stackrel{\forall l \in D:}{\sigma_l \in S(l)}} \sum_{a \subseteq R \cup \bigcup_{l\in D} \{W(l,\sigma_l)\}} L(a,a^c)(-1)^{|a|+  \frac{(|R| + |D|)(|R|+|D|+1)}{2}}
\end{align*}
\end{Def}

Now we are able to state the main theorem which will help us do quantitative estimates.

\begin{Thm}
Let \(n\in\mathbb{N}\), \(X_1,\dots, X_n \in Y\) then the following equation holds

\begin{equation}\label{eq:ProductG}
\prod_{k=1}^n G(X_k)= 
\sum_{\stackrel{\langle n \rangle = \dot{\Cup}_{l\in T} l \dot{\cup} \dot{\Cup}_{l\in D} l \dot{\cup} R}{ \forall l \in T \cup D: |l|\ge 2}} \text{PT}(T) \text{Op}(R,D),
\end{equation}
where the abbreviation \(\langle n\rangle:= \{X_k\mid k\le n\}\) was used.
\end{Thm}
{\bfseries Proof:} The proof will be by induction on \(n\). Since the formula in the claim reduces to 1 for \(n=0\) we will
not spend any more time on the start of the induction. The general strategy of the proof is to break up the right 
hand side of \eqref{eq:ProductG} for \(n+1\) into small pieces and show for each piece that it corresponds to one of
the contributions of lemma \ref{lem:Ntimes1}, while also each term in this lemma is represented by one of the terms
obtained by breaking up \eqref{eq:ProductG}.

As a first step we break the right hand side of \eqref{eq:ProductG} into three pieces separated by in which set \(X_{n+1}\) 
ends up in :
\begin{align}\notag
\sum_{\stackrel{\langle n +1 \rangle = \dotCup_{l\in T} l \dotcup \dotCup_{l\in D} l \dotcup R}{ \forall l \in T \cup D: |l|\ge 2}} \text{PT}(T) \text{Op}(R,D)=\\\label{ProductGI}
\sum_{\stackrel{\langle n +1 \rangle = \dotCup_{l\in T} l \dotcup \dotCup_{l\in D} l \dotcup R}{\stackrel{\exists l \in T: X_{n+1}\in l}{ \forall l \in T \cup D: |l|\ge 2}}} \text{PT}(T) \text{Op}(R,D)\\\label{ProductGII}
+\sum_{\stackrel{\langle n +1 \rangle = \dotCup_{l\in T} l \dotcup \dotCup_{l\in D} l \dotcup R}{\stackrel{\exists l\in D: X_{n+1}\in l}{ \forall l \in T \cup D: |l|\ge 2}}} \text{PT}(T) \text{Op}(R,D)\\\label{ProductGIII}
+\sum_{\stackrel{\langle n +1 \rangle = \dotCup_{l\in T} l \dotcup \dotCup_{l\in D} l \dotcup R}{\stackrel{X_{n+1}\in R}{ \forall l \in T \cup D: |l|\ge 2}}} \text{PT}(T) \text{Op}(R,D),
\end{align}

We will discuss each term separately. For term \eqref{ProductGI} the term containing \(X_{n+1}\) is in one of the elements \(l'\) of \(T\), 
but each such element has to have more than one element. So if we were to sum over the partitions of \(\langle n\rangle \) instead,
the rest of \(l'\backslash \{X_{n+1}\}\) is either an element of \(D\) or, if it contains only one element, of \(R\). Picking \(D\) instead of \(T\)
is at this stage an arbitrary choice, but this choice leads to the terms of lemma \ref{lem:Ntimes1}. Alls this means that one
correct rewriting of term \eqref{ProductGI} is

\begin{multline}\label{eq:ProductGI2}
\eqref{ProductGI}= \sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D} l \dotcup R}{\forall l \in D \cup T: |l|>2} } \sum_{b \in D \cup \{ \{r\} \mid r\in R \}}  \\
\text{PT}(T\cup \{\{X_{n+1} \cup f\}) \text{Op}(R \backslash b, D\backslash \{b\}).
\end{multline}

Next we pull one factor and the corresponding sum out of PT and write out Op. Then we see that the sums over permutations 
can be merged into one. There we take the convention that for any set \(f\) such that \(|f|=1\) holds, we define \(\sigma_f\) to be
the identity on that set.

This results in

\begin{align}\notag
\eqref{eq:ProductGI2}= \sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D} l \dotcup R}{\forall l \in D \cup T: |l|>2} }\sum_{b \in D \cup \{ \{r\} \mid r\in R \}}  \sum_{\sigma_b \in S(b)}\\\notag
 \tr[P_+ X_{n+1} P_- W(b,\sigma_b)] \text{PT}(T) \sum_{\stackrel{\forall l \in D\backslash \{b\}}{\sigma_l \in S(l)}} \\\notag
 \sum_{a\subseteq R\backslash b \cup \bigcup_{l\in D\backslash \{b\}}\{W(l,\sigma_l)\}}  L(a,a^c) (-1)^{|a|+ \frac{(|R|+|D|-1)(|R|+|D|)}{2}}\\\notag
 =\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D} l \dotcup R}{\forall l \in D \cup T: |l|>2} }\sum_{\stackrel{\forall l \in D}{\sigma_l \in S(l)}}  \sum_{b \in D \cup \{ \{r\} \mid r\in R \}} \text{PT}(T)\\\notag
 \sum_{a\subseteq R\backslash b \cup \bigcup_{l\in D\backslash \{b\}}\{W(l,\sigma_l)\}}  L(a,a^c) (-1)^{|a|+ \frac{(|R|+|D|-1)(|R|+|D|)}{2}}\\\notag
\tr[P_+ X_{n+1} P_- W(b,\sigma_b)]   \\\notag
=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D} l \dotcup R}{\forall l \in D \cup T: |l|>2} }\sum_{\stackrel{\forall l \in D}{\sigma_l \in S(l)}}   \text{PT}(T)  \sum_{a\subseteq R \cup \bigcup_{l\in D}\{W(l,\sigma_l)\}}\\\notag
\sum_{b \in D \cup \{ \{r\} \mid r\in R \}} \id_{W(b,\sigma_b)\in a}  L(a\backslash\{b\},a^c) (-1)^{|a|+1}\\\notag
(-1)^\frac{(|R|+|D|-1)(|R|+|D|)}{2} \tr[P_+ X_{n+1} P_- W(b,\sigma_b)]\\\notag
=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D} l \dotcup R}{\forall l \in D \cup T: |l|>2} }\text{PT}(T) \sum_{\stackrel{\forall l \in D}{\sigma_l \in S(l)}}  \sum_{a\subseteq R \cup \bigcup_{l\in D}\{W(l,\sigma_l)\}} \\\notag
 \sum_{b \in a  }   L(a\backslash\{b\},a^c) (-1)^{|a|+\frac{(|R|+|D|+1)(|R|+|D|)}{2}}\\\notag
 (-1)^{1+|R|+|D|} \tr[P_+ X_{n+1} P_- W(b,\sigma_b)]\\\notag
 =\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D} l \dotcup R}{\forall l \in D \cup T: |l|>2} }\text{PT}(T) \sum_{\stackrel{\forall l \in D}{\sigma_l \in S(l)}}  \sum_{a\subseteq R \cup \bigcup_{l\in D}\{W(l,\sigma_l)\}} \\
   (-1)^{|a|+\frac{(|R|+|D|+1)(|R|+|D|)}{2}}  \eqref{Ntimes1:TrTerm}_{L(a,a^c) G(X_{n+1})},
\end{align}

where the notation in the last line is to be taken as ``apply Lemma \ref{lem:Ntimes1} apply it to \(L(a,a^c) G(X_{n+1})\) and
pick only term \eqref{Ntimes1:TrTerm}''. We will use this abbreviating notation also for the next terms.

The next term is \eqref{ProductGII}. Here we need a few more notational conventions. For any  set 
\(b\subseteq \langle n \rangle\) and corresponding
permutation \(\sigma_b\in S(b)\), we denote by the same symbol \(\sigma_b\) the continuation of \(\sigma_b\) to \(b\cup \{X_{n+1}\}\),
where for this continuation \(X_{n+1}\) is a fixed point. Furthermore we define for any set 
\(b\subseteq \langle n \rangle\), \(\sigma_c^b\) by

\begin{align}\notag
&\sigma_c^b \in  S(b\cup\{X_{n+1}\}), \\\notag
\forall k\le |b|: &\sigma_c^b(f_{b\cup \{X_{n+1}\}}(k))=f_{b\cup \{X_{n+1}\}}(k+1)\\
 &\sigma^b_c (X_{n+1})=f_b(1).
\end{align}

Finally we define for sets \(b_1,b_2 \subseteq \langle n \rangle, b_1\cap b_2=\emptyset\) and corresponding
permutations \(\sigma_{b_1}\in S(b_1), \sigma_{b_2} \in S(b_2)\) the permutation 
\(\sigma_{b_1,b_2}^{n+1}\) by

\begin{align}\notag
M_{b_1,b_2}^{n+1} :=b_1\cup b_2 \cup \{X_{n+1}\}\\\notag
\sigma_{b_1,b_2}^{n+1}\in S(M_{b_1,b_2}^{n+1} )\\\notag
\forall 1\le k \le |b_1|: \sigma_{b_1,b_2}^{n+1}(f_{M_{b_1,b_2}^{n+1}}(k))=\sigma_{b_1}(f_{b_1}(k))\\\notag
\sigma_{b_1,b_2}^{n+1}(f_{M_{b_1,b_2}^{n+1}}(|b_1|+1))=X_{n+1}\\\notag
\forall |b_1|+2\le k \le |b_1|+|b_2|+1:\\
 \sigma_{b_1,b_2}^{n+1}(f_{M_{b_1,b_2}^{n+1}}(k))
= \sigma_{b_2} ( f_{b_2}(k-|b_1|-1))
\end{align}

The beginning of the treatment of term \eqref{ProductGII} is analogous to \eqref{ProductGI}, we rewrite the
partition of \(\langle n+1\rangle \) into one of \(\langle n \rangle \) with an additional sum over where the
other operators packed to together with \(X_{n+1}\) come from. This splits into three parts, either \(X_{n+1}\)
is put at the beginning of the compound operator, or its put at the end of the compound object, or 
to the left as well as to the right are operators with smaller index. Since the overall sign is decided by 
how often the operator index rises or falls, this separation into cases is helpful. The last case we then
rewrite as picking two sets of operators, one of which will be in front of \(X_{n+1}\) and the other one
behind this operator. 

The calculation is as follows

\begin{align}\notag
&\eqref{ProductGII}= \sum_{\stackrel{\langle n +1 \rangle = \dotCup_{l\in T} l \dotcup \dotCup_{l\in D} l \dotcup R}{\stackrel{\exists l\in D: X_{n+1}\in l}{ \forall l \in T \cup D: |l|\ge 2}}} \text{PT}(T) \text{Op}(R,D)\\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D}l \dotcup R}{\forall l \in D\cup T: |l|\ge 2}} \text{PT}(T) \sum_{b\in D \cup \{\{r\}\mid r \in R\}} \text{Op} (R\backslash b , D\cup \{ b \cup \{X_{n+1}\}\backslash \{b\}\})\\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D}l \dotcup R}{\forall l \in D\cup T: |l|\ge 2}} \text{PT}(T) \sum_{b\in D \cup \{\{r\}\mid r \in R\}} \sum_{\stackrel{\forall l \in D \cup \{b \cup \{X_{n+1}\}\}}{\sigma_l \in S(l)}}\\\notag
 &\sum_{a \subseteq R\backslash b \cup \bigcup_{l \in D \cup \{b \cup \{X_{n+1}\} \}\backslash \{b\}}\{W(l,\sigma_l)\}} L(a,a^c) (-1)^{|a|+ \frac{(|R|+|D|)(|R|+|D|+1)}{2}}\\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D}l \dotcup R}{\forall l \in D\cup T: |l|\ge 2}} \text{PT}(T) \sum_{b\in D \cup \{\{r\}\mid r \in R\}} \sum_{\stackrel{\forall l \in D}{\sigma_l \in S(l)}}\Big[\\\label{ProductGII.1}
 &\sum_{a \subseteq R\backslash b \cup \bigcup_{l \in D\backslash \{b\}}\{W(l,\sigma_l)\} \cup \{W(b\cup \{X_{n+1}\},\sigma_b)\}} L(a,a^c) (-1)^{|a|+ \frac{(|R|+|D|)(|R|+|D|+1)}{2}}\\\label{ProductGII.2}
&+\sum_{a \subseteq R\backslash b \cup \bigcup_{l \in D\backslash \{b\}}\{W(l,\sigma_l)\}\cup \{W(b\cup \{X_{n+1}\},\sigma^b_c \circ\sigma_b)\}} L(a,a^c) (-1)^{|a|+ \frac{(|R|+|D|)(|R|+|D|+1)}{2}}\Big]\\\notag
&+\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in \overline{D}}l \dotcup \overline{R}}{\forall l \in \overline{D}\cup T: |l|\ge 2}} \text{PT}(T) \sum_{\stackrel{b_1,b_2\in \overline{D} \cup \{\{r\}\mid r \in \overline{R}\}}{b_1 \neq b_2}} \sum_{\stackrel{\forall l \in \overline{D} }{\sigma_l \in S(l)}}\\\label{ProductGII.3}
&\sum_{a \subseteq \tilde{R}\cup \bigcup_{l \in \tilde{D}}\{W(l,\sigma_l)\} \cup \{W(b_1\cup \{X_{n+1}\}\cup b_2,\sigma^{n+1}_{b_1,b_2})\}} L(a,a^c) (-1)^{|a|+ \frac{(|\overline{R}|+|\overline{D}|-1)(|\overline{R}|+|\overline{D}|)}{2}}
\end{align}

where \(\tilde{R}=\overline{R} \backslash (b_1\cup b_2)\) and
\(\tilde{D}:=\overline{D} \cup \{b_1 \cup \{X_{n+1}\} \cup b_2 \}\backslash \{b_1,b_2\}\). For the term 
\eqref{ProductGII.3} we had to reshuffle the outermost
sum a bit. For each term in the original sum where \(X_{n+1}\) is neither the first nor the last factor in its product (we will call the set of factors in front of \(X_{n+1}\)  \( \alpha\) and the factors behind it \(\beta\)) there is a different splitting of \(\langle n \rangle \) into \(\overline{R}\) and \(\overline{D}\) such that \(\alpha\) and \(\beta\) are separate elements of \(\overline{D}\cup \{\{r\}\mid r \in \overline R\}\).
So we replace the original sum over \(D\) and \(R\) into one of \(\overline{D}\) and \(\overline{R}\). Since this is a one to one
correspondence and the sum is finite this is always possible. The exponent of the sign also changes for this reason, since
\(|R|+|D|=|\overline{R}|+|\overline{D}|-1\) holds. Continuing with \eqref{ProductGII.1} the next steps are similar to the last steps
in treating \eqref{ProductGI}. They are

\begin{align}\notag
&\eqref{ProductGII.1}=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D}l \dotcup R}{\forall l \in D\cup T: |l|\ge 2}} \text{PT}(T) \sum_{b\in D \cup \{\{r\}\mid r \in R\}} \sum_{\stackrel{\forall l \in D}{\sigma_l \in S(l)}}\\\notag
 &\sum_{a \subseteq R\backslash b \cup \bigcup_{l \in D\backslash \{b\}}\{W(l,\sigma_l)\} \cup \{W(b\cup \{X_{n+1}\},\sigma_b)\}} L(a,a^c) (-1)^{|a|+ \frac{(|R|+|D|)(|R|+|D|+1)}{2}}\\\notag
 &=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D}l \dotcup R}{\forall l \in D\cup T: |l|\ge 2}} \text{PT}(T) \sum_{b\in D \cup \{\{r\}\mid r \in R\}} \sum_{\stackrel{\forall l \in D}{\sigma_l \in S(l)}} \\\notag
 &\sum_{a \subseteq R \cup \bigcup_{l \in D}\{W(l,\sigma_l)\}} (-1)^{|a|+ \frac{(|R|+|D|)(|R|+|D|+1)}{2}}\id_{W(b,\sigma_b)\in a} \\\notag
& \big[L(a\backslash \{W(b,\sigma_b)\} \cup \{W(b\cup \{X_{n+1}\},\sigma_b)\},a^c) \\\notag
& -L(a\backslash \{W(b,\sigma_b)\},a^c \cup \{W(b\cup \{X_{n+1}\},\sigma_b)\}) \big]\\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D}l \dotcup R}{\forall l \in D\cup T: |l|\ge 2}} \text{PT}(T)  \sum_{\stackrel{\forall l \in D}{\sigma_l \in S(l)}} \\\notag
 &\sum_{a \subseteq R \cup \bigcup_{l \in D}\{W(l,\sigma_l)\}} (-1)^{|a|+ \frac{(|R|+|D|)(|R|+|D|+1)}{2}}
 \sum_{W(b,\sigma_b) \in a }  \\\notag
& \big[L(a\backslash \{W(b,\sigma_b)\} \cup \{W(b\cup \{X_{n+1}\},\sigma_b)\},a^c) \\\notag
& -L(a\backslash \{W(b,\sigma_b)\},a^c \cup \{W(b\cup \{X_{n+1}\},\sigma_b)\}) \big]\\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D}l \dotcup R}{\forall l \in D\cup T: |l|\ge 2}} \text{PT}(T)  \sum_{\stackrel{\forall l \in D}{\sigma_l \in S(l)}} \sum_{a \subseteq R \cup \bigcup_{l \in D}\{W(l,\sigma_l)\}} \\
 & (-1)^{|a|+ \frac{(|R|+|D|)(|R|+|D|+1)}{2}}
 (\eqref{Ntimes1:n+1atEnd1}+\eqref{Ntimes1:n+1atEnd2})_{L(a,a^c)G(X_{n+1})}.
\end{align}

Almost the same procedure applies to \eqref{ProductGII.2}. It yields %\label{Ntimes1:n+1atBeginnign1}

\begin{align}\notag
&\eqref{ProductGII.2}=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D}l \dotcup R}{\forall l \in D\cup T: |l|\ge 2}} \text{PT}(T) \sum_{b\in D \cup \{\{r\}\mid r \in R\}} \sum_{\stackrel{\forall l \in D}{\sigma_l \in S(l)}}\\\notag
 &\sum_{a \subseteq R\backslash b \cup \bigcup_{l \in D\backslash \{b\}}\{W(l,\sigma_l)\} \cup \{W(b\cup \{X_{n+1}\},\sigma^b_c\circ \sigma_b)\}} L(a,a^c) (-1)^{|a|+ \frac{(|R|+|D|)(|R|+|D|+1)}{2}}\\\notag
 &=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D}l \dotcup R}{\forall l \in D\cup T: |l|\ge 2}} \text{PT}(T) \sum_{b\in D \cup \{\{r\}\mid r \in R\}} \sum_{\stackrel{\forall l \in D}{\sigma_l \in S(l)}} \\\notag
 &\sum_{a \subseteq R \cup \bigcup_{l \in D}\{W(l,\sigma_l)\}} (-1)^{|a|+ \frac{(|R|+|D|)(|R|+|D|+1)}{2}} \\\notag
& \big[ \id_{W(b,\sigma^b_c\circ\sigma_b)\in a} L(a\backslash \{W(b,\sigma_b)\} \cup \{W(b\cup \{X_{n+1}\},\sigma^b_c \circ\sigma_b)\},a^c) \\\notag
& +\id_{W(b,\sigma^b_c\circ \sigma_b)\in a^c} L(a\backslash \{W(b,\sigma_b)\},a^c \cup \{W(b\cup \{X_{n+1}\},\sigma^b_c \circ \sigma_b)\}) \big]\\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D}l \dotcup R}{\forall l \in D\cup T: |l|\ge 2}} \text{PT}(T)  \sum_{\stackrel{\forall l \in D}{\sigma_l \in S(l)}} \\\notag
 &\sum_{a \subseteq R \cup \bigcup_{l \in D}\{W(l,\sigma_l)\}} (-1)^{|a|+ \frac{(|R|+|D|)(|R|+|D|+1)}{2}}
  \\\notag
& \big[ \sum_{W(b,\sigma_b) \in a }L(a\backslash \{W(b,\sigma_b)\} \cup \{W(b\cup \{X_{n+1}\},\sigma^b_c \circ\sigma_b)\},a^c) \\\notag
&+  \sum_{W(b,\sigma_b) \in a^c } L(a,a^c \backslash \{W(b,\sigma_b)\} \cup \{W(b\cup \{X_{n+1}\},\sigma^b_c \circ \sigma_b)\}) \big]\\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D}l \dotcup R}{\forall l \in D\cup T: |l|\ge 2}} \text{PT}(T)  \sum_{\stackrel{\forall l \in D}{\sigma_l \in S(l)}} \sum_{a \subseteq R \cup \bigcup_{l \in D}\{W(l,\sigma_l)\}} \\
 & (-1)^{|a|+ \frac{(|R|+|D|)(|R|+|D|+1)}{2}}
 (\eqref{Ntimes1:n+1atBeginning1}+\eqref{Ntimes1:n+1atBeginning2})_{L(a,a^c)G(X_{n+1})}.
\end{align}


Also for \eqref{ProductGII.3} the procedure is almost the same. We bring the sums into a form such that one can read off the
terms generated by the induction. We begin be renaming the sets which we had to change by resumming back to the names 
of the original sets. 


\begin{align}\notag
&\eqref{ProductGII.3}=
\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in \overline{D}}l \dotcup \overline{R}}{\forall l \in \overline{D}\cup T: |l|\ge 2}} 
\text{PT}(T) 
\sum_{\stackrel{b_1,b_2\in \overline{D} \cup \{\{r\}\mid r \in \overline{R}\}}{b_1 \neq b_2}} 
\sum_{\stackrel{\forall l \in \overline{D} }{\sigma_l \in S(l)}}\\\notag
&\sum_{a \subseteq \tilde{R}\cup \bigcup_{l \in \tilde{D}}\{W(l,\sigma_l)\} \cup \{W(b_1\cup \{X_{n+1}\}\cup b_2,\sigma^{n+1}_{b_1,b_2})\}} L(a,a^c) (-1)^{|a|+ \frac{(|\overline{R}|+|\overline{D}|-1)(|\overline{R}|+|\overline{D}|)}{2}}\\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D}l \dotcup R}{\forall l \in D\cup T: |l|\ge 2}} 
\text{PT}(T) 
\sum_{\stackrel{b_1,b_2\in D \cup \{\{r\}\mid r \in R\}}{b_1 \neq b_2}} 
\sum_{\stackrel{\forall l \in D }{\sigma_l \in S(l)}}\\\notag
&\sum_{a \subseteq R\cup \bigcup_{l \in D}\{W(l,\sigma_l)\} \cup \{W(b_1\cup \{X_{n+1}\}\cup b_2,\sigma^{n+1}_{b_1,b_2})\}} L(a,a^c) (-1)^{|a|+ \frac{(| R|+|D|-1)(|R|+|D|)}{2}}\\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D}l \dotcup R}{\forall l \in D\cup T: |l|\ge 2}} 
\text{PT}(T) 
\sum_{\stackrel{b_1,b_2\in D \cup \{\{r\}\mid r \in R\}}{b_1 \neq b_2}} 
\sum_{\stackrel{\forall l \in D }{\sigma_l \in S(l)}} \sum_{a \subseteq R\cup \bigcup_{l \in D}\{W(l,\sigma_l)\}} \\\notag
&(-1)^{|R|+|D|+ \frac{(|R|+|D|)(|R|+|D|+1)}{2}} \id_{W(b_1,\sigma_1)\in a}\\\notag
&\left[+(-1)^{|a|+1}\id_{W(b_2,\sigma_2)\in a} L\Big(a\backslash\{W(b_1,\sigma_1),W(b_2,\sigma_2)\}\cup\right.\\\notag
&\cup \{W(b_1\cup \{X_{n+1}\}\cup b_2,\sigma^{n+1}_{b_1,b_2})\} ,a^c\Big) \\\notag
&+(-1)^{|a|+1}\id_{W(b_2,\sigma_2)\in a^c}L\Big(a\backslash\{W(b_1,\sigma_1)\} ,a^c \backslash\{W(b_2,\sigma_2)\}\cup\\\notag
&\left. \cup \{W(b_1\cup \{X_{n+1}\}\cup f_2,\sigma^{n+1}_{b_1,b_2})\}\Big) \right]\\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D}l \dotcup R}{\forall l \in D\cup T: |l|\ge 2}} 
\text{PT}(T) 
\sum_{\stackrel{\forall l \in D }{\sigma_l \in S(l)}} \sum_{a \subseteq R\cup \bigcup_{l \in D}\{W(l,\sigma_l)\}} \\\notag
&(-1)^{|R|+|D|+ \frac{(|R|+|D|)(|R|+|D|+1)}{2}} \\\notag
&\Big[(-1)^{|a|+1}\sum_{\stackrel{b_1,b_2 \in a}{b_1 \neq b_2}}  L\Big(a\backslash\{W(b_1,\sigma_1),W(b_2,\sigma_2)\}\cup\\\notag
&\cup \{W(b_1\cup \{X_{n+1}\}\cup b_2,\sigma^{n+1}_{b_1,b_2})\} ,a^c\Big) \\\notag
&+(-1)^{|a|+1}\sum_{b_1\in a,b_2\in a^c}  L\Big(a\backslash\{W(b_1,\sigma_1)\} ,a^c \backslash\{W(b_2,\sigma_2)\}\cup\\\notag
&\left. \cup \{W(b_1\cup \{X_{n+1}\}\cup f_2,\sigma^{n+1}_{b_1,b_2})\}\Big) \right]\\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D}l \dotcup R}{\forall l \in D\cup T: |l|\ge 2}} 
\text{PT}(T) 
\sum_{\stackrel{\forall l \in D }{\sigma_l \in S(l)}} \sum_{a \subseteq R\cup \bigcup_{l \in D}\{W(l,\sigma_l)\}} \\\notag
&(-1)^{|a|+ \frac{(|R|+|D|)(|R|+|D|+1)}{2}} (\eqref{Ntimes1:middle1}+\eqref{Ntimes1:middle2})_{L(a,a^c)G(X_{n+1})}
\end{align}

Lastly we will discuss term \eqref{ProductGIII}; luckily, this term is less involved than the other two. The general procedure;
however, stays the same. First we reformulate the partition of \(\langle n+1\rangle\) into one of \(\langle n \rangle \), where
the terms acquire modifications. Secondly we massage these terms until the involved sums look exactly like the one 
in our induction hypothesis \eqref{eq:ProductG} and realise that the terms are produced by lemma \ref{lem:Ntimes1}.
For term \eqref{ProductGIII}  this results in

\begin{align}\notag
&\eqref{ProductGIII}
=\sum_{\stackrel{\langle n +1 \rangle = \dotCup_{l\in T} l \dotcup \dotCup_{l\in D} l \dotcup R}{\stackrel{X_{n+1}\in R}{ \forall l \in T \cup D: |l|\ge 2}}} \text{PT}(T) \text{Op}(R,D)\\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T} l \dotcup \dotCup_{l\in D} l \dotcup R}{ \forall l \in T \cup D: |l|\ge 2}} \text{PT}(T)
\text{Op}(R\cup \{X_{n+1}\},D)\\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T} l \dotcup \dotCup_{l\in D} l \dotcup R}{ \forall l \in T \cup D: |l|\ge 2}} \text{PT}(T)
\sum_{\stackrel{\forall l \in D:}{\sigma_l \in S(l)}} \quad \sum_{a\subseteq R \cup \{X_{n+1}\} \cup \bigcup_{l\in D} \{W(l,\sigma_l)\}}\\\notag
&L(a,a^c)(-1)^{|a| + \frac{(|R|+1+|D|)(|R|+|D|+2)}{2}}\\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T} l \dotcup \dotCup_{l\in D} l \dotcup R}{ \forall l \in T \cup D: |l|\ge 2}} \text{PT}(T)
\sum_{\stackrel{\forall l \in D:}{\sigma_l \in S(l)}} \quad \sum_{a\subseteq R \cup \bigcup_{l\in D} \{W(l,\sigma_l)\}}
(-1)^{|a| + \frac{(|R|+1+|D|)(|R|+|D|)}{2}}\\\notag
&\big(-L(a\cup\{X_{n+1}\},a^c)+L(a,a^c\cup \{X_{n+1}\}) \big)(-1)^{|R|+|D|+1} \\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T} l \dotcup \dotCup_{l\in D} l \dotcup R}{ \forall l \in T \cup D: |l|\ge 2}} \text{PT}(T)
\sum_{\stackrel{\forall l \in D:}{\sigma_l \in S(l)}} \quad \sum_{a\subseteq R \cup \bigcup_{l\in D} \{W(l,\sigma_l)\}}
(-1)^{|a| + \frac{(|R|+1+|D|)(|R|+|D|)}{2}}\\\notag
&\big(L(a\cup\{X_{n+1}\},a^c)(-1)^{|R|+|D|}+L(a,a^c\cup \{X_{n+1}\})(-1)^{|R|+|D|+1} \big) \\\notag
&=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T} l \dotcup \dotCup_{l\in D} l \dotcup R}{ \forall l \in T \cup D: |l|\ge 2}} \text{PT}(T)
\sum_{\stackrel{\forall l \in D:}{\sigma_l \in S(l)}} \quad \sum_{a\subseteq R \cup \bigcup_{l\in D} \{W(l,\sigma_l)\}}
(-1)^{|a| + \frac{(|R|+1+|D|)(|R|+|D|)}{2}}\\\notag
&\big(\eqref{Ntimes1:simplyAdd1}+\eqref{Ntimes1:simplyAdd2} \big)_{L(a,a^c)G(X_{n+1})}.
\end{align}

Summarising we showed 
\begin{align*}
&\sum_{\stackrel{\langle n +1 \rangle = \dotCup_{l\in T} l \dotcup \dotCup_{l\in D} l \dotcup R}{ \forall l \in T \cup D: |l|\ge 2}} \text{PT}(T) \text{Op}(R,D)\\
 &=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D} l \dotcup R}{\forall l \in D \cup T: |l|>2} }\text{PT}(T) \sum_{\stackrel{\forall l \in D}{\sigma_l \in S(l)}}  \sum_{a\subseteq R \cup \bigcup_{l\in D}\{W(l,\sigma_l)\}} \\
  & (-1)^{|a|+\frac{(|R|+|D|+1)(|R|+|D|)}{2}}  \\
  &\big( \eqref{Ntimes1:TrTerm}+\eqref{Ntimes1:n+1atEnd1}+\eqref{Ntimes1:n+1atEnd2} 
  +\eqref{Ntimes1:n+1atBeginning1}+\eqref{Ntimes1:n+1atBeginning2}\\
 &+\eqref{Ntimes1:middle1}+\eqref{Ntimes1:middle2}
  +\eqref{Ntimes1:simplyAdd1}+\eqref{Ntimes1:simplyAdd2}\big)_{L(a,a^c) G(X_{n+1})}\\
 &=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D} l \dotcup R}{\forall l \in D \cup T: |l|>2} }\text{PT}(T) \sum_{\stackrel{\forall l \in D}{\sigma_l \in S(l)}}  \sum_{a\subseteq R \cup \bigcup_{l\in D}\{W(l,\sigma_l)\}} \\
  & (-1)^{|a|+\frac{(|R|+|D|+1)(|R|+|D|)}{2}} L(a,a^c)G(X_{n+1})\\
  &=\sum_{\stackrel{\langle n \rangle = \dotCup_{l\in T}l \dotcup \dotCup_{l\in D} l \dotcup R}{\forall l \in D \cup T: |l|>2} }\text{PT}(T) \text{Op}(R,D)G(X_{n+1})\\
  &=\prod_{l=1}^n G(X_{l}) \quad G(X_{n+1}),
\end{align*}

which ends our proof by induction.\qed




\appendix

\backmatter

\chapter{Appendix}

\section{Heuristic Construction of \(S\)-Matrix expression}\label{sec:heuristic construction}

In the following I derive a recursive equation for the coefficients of the expansion 
of the second quantized scattering operator. The starting point of this derivation is 
the commutator of \(T_m\), equation \eqref{logarithmic lift condition}.

\subsection{Guessing Equations}

Why at this point one might suspect that such a representation exists is, because
looking at equation \eqref{logarithmic lift condition} for a while, one
comes to the conclusion that if one replaces \(T_m\) by 
\begin{equation}
T_m - \frac{1}{2} \sum_{k=1}^{m-1} \begin{pmatrix} m \\ k\end{pmatrix} T_k T_{m-k},
\end{equation}
no \(T_k\) with \(k>m-2\) will occur on the right hand side of the resulting equation.
So if one subtracts the right polynomial in \(T_k\) for suitable \(k\) one might achieve
a commutator which contains only the creation respectively annihilation operator 
concatenated with some one particle operator. From our treatment of \(T_1\)
\noch{place proper reference to definition of G operator} we know which
operators have such commutation relations. 

So having this in Mind we start with the ansatz

\begin{equation}\label{Def Gamma_m}
\Gamma_m := \sum_{g=2}^m \sum_{\stackrel{b\in\mathbb{N}^g}{|b|=m}} c_{b} \prod_{k=1}^g T_{b_k}.
\end{equation}

Now in order to show that \(T_m\) and \(\Gamma_m\) agree up to operators which have a commutation 
relation of the form \eqref{G commutator}, we calculate \(\left[ T_m-\Gamma_m, a^\#(\varphi_n) \right]\)
for arbitrary \(n\in\mathbb{Z}\) and try to choose the coefficients \(c_b\) of \eqref{Def Gamma_m}
such that all contributions vanish which do not have the form \(a^\# \left( \prod_k Z_{\alpha_k}\right)\)
for any suitable \((\alpha_k)_k\subset \mathbb{N} \). If one does so, one is led to a system of equations
of which I wrote down a few to give an overview of its structure. The objects \(\alpha_k, \beta_l\) 
in the system of equations can be any natural Number for any \(k,l\in\mathbb{N}\).

\begin{align*}
&0 =c_{\alpha_1,\beta_1} + c_{\beta_1,\alpha_1}+ \binom{ \alpha_1 + \beta_1}{ \alpha_1} \\
&0 = c_{\alpha_1,\alpha_2,\beta_1} + c_{\beta_1,\alpha_1,\alpha_2} + c_{\alpha_2,\alpha_1,\beta_1} + 
\binom{\alpha_2 + \beta_1}{ \alpha_2} c_{\alpha_1,\alpha_2+\beta_1} \\
&\hspace{2cm}  +\binom{\alpha_1+\beta_1}{\alpha_1} c_{\alpha_1+\beta_1,\alpha_2}\\
&0= c_{\alpha_1,\alpha_2,\alpha_3,\beta_1} + c_{\alpha_1,\alpha_2,\beta_1,\alpha_3} 
+ c_{\alpha_1,\beta_1,\alpha_2,\alpha_3} + c_{\beta_1,\alpha_1,\alpha_2,\alpha_3}\\
&+\binom{\alpha_1+\beta_1}{\beta_1} 
c_{\alpha_1+\beta_1,\alpha_2,\alpha_3} 
+ \binom{\alpha_2+\beta_1}{\beta_1} c_{\alpha_1,\alpha_2+\beta_1,\alpha_3}\\
&\hspace{2cm} + \binom{\alpha_3+\beta_1}{\beta_1} c_{\alpha_1,\alpha_2,\alpha_3+\beta_1}\\
&0= c_{\alpha_1,\alpha_2,\beta_1,\beta_2} +c_{\alpha_1,\beta_1,\alpha_2,\beta_2} +
c_{\beta_1,\alpha_1,\alpha_2,\beta_2} +c_{\alpha_1,\beta_1,\beta_2,\alpha_2} \\
&+c_{\beta_1,\alpha_1,\beta_2,\alpha_2} +c_{\beta_1,\beta_2,\alpha_1,\alpha_2} 
+\binom{\alpha_1+\beta_1}{\alpha_1} (c_{\alpha_1+\beta_1,\alpha_2,\beta_2} \\
&+ c_{\alpha_1+\beta_1,\beta_2,\alpha_2})  
+\binom{\alpha_1+\beta_2}c_{\beta_1,\alpha_1+\beta_2,\alpha_1} \\
&+\binom{\alpha_2+\beta_1}{\alpha_2} c_{\alpha_1,\alpha_2+\beta_1,\beta_2}
+ \binom{\alpha_2+\beta_2}{\alpha_2} (c_{\alpha_1,\beta_1,\alpha_2+\beta_2}\\
&+ c_{\beta_1,\alpha_1,\alpha_2+\beta_2})
+ \binom{\alpha_1+\beta_1}{\alpha_1} \binom{\alpha_2+\beta_2}{\alpha_2}
 c_{\alpha_1+\beta_1,\alpha_2+\beta_2}\\
&0=c_{\alpha_1,\beta_1,\beta_2,\beta_3,\beta_4} 
+ c_{\beta_1,\alpha_1,\beta_2,\beta_3,\beta_4} 
+ c_{\beta_1,\beta_2,\alpha_1,\beta_3,\beta_4} \\
&\hspace{1cm}+ c_{\beta_1,\beta_2,\beta_3,\alpha_1,\beta_4} 
+ c_{\beta_1,\beta_2,\beta_3,\beta_4,\alpha_1} \\
&+\binom{\alpha_1+\beta_1}{\alpha_1} c_{\alpha_1+\beta_1,\beta_2,\beta_3,\beta_4}
+ \binom{\alpha_1+\beta_2}{\alpha_1} c_{\beta_1,\alpha_1+\beta_2,\beta_3,\beta_4}\\
&+ \binom{\alpha_1+\beta_3}{\alpha_1} c_{\beta_1,\beta_2,\alpha_1+\beta_3,\beta_4}
+ \binom{\alpha_1+\beta_4}{\alpha_1} c_{\beta_1,\beta_2,\beta_3,\alpha_1+\beta_4}\\
&0= c_{\alpha_1,\alpha_2,\beta_1,\beta_2,\beta_3} 
+c_{\alpha_1,\beta_1,\alpha_2,\beta_2,\beta_3} 
+c_{\beta_1,\alpha_1,\alpha_2,\beta_2,\beta_3} \\
&+c_{\alpha_1,\beta_1,\beta_2,\alpha_2,\beta_3} 
+c_{\beta_1,\alpha_1,\beta_2,\alpha_2,\beta_3} 
+c_{\beta_1,\beta_2,\alpha_1,\alpha_2,\beta_3} \\
&+c_{\alpha_1,\beta_1,\beta_2,\beta_3,\alpha_2} 
+c_{\beta_1,\alpha_1,\beta_2,\beta_3,\alpha_2} 
+c_{\beta_1,\beta_2,\alpha_1,\beta_3,\alpha_2} \\
&+c_{\beta_1,\beta_2,\beta_3,\alpha_1,\alpha_2} 
+\binom{\alpha_1+\beta_1}{\beta_1} (c_{\alpha_1+\beta_1,\alpha_2,\beta_2,\beta_3}\\
&+c_{\alpha_1+\beta_1,\beta_2,\alpha_2,\beta_3}
+c_{\alpha_1+\beta_1,\beta_2,\beta_3,\alpha_2})\\
&+\binom{\alpha_2+\beta_1}{\beta_1} c_{\alpha_1,\alpha_2+\beta_1,\beta_2,\beta_3}\\
&+\binom{\alpha_2+\beta_2}{\beta_2} (c_{\beta_1,\alpha_1,\alpha_2+\beta_2,\beta_3}
+c_{\alpha_1,\beta_1,\alpha_2+\beta_2,\beta_3})\\
&+\binom{\alpha_1+\beta_2}{\beta_2} (c_{\beta_1,\alpha_1+\beta_2,\alpha_2,\beta_3}
+c_{\beta_1,\alpha_1+\beta_2,\beta_3,\alpha_2})\\
&+\binom{\alpha_2+\beta_3}{\beta_3}( c_{\alpha_1,\beta_1,\beta_2,\alpha_2+\beta_3}
+c_{\beta_1,\alpha_1,\beta_2,\alpha_2+\beta_3}\\
&+c_{\beta_1,\beta_2,\alpha_1,\alpha_2+\beta_3})
+\binom{\alpha_1+\beta_3}{\beta_3} c_{\beta_1,\beta_2,\alpha_1+\beta_3,\alpha_2}\\
&+\binom{\alpha_1+\beta_1}{\alpha_1} \binom{\alpha_2+\beta_2}{\alpha_2} 
c_{\alpha_1+\beta_1,\alpha_2+\beta_2,\beta_3}\\
&+\binom{\alpha_1+\beta_2}{\alpha_1} \binom{\alpha_2+\beta_3}{\alpha_2} 
c_{\beta_1,\alpha_1+\beta_2,\alpha_2+\beta_3}\\
&+\binom{\alpha_1+\beta_1}{\alpha_1} \binom{\alpha_2+\beta_3}{\alpha_2} 
c_{\alpha_1+\beta_1,\beta_2,\alpha_2+\beta_3}\\
& \hspace{3cm} \vdots
\end{align*}

Solving the first few equations and plugging the solution into the consecutive
 equations one can see that at least the first equations are solved by 
\begin{equation}
c_{\alpha_1,\dots, \alpha_k} = \frac{(-1)^k}{k} \begin{pmatrix}\sum_{l=1}^k \alpha_l\\ \alpha_1\ \alpha_2 \ \cdots \alpha_k\end{pmatrix},
\end{equation}
where the last factor is the multinomial coefficient of the indices \(\alpha_1,\dots, \alpha_k\in\mathbb{N}\).

\subsection{Recursive equation for Coefficients of the second quantised scattering operator}

For the rest of this chapter, we are going to derive a concrete form of the second quantised scattering matrix. In order to turn the ``conjectures'' 
into ``theorems'' not only would one have to turn the rough sketches of the combinatorics into proofs, one also would have to show linearity (over real numbers) and
continuity of \(\mathrm{d}\Gamma(B)\) in \(B\).
However, since the final result can be verified to be well defined and to fulfil the lift conditions, this will not be necessary.
We will nonetheless come across various combinatorial assertions that we are going to prove rigorously. 
These will be clearly marked: ``lemma'' and ``proof''.

We are going to use the following definition of binomial coefficients:
\begin{Def}
For \(a\in\mathbb{C}, b\in\mathbb{Z}\) we define

\begin{equation}
\binom{a}{b} := \left\{\begin{matrix}
\prod_{l=0}^{b-1} \frac{a-l}{l+1} \quad \text{for } b\ge 0\\
0 \hspace{1.7cm} \text{otherwise.}
\end{matrix}\right.
\end{equation}
\end{Def}

Defining the binomial coefficient for negative lower index to be zero has the merit, that one can extend the
range of validity of many rules and sums involving binomial coefficients, also one does not have 
to worry about the range of summation in many cases.



The coefficients which we have already guessed more generally to be

\begin{Conj}\label{thm: T_n recursive}
For any \(n\in\mathbb{N}\) the n-th expansion coefficient of the second quantised scattering operator \(T_n\) is given by
\begin{align}\notag
&T_n = \sum_{g=2}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^g}{g} 
\binom{n}{\vec{b}} \prod_{l=1}^g T_{b_l} + C_n \id_{\mathcal{F}} \\ \label{T_n recursive}
&+ d\Gamma\left( \sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^{g+1}}{g} 
\binom{n}{\vec{b}} \prod_{l=1}^g Z_{b_l}  \right),
\end{align}
for some \(C_n\in \mathbb{C}\) which depends on the external field \(A\). The last summand will henceforth
be abbreviated by \(\Gamma_n\).
\end{Conj}

\textbf{Motivation:} The way we will prove this is to compute the commutator of the difference between \(T_n\) and
the first summand of \eqref{T_n recursive} with the creation and annihilation operator of an element of the
basis of \(\mathcal{H}\). This will turn out to be exactly equal
to the corresponding commutator
of the second summand of \eqref{T_n recursive}, since two operators on Fock space only
have the same commutator with general creation and annihilation operators if they
agree up to multiples of the identity this will conclude the motivation of this conjecture. 

 In order to simplify the notation as much as possible, 
I will denote by \(a^\# z\) either \(a(z(\varphi_p))\) or
 \(a^*(z(\varphi_p))\) for any one particle operator \(z\) and any element
 \(\varphi_p\) of the orthonormal basis \((\varphi_p)_{p\in\mathbb{Z}\backslash\{0\}}\) of
 \(\mathcal{H}\). (We need not decide between creation and annihilation 
 operator, since the expressions all agree)
 
In order to organize the bookkeeping of all the summands which arise from iteratively
making use of the commutation rule \eqref{logarithmic lift condition} we organize them 
by the looking at a spanning set of the possible terms that arise my choice is

\begin{equation}\label{combinatorics span}
\left\{ \left. a^\# \prod_{k=1}^{m_1} Z_{\alpha_k} \prod_{k=1}^{m_2}T_{\beta_k} \right|
m_1\in\mathbb{N},m_2\in\mathbb{N}_0, \alpha\in \mathbb{N}^{m_1}, 
\beta \in \mathbb{N}^{m_2}, |\alpha|+|\beta|=n\right\}_.
\end{equation}

As a first step of computing the commutator in question we look at the summand
corresponding to a fixed value of the summation index \(g\) of 

\begin{equation}\label{combinatorics total sum of T}
-\sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^g}{g} 
\binom{n}{\vec{b}} \prod_{l=1}^g T_{b_l}.
\end{equation}

 We need to bring this object into the form of a sum
of terms which are multiples of elements of the set \eqref{combinatorics span}.
This we will commit ourselves to for the next few pages. First we apply
the product rule for the commutator:

\begin{align*}
&\left[ \sum_{\stackrel{\vec{l}\in\mathbb{N}^g}{|\vec{l}|=n}} \frac{(-1)^g}{g} \binom{n}{\vec{l}}
 \prod_{k=1}^g T_{l_k},a^\#\right]\\
 &= \sum_{\stackrel{\vec{l}\in\mathbb{N}^g}{|\vec{l}|=n}} \frac{(-1)^g}{g} \binom{n}{\vec{l}}
 \sum_{\tilde{k}=1}^g \prod_{j=1}^{\tilde{k}-1} T_{l_j} 
 \left[ T_{l_{\tilde{k}}},a^\#\right] \prod_{j=\tilde{k}+1}^g T_{l_j}\\
&=\sum_{\stackrel{\vec{l}\in\mathbb{N}^g}{|\vec{l}|=n}} \frac{(-1)^g}{g} \binom{n}{\vec{l}}
 \sum_{\tilde{k}=1}^g \prod_{j=1}^{\tilde{k}-1} T_{l_j} 
\sum_{\sigma_{\tilde{k}}=1}^{l_{\tilde{k}}} \binom{l_{\tilde{k}}}{\sigma_{\tilde{k}}} 
a^\# Z_f T_{l_{\tilde{k}}-\sigma_{\tilde{k}}} \prod_{j=\tilde{k}+1}^g T_{l_j},
\end{align*}
in the second step we used \eqref{logarithmic lift condition}. Now we commute
all the \(T_l\)s to the left of \(a^\#\) to its right:

\begin{equation}\label{combinatorics ordered product}
=\!\!\! \sum_{\stackrel{\vec{l}\in\mathbb{N}^g}{|l|=n}} \!\!\frac{(-1)^g}{g}\! \binom{n}{\vec{l}}
\!\! \sum_{\tilde{k}=1}^g \!\sum_{\stackrel{\forall 1\le j <\tilde{k}}{0\le \sigma_{j}\le l_j}}
\!\sum_{\sigma_{\tilde{k}}=1}^{l_{\tilde{k}}} \prod_{j=1}^{\tilde{k}}\! \binom{l_j}{\sigma_j}
a^\# \prod_{j=1}^{\tilde{k}} Z_{\sigma_j} \prod_{j=1}^{\tilde{k}} T_{l_j-\sigma_j}
\!\!\prod_{j=\tilde{k}+1}^g T_{l_j}.
\end{equation}
At this point we notice that the multinomial coefficient can be combined with all
the binomial coefficients to form a single multinomial coefficient of degree
\(g+\tilde{k}\). Incidentally this is also the amount of \(Z\) operators plus the amount
of \(T\) operators in each product. Moreover the indices of the Multinomial index
agree with the indices of the \(Z\) and \(T\) operators in the product. Because of 
this, we see that if we fix an element of the spanning set \eqref{combinatorics span}
\(a^\# \prod_{k=1}^{m_1} Z_{\alpha_k} \prod_{k=1}^{m_2}T_{\beta_k}\), each 
summand of \eqref{combinatorics ordered product} which contributes to
this element, has the prefactor

\begin{equation}
\frac{(-1)^g}{g} \binom{n}{\alpha_1 \ \cdots \alpha_{m_1} \ \beta_1 \cdots \beta_{m_2}}
\end{equation}

no matter which summation index \(l\in\mathbb{N}^g\) it corresponds to. In order
to do the matching one may ignore the indices \(\sigma_j\) and \(l_j-\sigma_j\) 
which vanish, because the corresponding operators \(Z_0\) and \(T_0\) are equal to
the identity operator on \(\mathcal{H}\) respectively Fock space. 

Since we know that 

\begin{align*}
\left[ d\Gamma\left(\sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^g}{g} 
\binom{n}{\vec{b}}  \prod_{l=1}^g Z_{b_l}\right), a^\#\right]\\
= a^\# \sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^g}{g} 
\binom{n}{\vec{b}}  \prod_{l=1}^g Z_{b_l}
\end{align*}
holds, all that is left to show is that 
\begin{align}\label{combinatorics total commutator}
&\left[-\sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^g}{g} 
\binom{n}{\vec{b}} \prod_{l=1}^g T_{b_l}, a^\#\right]\\\notag
&= a^\# \sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^{g+1}}{g} 
\binom{n}{\vec{b}}  \prod_{l=1}^g Z_{b_l}
\end{align}
also holds. For which we need to count the summands which are multiples of each element of \eqref{combinatorics span}
 corresponding to each \(g\) in \eqref{combinatorics total sum of T}. So let us fix some element
 \(K(m_1,m_2)\) of \eqref{combinatorics span} corresponding to some \(m_1\in\mathbb{N},
 m_2\in\mathbb{N}_0, \alpha \in \mathbb{N}^{m_1}\) and \( \beta\in \mathbb{N}^{m_2}\).
Rephrasing this problem, we can ask which products
\begin{equation}
\prod_{l=1}^g T_{\gamma_l}
\end{equation}
 for suitable \(g\) and \((\gamma_l)_l\) produce, when commuted with a creation or annihilation operator, 
  multiples of \(K(m_1,m_2)\)? We will call this number of 
 total contributions weighted with the factor 
 \( - \frac{(-1)^g}{g}\) borrowed from \eqref{combinatorics total sum of T} \(\#K(m_1,m_2)\).  
 Looking at the commutation relations 
\eqref{logarithmic lift condition} we split the set of indices \(\{\gamma_1\dots \gamma_g\}\) into
three sets \(A,B\) and \(C\), where the commutation relation has to be used in such a way, that

\begin{align*}
&\forall k: \gamma_k \in A \iff \exists j\le m_1: \gamma_k = \alpha_j, \\
\wedge& \forall k: \gamma_k \in B \iff \exists j\le m_2: \gamma_k = \beta_j\\
\wedge & \forall k: \gamma_k \in C \iff \exists j\le m_1, l\le m_2: \gamma_k = \alpha_j + \beta_l
\end{align*}
 holds. Unfortunately not every splitting corresponds to a contribution and not every
 order of multiplication of a legal splitting corresponds to a contribution either.
 However \(\prod_{j} T_{\alpha_j} \prod_j T_{\beta_j}\) gives
 a contribution and it is in fact the longes product that does.
 We may apply the commutation relations backwards to obtain any
 shorter valid combination and hence all combinations. 
 Transforming the commutation rule for \(T_k\) read from right to left
 into a game results in the following rules.
 
Starting from the string 
\begin{equation}
A_1A_2\dots A_{m_1} B_1 B_2 \dots B_{m_2},
\end{equation}

representing the longes product, where here and in the following \(A\)'s
represent operators \(T_k\) which will turn into \(Z_k\) by the commutation rule,
\(B\)'s represent operators \(T_k\) which will stay \(T_k\) after commutation and
\(C\)'s represent operators \(T_k\) which will produce both a \(Z_l\) in the creation/annihilation
operator and a \(T_{k-l}\) behind that operator. The indices are merely there to keep track of
which operator moved where.

So the game consists in the answering how many strings can we produce by 
applying the following rules to the initial string?
\begin{enumerate}
\item You may replace any occurrence of \(A_k B_j\) by \(B_j A_k\) for any \(j\) and \(k\).
\item You may replace any occurrence of \(A_k B_j\) by \(C_{k,j}\) for any \(j\) and \(k\).
\end{enumerate}
Where we have to count the number of times we applied the second rule, or equivalently
the number \(\#C\) of \(C\)'s in the resulting string, because the summation index \(g\) in 
\eqref{combinatorics total sum of T} corresponds to \(m_1+m_2-\#C\). 

Fix \(\#C \in\{0,\dots ,\min(m_1,m_2)\}\). A valid string has \(m_1+m_2-\#C\) characters,
because the number of its \(C\)s is \(\#C\), its number of \(A\)s is \(m_1-\#C\) and 
its number of \(B\)s is \(m_2-\#C\). Ignoring the labelling of the \(A\)s, \(B\)s and \(C\)s 
there are \(\binom{m_1+m_2-\#C}{\#C \ (m_1 - \#C) \ (m_2-\#C)}\) such strings. Now if
we consider one such string without labelling, e.g.

\begin{equation}
C A A B A C C B B A C B B A B B B B,
\end{equation}

there is only one correct labelling to be restored, namely the one where each \(A\) and the first index of 
any \(C\)  receive increasing labels from left to right and analogously for \(B\) and the second 
 index of any \(C\), resulting for our example in
 
\begin{equation}
 C_{1,1} A_2 A_3 B_2 A_4 C_{5,3} C_{6,4} B_{5} B_6 A_7 C_{8,7} B_8 B_9 A_9 B_{10} B_{11} B_{12} B_{13}.
\end{equation}

So any unlabelled  string corresponds to exactly one labelled string which in turn corresponds to 
exactly one choice of operator product \(\prod T\). 
So returning to our Operators, we found the number \(\#K(m_1,m_2)\) it is

\begin{equation}\label{combinatorics binomial sum}
\#K(m_1,m_2) =-\sum_{g=\max(m_1,m_2)}^{m_1+m_2} \frac{(-1)^g}{g} \binom{g}{(m_1+m_2-g) \ (g-m_1) \ (g - m_2)},
\end{equation}
where the total minus sign comes from the total minus sign in front of \eqref{combinatorics total commutator}
with respect to \eqref{T_n recursive}. 

Now since we introduced the slightly non-standard definition of binomial coefficients used in \cite{graham1994concrete} we
can make use of the rules for summing binomial coefficients derived there.
As a first step to evaluate \eqref{combinatorics binomial sum} we split the trinomial coefficient into binomial
ones and make use of the absorption identity

\begin{equation}\tag{absorption}\label{absorption}
\forall a \in \mathbb{C}\ \forall b \in \mathbb{Z}: b \binom{a}{b} = a \binom{a-1}{b-1} 
\end{equation}

for \(m_2,m_1\neq 0\) as follows

\begin{align*}
&\#K(m_1,m_2) \\
&=-\sum_{g=\max(m_1,m_2)}^{m_1+m_2} \frac{(-1)^g}{g} \binom{g}{(m_1+m_2-g) \ (g-m_1) \ (g - m_2)}\\
&=-\sum_{g=\max(m_1,m_2)}^{m_1+m_2} \frac{(-1)^g}{g} \binom{g}{m_2}\binom{m_2}{g-m_1}\\
&\stackrel{\eqref{absorption}}{=}-\sum_{g=\max(m_1,m_2)}^{m_1+m_2} \frac{(-1)^g}{m_2} \binom{g-1}{m_2-1}\binom{m_2}{g-m_1}\\
&=\frac{-1}{m_2} \sum_{g=\max(m_1,m_2)}^{m_1+m_2} (-1)^g \binom{g-1}{m_2-1}\binom{m_2}{g-m_1}\\
&\overset{m_1>0}{=}\frac{-1}{m_2} \sum_{g\in\mathbb{Z}} (-1)^g \binom{m_2}{g-m_1}\binom{g-1}{m_2-1}\\
&\stackrel{*}{=} \frac{-1}{m_2} (-1)^{m_2-m_1} \binom{m_1-1}{-1} = 0,
\end{align*}
where for the second but last equality \(m_1>0\) is needed for the \(g=0\) summand not to contribute and
for the marked equality we used summation rule (5.24) of \cite{graham1994concrete}. 
So all the coefficients vanish that fulfil \(m_1,m_2\neq 0\). The sum for the remaining cases
is readily computed, since there is just one summand. Summarising we find

\begin{equation*}
\#K(m_1,m_2)= \delta_{m_2,0} \frac{(-1)^{1+m_1}}{m_1} + \delta_{m_1,0} \frac{(-1)^{1+m_2}}{m_2},
\end{equation*}

where the second summand can be ignored, since terms with \(m_1=0\) are irrelevant for our considerations.

 So the left hand side of 
\eqref{combinatorics total commutator} can be evaluated

\begin{align*}
\left[-\sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^g}{g} 
\binom{n}{\vec{b}} \prod_{l=1}^g T_{b_l}, a^\#\right]\\
= \sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|b|=n}} \frac{(-1)^{g+1}}{g} \binom{n}{\vec{b}}  a^\# \prod_{l=1}^g Z_{b_l},
\end{align*}

which is exactly equal to the right hand side of \eqref{combinatorics total commutator}. This ends the motivation of the conjecture.



\subsection{Solution to Recursive Equation}

So we found a recursive equation for the \(T_n\)s, now we need to solve it. 
In order to do so we need the following lemma about combinatorial distributions

\begin{Lemma}\label{stirling lemma}
For any \(g\in\mathbb{N},k\in\mathbb{N}\)
\begin{equation}
\sum_{\stackrel{\vec{g}\in\mathbb{N}^g}{|\vec{g}|=k}} \binom{k}{\vec{g}}=\sum_{l=0}^g (-1)^l (g-l)^k \binom{g}{l}
\end{equation}
holds. The reader interested in terminology may be eager to know, that the right hand side is equal to
 \(g!\) times the Stirling 
number of the second kind \(\left\{\begin{matrix}k\\g\end{matrix}\right\}\).
\end{Lemma}
\textbf{Proof:} We would like to apply the multinomial theorem but there are all the summands missing where at least
one of the entries of \(\vec{g}\) is zero, so we add an appropriate expression of zero. We also give the expression in
question a name, since we will later on arrive at a recursive expression.
\begin{multline}
F(g,k):=\sum_{\stackrel{\vec{g}\in\mathbb{N}^g}{|\vec{g}|=k}} \binom{k}{\vec{g}}
= \sum_{\stackrel{\vec{g}\in\mathbb{N}_0^g}{|\vec{g}|=k}} \binom{k}{\vec{g}}
- \sum_{\stackrel{\stackrel{\vec{g}\in\mathbb{N}_0^g}{|\vec{g}|=k}}{\exists l: g_l=0}} \binom{k}{\vec{g}}\\
= g^k 
 - \sum_{\stackrel{\stackrel{\vec{g}\in\mathbb{N}_0^g}{|\vec{g}|=k}}{\exists l: g_l=0}} \binom{k}{\vec{g}}
=g^k 
- \sum_{n=1}^{g-1} \sum_{\stackrel{\vec{g}\in\mathbb{N}_0^g}{|\vec{g}|=k}}
 \binom{k}{\vec{g}} 1_{\exists! i_1\dots i_n : \forall i_l\neq i_k \wedge \forall l :g_{i_l}=0}
\end{multline}
where in the last line the indicator function is to enforce there being exactly n different indices \(i_l\) for which \(g_{i_l}=0\)
holds. Now since it does not matter which entries of the vector vanish because the multinomial coefficient 
is symmetric and its value is identical to the corresponding multinomial coefficient where the vanishing entries
are omitted, we can further simplify the sum:

\begin{equation*}
F(g,k)= g^k -  \sum_{n=1}^{g-1} \binom{g}{n} \sum_{\stackrel{\vec{g}\in\mathbb{N}^n}{|\vec{g}|=k}}
 \binom{k}{\vec{g}}
\end{equation*}

The inner sum turns out to be \(F(g-n,k)\), so we found the recursive relation for \(F\):
\begin{equation}\label{combinatorics solution recursive}
F(g,k)= g^k -  \sum_{n=1}^{g-1} \binom{g}{n} F(n,k)= g^k -  \sum_{n=1}^{g-1} \binom{g}{n} F(g-n,k),
\end{equation}

where for the last equality we used the symmetry of binomial coefficients.
By iteratively applying this equation, we find the following formula, which we will now prove by induction

\begin{multline}\label{combinatorics induction}
\forall d\in\mathbb{N}_0: F(g,k)=\sum_{l=0}^d (-1)^l (g-l)^k \binom{g}{l}\\
+(-1)^{d+1} \sum_{n=1}^{g-d-1} \binom{n+d-1}{d} \binom{g}{n+d} F(g-d-n,k).
\end{multline}

We already showed the start of the induction, so what's left is the induction step. Before we do so the
following remark is in order: We are only interested in the case \(d=g\) and the formula seems meaningless
for \(d>g\); however, the additional summands in the left sum vanish, where as the right sum is empty
for these values of \(d\) since the  upper bound of the summation index is lower than its lower bound.

For the induction step, pick \(d\in\mathbb{N}_0\), use \eqref{combinatorics induction} and pull the first summand
 out of the second sum,
on this summand we apply the recursive relation \eqref{combinatorics solution recursive} resulting in

\begin{multline}
F(g,k)=\sum_{l=0}^d (-1)^l (g-l)^k \binom{g}{l}\\
 + (-1)^{d+1}\sum_{n=2}^{g-d-1} \binom{n+d-1}{d}\binom{g}{n+d} F(g-d-n,k)\\
  + (-1)^{d+1} \binom{d}{d} \binom{g}{d+1} F(g-d-1,k)\\
\overset{\eqref{combinatorics solution recursive}}{=}\sum_{l=0}^{d+1} (-1)^l (g-l)^k \binom{g}{l}\\
+(-1)^{d+1} \sum_{n=2}^{g-d-1} \binom{n+d-1}{d} \binom{g}{n+d} F(g-d-n,k)\\
-(-1)^{d+1} \binom{g}{d+1} \sum_{n=1}^{g-d-2} \binom{g-d-1}{n} F(g-d-1-n,k)\\
=\sum_{l=0}^{d+1} (-1)^l (g-l)^k \binom{g}{l}\\
+(-1)^{d+1} \sum_{n=1}^{g-d-2} \binom{n+d}{d} \binom{g}{n+d+1} F(g-d-1-n,k)\\
-(-1)^{d+1} \binom{g}{d+1} \sum_{n=1}^{g-d-2} \binom{g-d-1}{n} F(g-d-1-n,k).
\end{multline}
After the index shift we can combine the last two sums. 

\begin{multline}
F(g,k)= \sum_{l=0}^{d+1} (-1)^l (g-l)^k \binom{g}{l}\\
+ \sum_{n=1}^{g-d-2}\left[\binom{g}{d+1} \binom{g-d-1}{n} - \binom{n+d}{d} \binom{g}{n+d+1} \right] 
\\(-1)^{d+2} F(g-d-1-n,k).
\end{multline}


In order to combine the two binomials we reassemble \(\binom{g}{d+1}\binom{g-d-1}{n}\) 
into \(\binom{g}{n+d+1}\binom{n+d+1}{d+1}\), which can be seen to be possible by representing everything
in terms of factorials. This results in
\begin{multline}
F(g,k)= \sum_{l=0}^{d+1} (-1)^l (g-l)^k \binom{g}{l}\\
+(-1)^{d+2} \sum_{n=1}^{g-d-2}\left[\binom{n+d+1}{d+1} - \binom{n+d}{d}\right] \binom{g}{n+d+1} 
 F(g-d-1-n,k)\\
=\sum_{l=0}^{d+1} (-1)^l (g-l)^k \binom{g}{l}\\
+(-1)^{d+2}  \sum_{n=1}^{g-d-2} \binom{n+d}{d+1} \binom{g}{n+d+1}  F(g-d-1-n,k),
\end{multline}
where we used the addition formula for binomials:

\begin{equation}
\forall n\in \mathbb{C} \forall k \in \mathbb{Z}: \binom{n}{k} = \binom{n-1}{k} + \binom{n-1}{k-1}.
\end{equation}
This concludes the proof by induction. By setting \(d=g\) in equation \eqref{combinatorics induction} 
we arrive at the desired result. \qed

Using the previous lemma, we are able to show the next

\begin{Lemma}\label{combinatorics weak conjecture lemma 2}
For any \(k \in \mathbb{N}\backslash \{1\}\) the following equation holds
\begin{equation}
\sum_{g=1}^k \frac{(-1)^g}{g} \sum_{\stackrel{\vec{g}\in\mathbb{N}^g}{|\vec{g}|=k}}\binom{k}{\vec{g}}=0.
\end{equation}
\end{Lemma}
\textbf{Proof:} Let \(k\in\mathbb{N}\backslash\{1\}\), as a first step we apply lemma \ref{stirling lemma}.
We change the order of summation, use \eqref{absorption}, extend the range of summation and shift 
summation index  to arrive at

\begin{multline}
\sum_{g=1}^k \frac{(-1)^g}{g} \sum_{l=0}^g (-1)^l (g-l)^k \binom{g}{l}
= \sum_{g=1}^k \frac{1}{g} \sum_{l=0}^g (-1)^{g-l} (g-l)^k \binom{g}{g-l}\\
= \sum_{g=1}^k \sum_{p=0}^g (-1)^{p} p^k \frac{1}{g} \binom{g}{p}
=\sum_{g=1}^k \sum_{p=0}^g (-1)^{p} p^k \frac{1}{p} \binom{g-1}{p-1}\\
=\sum_{g=1}^k \sum_{p\in\mathbb{Z}} (-1)^{p} p^{k-1}\binom{g-1}{p-1}
=\sum_{p\in\mathbb{Z}} (-1)^{p} p^{k-1} \sum_{g=1}^k \binom{g-1}{p-1}\\
=\sum_{p\in\mathbb{Z}} (-1)^{p} p^{k-1} \sum_{g=0}^{k-1} \binom{g}{p-1}.
\end{multline}

Now we use equation (5.10) of \cite{graham1994concrete}:

\begin{equation}\tag{upper summation}
\forall m,n\in\mathbb{N}_0: \sum_{k=0}^n \binom{k}{m} = \binom{n+1}{m+1},
\end{equation}
which can for example be proven by induction on \(n\).

We furthermore rewrite the power of the summation index \(p\) in terms of the derivative of an 
exponential and change order summation and differentiation. This results in

\begin{multline*}
\sum_{g=1}^k \frac{(-1)^g}{g} \sum_{l=0}^g (-1)^l (g-l)^k \binom{g}{l}
=\sum_{p\in\mathbb{Z}} (-1)^{p} p^{k-1}  \binom{k}{p}\\
=\sum_{p=0}^k (-1)^{p} \left. \frac{\partial^{k-1}}{\partial \alpha^{k-1}} e^{\alpha p}\right|_{\alpha=0}  \binom{k}{p}
=\left. \frac{\partial^{k-1}}{\partial \alpha^{k-1}}  \sum_{p=0}^k (-1)^{p} e^{\alpha p} \binom{k}{p}\right|_{\alpha=0} \\
=\left. \frac{\partial^{k-1}}{\partial \alpha^{k-1}}  \left( 1-e^{\alpha p}\right)^k \right|_{\alpha=0} 
=(-1)^k \left. \frac{\partial^{k-1}}{\partial \alpha^{k-1}} \left( \sum_{l=1}^\infty \frac{(\alpha p)^l}{l!} \right) ^k \right|_{\alpha=0} \\
=(-1)^k \left. \frac{\partial^{k-1}}{\partial \alpha^{k-1}} ((\alpha p)^k + \mathcal{O} ((\alpha p)^{k+1}) ) \right|_{\alpha=0} =0.
\end{multline*}
\qed




We are now in a position to state the solution to the recursive equation \eqref{T_n recursive}
and motivate that it is in fact a solution. 

\begin{Conj}
For \(n\in\mathbb{N}\) the solution of the recursive equation \eqref{T_n recursive} 
solely in terms of \(G_a\) and \(C_a\) is given by

\begin{equation}\label{recursive solution}
T_n = \sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|= n}} \sum_{\vec{d}\in {\{0,1\}}^g} 
\frac{1}{g!} \binom{n}{\vec{b}} \prod_{l=1}^g F_{b_l,d_l},
\end{equation}
where \(F\) is given by
\begin{equation}\label{eq resursive weak solution}
F_{a,b} = \left\{ \begin{matrix}\mathrm\Gamma_a \quad \text{for } b=0 \\ C_a \quad \text{for } b=1  \end{matrix} \right._. 
\end{equation}
For the readers convenience we remind her, that \(\mathrm\Gamma_a\) and
the constants \(C_n\) are defined in theorem \ref{thm: T_n recursive}. 
\end{Conj}

\textbf{Motivation:} The structure of this proof will be induction over \(n\). For \(n=1\) the whole expression
on the right hand side collapses to \(C_1 + \mathrm\Gamma_1\), which we already know to be equal to \(T_1\). For
arbitrary \(n+1\in\mathbb{N}\backslash\{1\}\) we apply the recursive equation \eqref{T_n recursive}
once and use the induction hypothesis for all \(k\le n\) and thereby arrive at the rather convoluted 
expression

\begin{multline}\label{recursive weak conjecture proof}
T_{n+1} \stackrel{\eqref{T_n recursive}}{=} 
\mathrm\Gamma_{n+1} + C_{n+1}+ \sum_{g=2}^{n+1} \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n+1}} 
\frac{(-1)^g}{g} \binom{n+1}{\vec{b}} \prod_{l=1}^g T_{b_l}\\
\stackrel{\text{induction hyp}}{=} \mathrm\Gamma_{n+1} + C_{n+1}+ \sum_{g=2}^{n+1} \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n+1}} 
\frac{(-1)^g}{g} \binom{n+1}{\vec{b}} \prod_{l=1}^g \\
\sum_{g_l=1}^{b_l} \sum_{\stackrel{\vec{c}_l \in\mathbb{N}^{g_l}}{|\vec{c}_l|=b_l}} \sum_{\vec{e}_l \in \{0,1\}^{g_l}}
\frac{1}{g_l!} \binom{b_l}{\vec{c}_l} \prod_{k=1}^{g_l} F_{c_{l,k},e_{l,k}}.
\end{multline}

If we were to count the contributions of this sum to a specific product \(\prod F_{c_j,e_j}\) for some choice of 
\((c_j)_j, (e_j)_j\) we would first recognize that all the multinomial factors in \eqref{recursive weak conjecture proof}
combine to a single one whose indices are given by the first indices of all the \(F\) factors involved.
Other than this factor each contribution adds \(\frac{(-1)^g}{g} \prod_{l=1}^g \frac{1}{g_l!}\) to the sum. So we 
need to keep track of how many contributions there are and which distributions of \(g_l\) they belong to. 

Fix some product \(\prod F :=\prod_{j=1}^{\tilde{g}} F_{\tilde{b}_j,\tilde{d}_j}\). In the sum 
\eqref{recursive weak conjecture proof} we pick some initial short product of length \(g\) and split each
factor into \(g_l\) pieces to arrive at one of length \(\tilde{g}\) if the product is to contribute to
\(\prod F\). So clearly \(\sum_{l=1}^g g_l = \tilde{g}\) holds for any contribution to \(\prod F\). 
The reverse is also true, for any
\(g\) and \(g_1, \dots, g_g\in\mathbb{N}\) such that \(\sum_{l=1}^g g_l=\tilde{g}\) holds
the corresponding expression in \eqref{recursive weak conjecture proof} contributes to 
\(\prod F\). Furthermore \(\prod F\) and \(g\), \(g_1,\dots g_g\) is enough to uniquely
determine the summand of \eqref{recursive weak conjecture proof} the contribution
belongs to. For an illustration of this splitting see

\begin{align*}
\underbrace{\underbrace{F^1_{3,1} F^2_{2,0} F^3_{7,1}}_{g_1=3} \underbrace{F^4_{5,0}}_{g_2=1} \underbrace{F^5_{4,1} F^6_{2,1}}_{g_3=2} \underbrace{F^7_{1,1} F^8_{3,0} F^9_{4,1}}_{g_4=3} \underbrace{F^{10}_{4,1} F^{11}_{1,0}}_{g_5=2}}_{g=5}\\
 g_1+g_2+g_3+g_4+g_5=11=\tilde{g},
\end{align*}

where I labelled the factors in the upper right index for the readers convenience. We recognize
that the sum we are about to perform is by no means unique for each order of \(n\) but only 
depends on the number of appearing factors and the number of splittings performed on
them. By the preceding argument we need 

\begin{equation}\label{combinatorics weak conjecture reformulated into equation}
\sum_{g=2}^{\tilde{g}} \frac{(-1)^g}{g} \sum_{\stackrel{\vec{g}\in\mathbb{N}^g}{|\vec{g}|=\tilde{g}}} \prod_{l=1}^g \frac{1}{g_l!}
= \frac{1}{\tilde{g}!} 
\end{equation}


to hold for \(\tilde{g}>1\), in order to find agreement with the proposed solution \eqref{eq resursive weak solution}.
Now proving \eqref{combinatorics weak conjecture reformulated into equation} is done by 
realizing, that one can include the right hand side into the sum as the \(g=1\) summand, dividing
the equation by \(\tilde{g}!\) and using lemma \ref{combinatorics weak conjecture lemma 2}
with \(k=\tilde{g}\). The remaining case, \(\tilde{g}=1\), can directly be
read off of \eqref{recursive weak conjecture proof}. This ends the motivation of this conjecture.

\begin{Conj}\label{Corollary T_n by G's and C's}
For \(n\in\mathbb{N}\), \(T_n\) can be written as

\begin{equation}
\frac{1}{n!} T_n = \sum_{\stackrel{1\le c+g\le n}{c,g\in\mathbb{N}_0}} 
\sum_{\stackrel{\stackrel{\vec{g}\in\mathbb{N}^g}{\vec{c}\in\mathbb{N}^c}}{|\vec{c}| + |\vec{g}|=n}} 
\frac{1}{c! g!} \prod_{l=1}^c \frac{1}{c_l!} C_{c_l} \prod_{l=1}^g \frac{1}{g_l!} \mathrm\Gamma_{g_l}.
\end{equation}
Please note that for ease of notation we defined \(\mathbb{N}^0:= \{1\}\).
\end{Conj}
\textbf{Motivation:} By an argument completely analogous to the combinatorial argument in the motivation of conjecture
\eqref{thm: T_n recursive} we see that we can disentangle the \(F\)s in \eqref{recursive solution}
into \(\mathrm\Gamma\)s and \(C\)s if we multiply by a factor of \(\binom{c+g}{c}\) where \(c\) is the 
number of \(C\)s and \(g\) is the number of \(\mathrm\Gamma\)s giving

\begin{multline}
T_n = \sum_{\stackrel{1\le c+g\le n}{c,g\in\mathbb{N}_0}}
\sum_{\stackrel{\stackrel{\vec{g}\in\mathbb{N}^g}{\vec{c}\in\mathbb{N}^c}}{|\vec{c}| + |\vec{g}|=n}} 
\binom{c+g}{c} \frac{1}{(c+g)!} \binom{n}{\vec{g}\oplus \vec{c}}
\prod_{l=1}^c  C_{c_l} \prod_{l=1}^g \mathrm\Gamma_{g_l},
\end{multline}

which directly reduces to the equation we wanted to prove, by plugging in the multinomials in terms of
factorials. 

\begin{Conj}
As a formal power series, the second quantized scattering operator can be written in the form
\begin{equation}\label{Corollary double exp}
S= e^{\sum_{l\in\mathbb{N}} \frac{C_{l}}{l!}}
 e^{\sum_{l\in\mathbb{N}} \frac{\mathrm\Gamma_{l}}{l!}}.
\end{equation}
\end{Conj}
\textbf{Proof:} We plug conjecture \ref{Corollary T_n by G's and C's} into the defining Series for the \(T_n\)s
giving

\begin{align}
&S= \sum_{n\in\mathbb{N}_0} \frac{1}{n!} T_n \\
&=\id_{\mathcal{F}}+ \sum_{n\in\mathbb{N}} \sum_{\stackrel{1\le c+g\le n}{c,g\in\mathbb{N}_0}} 
\sum_{\stackrel{\stackrel{\vec{g}\in\mathbb{N}^g}{\vec{c}\in\mathbb{N}^c}}{|\vec{c}| + |\vec{g}|=n}} 
\frac{1}{c! g!} \prod_{l=1}^c \frac{1}{c_l!} C_{c_l} \prod_{l=1}^g \frac{1}{g_l!} \mathrm\Gamma_{g_l}\\
&=\id_{\mathcal{F}}+  \sum_{\stackrel{1\le c+g }{c,g\in\mathbb{N}_0}} 
\sum_{\stackrel{\vec{g}\in\mathbb{N}^g}{\vec{c}\in\mathbb{N}^c}} 
\frac{1}{c! g!} \prod_{l=1}^c \frac{1}{c_l!} C_{c_l} \prod_{l=1}^g \frac{1}{g_l!} \mathrm\Gamma_{g_l}\\
&=\sum_{c,g\in\mathbb{N}_0}
\sum_{\stackrel{\vec{g}\in\mathbb{N}^g}{\vec{c}\in\mathbb{N}^c}} 
\frac{1}{c! g!} \prod_{l=1}^c \frac{1}{c_l!} C_{c_l} \prod_{l=1}^g \frac{1}{g_l!} \mathrm\Gamma_{g_l}\\
&=\sum_{c\in\mathbb{N}_0} \frac{1}{c!} \sum_{\vec{c}\in\mathbb{N}^c} \prod_{l=1}^c \frac{1}{c_l!} C_{c_l}
\sum_{g\in\mathbb{N}_0} \frac{1}{g!} \sum_{\vec{g}\in\mathbb{N}^g} \prod_{l=1}^g \frac{1}{g_l!} \mathrm\Gamma_{g_l}\\
&=\sum_{c\in\mathbb{N}_0} \frac{1}{c!} \prod_{l=1}^c \sum_{k\in\mathbb{N}} \frac{1}{k!} C_{k}
\sum_{g\in\mathbb{N}_0} \frac{1}{g!}  \prod_{l=1}^g \sum_{b\in\mathbb{N}} \frac{1}{b!} \mathrm\Gamma_{b}\\
&=\sum_{c\in\mathbb{N}_0} \frac{1}{c!} \left( \sum_{k\in\mathbb{N}} \frac{1}{k!} C_{k}\right)^c
\sum_{g\in\mathbb{N}_0} \frac{1}{g!}  \left( \sum_{b\in\mathbb{N}} \frac{1}{b!} \mathrm\Gamma_{b}\right)^g\\
&=e^{\sum_{l\in\mathbb{N}} \frac{1}{l!} C_{l}} e^{\sum_{l\in\mathbb{N}} \frac{1}{l!} \mathrm\Gamma_{l}}.
\end{align}


\begin{Conj}
For \(A\) such that 
\begin{equation}
\|\id-U^A\|<1.
\end{equation}
The second quantized scattering operator fulfils
\begin{equation}\label{conj:sleek_second_quantised_scattering_operator}
S= e^{\sum_{n\in\mathbb{N}} \frac{C_n}{n!}} e^{\mathrm{d}\Gamma(\ln (U))}
\end{equation}
where \(C_n\) must be imaginary for any \(n\in\mathbb{N}\) in order to satisfy unitarity.

\end{Conj}
\textbf{Motivation:} 
First the remark about \(C_n \in i \mathbb{R}\) for any \(n\) is a direct consequence of 
the second factor of \eqref{conj:sleek_second_quantised_scattering_operator} begin unitary.
This in turn follows directly from \(\mathrm{d}\Gamma^* (K)=-\mathrm{d}\Gamma(K)\) for any \(K\) in the domain of \(\mathrm{d}\Gamma\).
That \(\ln U\) is in the domain of \(\mathrm{d}\Gamma\) follows from \( (\ln U)^*=\ln U^*=\ln U^{-1}=-\ln U\)
and \(\|U-\id\|<1\).

 We are going to change the sum in the second exponential of 
\eqref{Corollary double exp}, so let's take a closer look at that: by exchanging summation
we can step by step simplify

\begin{multline}
\sum_{l\in\mathbb{N}} \frac{\mathrm\Gamma_{l}}{l!}= 
\sum_{n \in\mathbb{N}} \frac{1}{n!} 
\mathrm{d}\Gamma\left( \sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^{g+1}}{g} 
\binom{n}{\vec{b}} \prod_{l=1}^g Z_{b_l}  \right)\\
=\mathrm{d}\Gamma\left( \sum_{n \in\mathbb{N}} \frac{1}{n!} 
 \sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^{g+1}}{g} 
\binom{n}{\vec{b}} \prod_{l=1}^g Z_{b_l}  \right)\\
=\mathrm{d}\Gamma\left( \sum_{n \in\mathbb{N}}
 \sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^{g+1}}{g} 
 \prod_{l=1}^g \frac{Z_{b_l}}{b_l!}  \right)\\
=\mathrm{d}\Gamma\left( 
 \sum_{g\in\mathbb{N}} \sum_{\vec{b}\in\mathbb{N}^g}\frac{(-1)^{g+1}}{g} 
\prod_{l=1}^g \frac{Z_{b_l}}{b_l!}  \right)\\
=\mathrm{d}\Gamma\left( 
 \sum_{g\in\mathbb{N}} \frac{(-1)^{g+1}}{g} 
\prod_{l=1}^g \left(\sum_{b_l\in\mathbb{N}} \frac{Z_{b_l}}{b_l!} \right) \right)\\
=\mathrm{d}\Gamma\left( 
 \sum_{g\in\mathbb{N}} \frac{(-1)^{g+1}}{g} 
 \left(\sum_{b\in\mathbb{N}} \frac{Z_{b}}{b!} \right)^g \right)\\
 =\mathrm{d}\Gamma\left( 
 \sum_{g\in\mathbb{N}} \frac{(-1)^{g+1}}{g} 
 \left(U-\id \right)^g \right)
 =\mathrm{d}\Gamma\left( -
 \sum_{g\in\mathbb{N}} \frac{1}{g} 
 \left(\id-U \right)^g \right)\\
  =\mathrm{d}\Gamma\left( 
 \ln \left(\id-\left(\id-U\right) \right) \right)
 =\mathrm{d}\Gamma\left( \ln \left(U\right)\right).
\end{multline}



The last conjecture is proven directly in section \ref{sec:proof simple formula}



%\chapter{Bibliography concerning the state of the art, the research objectives, and the work program} 
%\vspace*{-0.68cm}

%\begingroup
%\renewcommand{\section}[2]{}
\bibliographystyle{amsplain}
\bibliography{ref}



\end{document}

