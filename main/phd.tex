%\documentclass[oneside,reqno,12pt]{amsart}
\documentclass[b5paper,draft,openbib,12pt]{memoir} 
%\documentclass[b5paper,openbib,12pt]{memoir} 
%check whether openbib option is necessary
%option final will remove some markings


%\usepackage{fontspec}

%\usepackage[a4paper, top=2.7cm, bottom=2.7cm]{geometry}


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{bbm}
\usepackage{graphicx}
\usepackage{slashed}
\usepackage{eurosym}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{longtable}
\usepackage[mathscr]{eucal}


%commutative diagram
\usepackage{amsmath,amscd}
%picture
\usepackage{wrapfig}

\usepackage[unicode=true, pdfusetitle, bookmarks=true,
  bookmarksnumbered=false, bookmarksopen=false, breaklinks=true, 
  pdfborder={0 0 0}, backref=false, colorlinks=true, linkcolor=blue,
  citecolor=blue, urlcolor=blue]{hyperref}
\hypersetup{final}
%needed to have hyperlinks in draft mode


% \numberwithin{equation}{section}
\allowdisplaybreaks[1]

\newtheorem{axiom}{Axiom}
\newtheorem{Def}{Definition}[section]
\newtheorem{Conj}[Def]{Conjecture}
\newtheorem{Thm}[Def]{Theorem}
\newtheorem{Prp}[Def]{Proposition}
\newtheorem{Lemma}[Def]{Lemma}
\newtheorem{lemma}{Lemma}
\newtheorem{Remark}[Def]{Remark}
\newtheorem{Corollary}[Def]{Corollary}
\newtheorem{Example}[Def]{Example}
\newtheorem{Assumption}[Def]{Assumption}


\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\supp}{supp}


\newcommand{\id}{{\mathbbm 1}}
\newcommand{\equaltext}[1]{\ensuremath{\stackrel{\text{#1}}{=}}}
\newcommand{\letext}[1]{\ensuremath{\stackrel{\text{#1}}{\le}}}
\newcommand{\Conv}{\mathop{\scalebox{1.7}{\raisebox{-0.2ex}{\(\ast\)}}}}
\newcommand{\CONV}{\mathop{\scalebox{3.0}{\raisebox{-0.2ex}{\(\ast\)}}}}

% Annotations
%\usepackage[normalem]{ulem}
% \usepackage{refcheck}
\usepackage[colorinlistoftodos,shadow,textsize=scriptsize,textwidth=2.75cm]{todonotes}
\newcommand{\noch}[1]{ \todo[color=blue!20]{Todo: #1} }
\newcommand{\black}{ \color{black} }


\renewcommand\chapterheadstart{
\vspace *{\beforechapskip}
\hrulefill
\vskip 0pt
}

\renewcommand\afterchaptertitle{%
\vskip 0pt
\hrulefill
\par \nobreak  \vskip  \afterchapskip  } 
%  \hrulefill}
%  \m@mindentafterchapter\@afterheading}
%\makeatother

\setsecnumdepth{all}


%all divisions are numbered in the text body

\parindent 0cm

\begin{document}



\frontmatter
%
%\noindent
%\begin{center}
%    \textsc{ \small{Description of the dissertation project of Markus Nöth,
%        Ludwig-Maximilians University of Munich} \\
%        \smallskip
%\large{ Electron-Positron Pair Creation in External Fields} \\
%\small{Rigorous Control of the Scattering-Matrix Expansion}}
%    \vskip.3cm
%    \small
% supervisor:    D.-A.\ Deckert (LMU)
% \vskip0.4cm
\title{Electron-Positron Pair Creation in External~Fields}
%\subtitle{\footnotesize{Rigorous Control of the Scattering-Matrix Expansion}}
\author{M. Nöth}
\maketitle

 \begin{abstract}
 In this project we investigate the phenomenon of creation of matter-antimatter
pairs of particles, more precisely electron-positron pairs, out of the vacuum
subject to strong external electromagnetic fields. 
Although this phenomenon was predicted already in 1929 and was
observed in many experiments, its rigorous
mathematical description still lies
at the frontier of human understanding of nature. 
Dirac introduced the heuristic description of the vacuum of quantum electrodynamics (QED) as a homogeneous sea of particles. Although the picture of pair creation as lifting a particle out of the Dirac sea, leaving a hole in the sea, is very
explanatory the mathematical formulation of the time evolution of the Dirac sea
faces many mathematical challenges. 
A straightforward interaction of sea particles and the radiation field is ill-defined and physically important
quantities such as the total charge current density are badly divergent due to the
infinitely many occupied states in the sea. 
Nevertheless, in the last century physicists and mathematicians
 have developed strong methods called
``perturbative renormalisation theory'' that allow at least to treat the
scattering regime perturbatively. %Though mathematically not well understood,
Non-perturbative methods have to be developed in order to give an adequate theoretical description of upcoming next-generation experiments allowing a study of the time evolution.
Our endeavour focuses on the so-called \emph{external
field model of QED} in which one neglects the interaction between the sea
particles and only allows an interaction with a prescribed electromagnetic field.
It is the goal of this project to develop the necessary non-perturbative
methods in this model to give a rigorous construction of the scattering
and the time evolution operator. 
\\
\smallskip
\noindent \textbf{Keywords:} Second Quantised Dirac Equation, non-perturbative QED, external-field QED, Scattering Operator
 \end{abstract}


%\vskip.5cm
%\thispagestyle{empty}


\tableofcontents

\newpage



\mainmatter

\chapter{Introduction}

\show\afterchaptertitle
\noch{Historische Einleitung durch Anfänge relativistischer Quantenphysik, zweitquantisierung Ruijsnaars Resultat und ivp0. Falls möglich Verbindung zur Physikliteratur. Falls möglich Resultat zur Bestimmung der Phase und Analytizität. }

\chapter[Nonperturbative \(S\)]{Nonperturbative discussion of the Scattering Operator}
As we have seen in the last chapter, straightforwardly lifting the one particle dynamics to Fock Space leads to difficulties whenever the vector part of the four potential is nonzero. Clearly this is quite devastating for the approach, but even more the result does not respect gauge symmetry, a symmetry of the physical system. This fact tells us, that our description of the physical system as an element of Fock space needs extra restraints, which are purely artefacts of our particular treatment. 

Inspired by this, we take a closer look at the construction of Fock space, we closely follow \cite{ivp0}. 

\chapter[Axiomatic Construction of Scattering Operator][Construction of \(S\)]{Axiomatic Construction of Scattering Operator} \label{sec:workprogram}

\noch{Starte von vorne, mache dies klar. Nehme 1-Teilchen stuff  und Axiome an, versuche Wohldefiniertheit zu zeigen. Motiviere Axiome durch Eigenschaften der 1-Teilchen Operatoren. Schreibe Induktionsschema auf.}

In order to be able to state our main conjecture \eqref{main_result} precisely
I will need to introduce the one-particle dynamics for electrons and their
scattering operator, see section \ref{sec:one-particle} below, and then move on
to the second quantised dynamics and its corresponding scattering operator  in
section \ref{sec:second quant}. Second quantization is the canonical
method of turning a one-particle theory into one of an arbitrary and
possibly changing number of particles. The informal
series expansion of the one-particle scattering operator \(U\) is derived 
from Dirac's equation of motion for the electron. In section
 \ref{sec:well-def} the convergence of this
expansion is shown. The informal expansion of the second quantized
scattering operator $S$ is then derived from $U$ by second quantisation in
section \ref{sec:second quant}. At this point I have gathered enough tools to present the main conjecture \ref{main_result} in section \ref{sec:main result}. 
After the main conjecture is known, I present several of my own results in sections \ref{sec: odd orders}, \ref{sec: first order} and \ref{sec: second order} about the first order, the second order and all other odd orders. 
These results give an intuition on how to control the convergence of the informal expansion of the scattering operator \(S\).

%\Dirk{gibt kurz die Motivation: wir wollen zur Conjecture, dazu
%Einteilchentheorie wichtig für die    Dirac-Seetheorie, wird also zuerst
%besprochen, danach kommen wir zur    zweit-quant. (führe den Begriff ein)}


\section{Defining One-Particle Scattering-Matrix}\label{sec:one-particle}


%The goal of this and the next section is to introduce some results, which are
%known to the community and form the basis of my work. 
In order to introduce the one-particle dynamics I introduce Diracs equation \eqref{dirac} and reformulate it in integral form in equation \eqref{dirac_integral}. By iterating this equation we will naturally be led to the informal series expansion of the scattering operator equation \eqref{DefU}, whose convergence is discussed in the next section. 
%\Dirk{ordne es anders: wir wollen $S$, dazu Zeitentwicklung $U$, dazu Dirac
%Gleichung, Integralversion, formale Iteration, Konvergenz}
%\Markus{So? Oder wolltest du, dass ich auch die Mathematik umordne?} Gut so.

Throughout this thesis I will consider four-potentials $A, F$ or \(G\) to be smooth functions
in \(C_{c}^\infty(\mathbb{R}^4)\otimes \mathbb{C}^4\), where the index \(c\)
denotes that the elements have compact support. Also throughout this thesis I
will denote by \(A, F\) and \(G\) some arbitrary but fixed four-potentials. The Dirac
equation for a wave function \(\phi \in L^2(\mathbb{R}^3)\otimes \mathbb{C}^4\)
is
\begin{equation}\label{dirac}
0= (i\slashed{\partial}-e\slashed{A}-m \id) \phi,
\end{equation}
where \(m\) is the mass of the electron, \(\id: \mathbb{C}^4\rightarrow \mathbb{C}^4\) is the identity  and crossed out letters mean that their four-index is contracted with Dirac matrices
\begin{equation}
\slashed{A}:= A_\alpha \gamma^\alpha,
\end{equation}
where Einstein's summation convention is used. These matrices fulfil the anti-commutation relation
\begin{equation}
\forall \alpha, \beta \in \{0,1,2,3\}:\{\gamma^\alpha, \gamma^\beta\}:= \gamma^\alpha \gamma^\beta+ \gamma^\beta \gamma^\alpha= g^{\alpha \beta},
\end{equation}
where \(g\) is the Minkowski metric. I work with the \(+---\) metric signature and the Dirac representation of this algebra. Squared four dimensional objects always refer to the Minkowski square, meaning for all \(a\in \mathbb{C}^4\), \(a^2:= a^{\alpha} a_{\alpha}\). 

In order to define Lorentz invariant measures for four dimensional integrals I employ the same notation as in \cite{ivp1}. The standard volume form over \(\mathbb{R}^4\) is denoted by \(\mathrm{d}^4 x= \mathrm{d}x^0 \mathrm{d}x^1\mathrm{d}x^2 \mathrm{d}x^3\), the product of forms is understood as the wedge product. The symbol \(\mathrm{d}^3x\)  means the 3-form \(\mathrm{d}^3x= \mathrm{d}x^1\mathrm{d}x^2\mathrm{d}x^3\) on \(\mathbb{R}^4\). Contraction of a form \(\omega\) with a vector \(v\) is denoted by \(\mathfrak{i}_v(\omega)\). The notation \(\mathfrak{i}_v (\omega)\) is also used for the spinor matrix valued vector \(\gamma=(\gamma^0,\gamma^1,\gamma^2,\gamma^3)=\gamma^\alpha e_\alpha\):
\begin{equation}
\mathfrak{i}_\gamma (\mathrm{d}^4x) := \gamma^\alpha \mathfrak{i}_{e_\alpha}(\mathrm{d}^4 x),
\end{equation} 

with \((e_\alpha)_{\alpha}\) being the canonical basis of \(\mathbb{C}^4\). Let \(\mathcal{C}_A\) be the space of solutions to \eqref{dirac} which have compact support on any spacelike hyperplane \(\Sigma\). Let \(\phi, \psi\) be in \(\mathcal{C_A}\), the scalar product \(\langle \cdot, \cdot\rangle\) of elements of \(\mathcal{C}_A\) is defined as

\begin{equation}
\langle \phi, \psi \rangle := \int_{\Sigma} \overline{\phi (x)} \mathfrak{i}_{\gamma} (\mathrm{d}^4x) \psi (x)=: \int_{\Sigma} \phi^\dagger (x) \gamma^0 \mathfrak{i}_{\gamma} (\mathrm{d}^4x) \psi (x) .
\end{equation}
Furthermore define \(\mathcal{H}\) to be \(\mathcal{H}:=\overline{\mathcal{C}_A}^{\langle \cdot, \cdot \rangle}\). The mas-shell \(\mathcal{M}\subset \mathbb{R}^4\) is given by
\begin{equation}
\mathcal{M}=\{p\in \mathbb{R}^4\mid p^2=m^2\}.
\end{equation}
The subset \(\mathcal{M}^+\) of \(\mathcal{M}\) is defined to be \(\mathcal{M}^+:=\{p\in\mathcal{M}\mid p^0>0\}\). The image  of \(\mathcal{H}\) by the projector \(1_{\mathcal{M^+}}\), given in momentum space representation, is denoted by \(\mathcal{H}^+\) and its orthogonal complement by \(\mathcal{H}^-\). 
I introduce a family of Cauchy hypersurfaces \((\Sigma_t)_{t\in\mathbb{R}}\) governed by a family of normal vector fields \((\left.v_t n\right|_{\Sigma_t})\), where \(n: \mathbb{R}^4 \times \mathbb{R} \rightarrow \mathbb{R}^4\) and \(v: \mathbb{R}^4 \times \mathbb{R} \rightarrow \mathbb{R}\) are smooth functions. For \(x\in\Sigma_t\) the vector \(n_t(x)\) denotes the future directed unit-normal vector to \(\Sigma_t\) at \(x\) and \(v_t(x)\) the corresponding normal velocity of the flow of the Cauchy surfaces. 

Now we have the tools to recast the Dirac equation into an integral version
which will allow me to define the scattering operator. 
Let \(\psi \in \mathcal{C}_A\), for any \(t\in\mathbb{R}\) I denote by \(\phi_t\) the solution to the free Dirac equation, that is equation \eqref{dirac} with \(A=0\), with \(\left.\psi\right|_{\Sigma_t}\) as initial condition on \(\Sigma_t\). Let \(t_0\in\mathbb{R}\) have some fixed value, equation \eqref{dirac} can be reformulated, c.f. theorem 2.23 of \cite{ivp1}, as
\begin{multline}\label{dirac_integral}
\phi_t(y)=\phi_{t_0}(y)
-i \int_{t_0}^t \text{d}s \int_{\Sigma_s}\int_{\mathcal{M}}\frac{\slashed{p}+m}{2m^2}e^{ip(x-y)}\mathfrak{i}_p(\text{d}^4p) \frac{\mathfrak{i}_{\gamma}(\text{d}^4x)}{(2\pi)^{3}}\\
v_s(x)\slashed{n}_s(x) \slashed{A}(x)\phi_s(x),
\end{multline}
which holds for any \(t\in\mathbb{R}\). Employing the following rewriting of integrals

\begin{equation}
\int_{\mathcal{M}}\frac{\slashed{p}+m}{2 m^2} f(p) \mathfrak{i}_p(\text{d}^4p)=\frac{1}{2\pi i}   \left( \int_{\mathbb{R}^4-i \epsilon e_0}-\int_{\mathbb{R}^4+i \epsilon e_0} \right) (\slashed{p}-m)^{-1} f(p)  \text{d}^4p,
\end{equation}
which is due to the theorem of residues, equation \eqref{dirac_integral} assumes the form

\begin{multline}
\phi_t(y)=\phi_{t_0}(y)
- \int_{[t_0,t]\times\mathbb{R}^3}  \left( \int_{\mathbb{R}^4-i \epsilon e_0}-\int_{\mathbb{R}^4+i \epsilon e_0} \right)\\
 (\slashed{p}-m)^{-1} e^{ip(x-y)}  \text{d}^4p  \frac{\text{d}^4x}{(2\pi)^{4}}\slashed{A}(x)\phi_s(x). 
\end{multline} 
In the last expression I picked all hypersurfaces \(\Sigma_s\) to be equal time
hyperplanes such that \(v_s=1\) and \(\slashed{n}_s=\gamma^0 e_0\). We identify
the advanced and retarded Greens functions of the Dirac equation:
\begin{equation}
\Delta^\pm (x):= \frac{-1}{(2\pi)^4} \int_{\mathbb{R}^4\pm i \varepsilon e_0} \frac{\slashed{p}+ m}{p^2-m^2} e^{-ipx} d^4 p,
\end{equation}
yielding
\begin{multline}\label{dirac integral split}
\phi_t(y)=\phi_{t_0}(y)
+ \int_{[t_0,t]\times\mathbb{R}^3}  (\Delta^--\Delta^+)(y-x)  \text{d}^4x \slashed{A}(x)\phi_s(x). 
\end{multline} 

 Iteratingequation \eqref{dirac integral split} and picking \(t\) in the future of
\(\supp A\) and \(t_0\) in the past of it, denoting them by \(\pm \infty\) since 
their exact value is no longer important, the following series expansion is 
obtained informally

\begin{equation}\label{DefU}
\phi_\infty(y)=U^A \phi_{-\infty} :=\sum_{k=0}^\infty Z_k(A) \phi_{-\infty},
\end{equation} 
with \(Z_0=\id \), the identity on \(\mathbb{C}^4\), and where for arbitrary \(\phi\in \mathcal{H}\), \(Z_k \) is defined as
\begin{align*}
&Z_k(A)\phi(y):= \int_{\mathbb{R}^4}  (\Delta^--\Delta^+)(y-x_1)  \text{d}^4x_1 \slashed{A}(x_1) \\
&\prod_{l=2}^k \left[\int_{[-\infty,x^0_{l-1}]\times\mathbb{R}^3}  (\Delta^--\Delta^+)(x_{l-1}-x_l)   \slashed{A}(x_l)\text{d}^4x_l\right]
 \phi(x_k).
\end{align*}
Now since the integration variables are time ordered and \(\supp \Delta^\pm \subseteq \text{Cau}^\pm\) 
in every one but the first factor the contribution of \(\Delta^-\) vanishes. Therefore we can simply \noch{führe Cau als kausale Menge ein}
drop it. Furthermore we may continue the integration domain to all of \(\mathbb{R}^4\), since
there \(\Delta^+\) gives no contribution, giving
\begin{multline}\label{Z_kDelta}
Z_k(A)\phi(y)= (-1)^{k-1}\int_{\mathbb{R}^4}\text{d}^4x_1  (\Delta^--\Delta^+)(y-x_1)   \slashed{A}(x_1) \\
\prod_{l=2}^k \left[\int_{\mathbb{R}^4}\text{d}^4x_l \Delta^+(x_{l-1}-x_l)   \slashed{A}(x_l)\right]
 \phi(x_k).
\end{multline}


 This is convenient, because we may now use the spacetime
integration with the exponential factor of the definition of \(\Delta^-\) as a Fourier transform
acting on the four-potentials and the wave function. 
Undoing the substitutions again for the first factor and executing the just mentioned Fourier
transforms using the convolution theorem inductively results in

\begin{multline}\label{explicit_Zk}
Z_k(A)\phi(y) =- i  \int_{\mathcal{M}}\frac{\mathfrak{i}_p(\text{d}^4p_1)}{(2\pi)^{3}} \frac{\slashed{p}_1+m}{2m} e^{-ip_1y}  \\
  \prod_{l=2}^{k} \left[ \int_{\mathbb{R}^4+i \epsilon e_0}\frac{\text{d}^4p_l}{(2\pi)^{4}} \slashed{A}(p_{l-1}-p_l)  (\slashed{p}_l-m)^{-1}  
 \right]\\
 \int_{\mathcal{M}}  \mathfrak{i}_p(\text{d}^4p_{k+1})\slashed{A}(p_{k}-p_{k+1})\hat{\phi}(p_{k+1}).
\end{multline}

Due to the representation \eqref{Z_kDelta} one may also represent \(Z_k\) in terms of The operators
\begin{align}
\Delta^0:= \Delta^+-\Delta^-\\
L_A^{\pm,0}:= \Delta^{\pm,0} \ast \slashed{A}
\end{align}
in this manner
\begin{equation}
Z_k(A)\phi(y)= (-1)^{k} L^0_A \left({L_A^+}^{k-1}( \phi)\right) (y),
\end{equation}
where the upper right index for an operator means iterative application of said operator.

\subsection{Well-definedness of \(U\) }\label{sec:well-def}

I will outline in this section how to prove that the informally inferred series
expansion of \(U\) in \eqref{DefU} is well-defined, i.e. that the series
 converges. In doing so it is crucial to find appropriate bounds
on the summands of said series. 
The domain of integration of the temporal variables in the iterated form of
equation \eqref{dirac_integral} is a simplex. The volume of this simplex is
related to the volume of the cube by the factor \(n!\), using this one usually
introduces the time ordering Operator and the factor of \(\frac{1}{n!}\). This
line of argument has been translated into the momentum space, which might turn
out to be more convenient for proving the main conjecture. 

Using Parsevals theorem one translates the operators \(Z_k\) into momentum space, then one applies standard approximation techniques and the theorem of Paley and Wiener and Youngs inequality for convolution operators. Next one minimizes with respect to the arbitrary \(\epsilon\) in the equation \eqref{explicit_Zk}, which can be done due to the rules for changing the contour of integration of analytic functions. The estimate is valid only for \(k>1\), it is given by
\begin{equation}
\|Z_k(A)\|\le \left(\frac{1}{\sqrt{2} }\right)^{k-1} \frac{\left(e a\right)^{k-1}}{(k-1)^{k-1}} \frac{ C_N^k}{\pi^{4k+2}8}  f^{k-1}  g,
\end{equation}
where \(C_N>0\) is a constant obtained by application of the theorem of Paley and Wiener (it can for example be found in \cite{reed1975methods}). In order to simplify the notation I used \(a:=\text{diam}(\text{supp}(A))\), \(f:=\left\|\frac{1}{(1+|\cdot |)^N} \right\|_{\mathcal{L}^1(\mathbb{R}^4, \mathrm{d}^4x)}\), \(g:=\left\|\frac{1}{(1+|\cdot |)^N} \right\|_{\mathcal{L}^1(\mathcal{M},\mathfrak{i}_p \mathrm{d}^4p)} \) and \(e\) being Euler's number. By \(\mathcal{L}^1(E, \mathrm{d}\mu )\) I denote the space of functions with domain of definition \(E\) which are integrable with respect to the measure \(\mathrm{d}\mu\), i.e.
\begin{equation}
\mathcal{L}^1(E, \mathrm{d}\mu ):=\{ \psi: E\rightarrow \mathbb{C}  \mid \int_E \|\psi (x)\| \mathrm{d}\mu (x)< \infty \}.
\end{equation}

For the operator norm of \(Z_1 (A)\) the bound
\begin{equation}
\|Z_1(A)\|\le \left\| \|\slashed{A}\|_{spec} \right\|_{\mathcal{L}^1(\mathcal{M})}
\end{equation}

can be found more easily. It is finite, because in position space \(A\) is compactly supported, which means that at infinity its Fourier transform falls off faster than any polynomial.
Some lengthy calculations and the use of the well known bound on the factorial \(n!\le \sqrt{2\pi n} \left(\frac{n}{e}\right)^n e^\frac{1}{12n}\)  result in the following bound for the series representing the operator \(U\)
\begin{multline}
\|U^A\|=\left\|\sum_{k=0}^\infty Z_k(A) \right\|
\le  1+ \left\| \|\slashed{A}\|_{spec} \right\|_{\mathcal{L}^1(\mathcal{M})} + f g \frac{a    C^2_N }{\pi^{\frac{19}{2}}4}
 e^{\frac{a C_N f}{\pi^4\sqrt{2}}+\frac{1}{12}} <\infty.
\end{multline}
The series representing \(U^A\) therefore converges, so it gives rise to a well defined operator.


\section{Construction of the Second Quantised Scattering-Matrix}\label{sec:second quant}
The main objective of my thesis is to do the analogous proof of section
\ref{sec:well-def} in the second quantised case, %\Dirk{Begriff second-quant.
%einführen}
i.e. to prove conjecture
\ref{main_result}. For doing so we have gathered a lot of tools from the
one-particle theory. In the following I outline how the construction of the
second quantised scattering operator is to be carried out, we will naturally be
led to an informal power series representation for the scattering operator \(S\). This time the construction is more delicate, so I will consider different kinds of terms of the expansion using different techniques. 
I will first consider all odd orders in the expansion in section \ref{sec: odd orders}, 
then mention additional results about the first order in section \ref{sec:
first order} and move on to the second order in section \ref{sec: second
order}. The control of the orders greater than two are outstanding and forms the main
part of the work in this project. In section \ref{sec:main result} below I
will give arguments why the necessary control for the convergence can be
achieved.

First I fix some more notation. Using the space of solutions of the Dirac equation \(\mathcal{H}\)
%\Dirk{am besten schon oben einführen}
one constructs Fock space in the following way

\begin{equation}
\mathcal{F}:=\bigoplus_{m,p=0}^\infty \left(\mathcal{H}^+ \right)^{\Lambda m} \otimes \left(\overline{\mathcal{H}^- }\right)^{\Lambda p},
\end{equation}
where the bar denotes complex conjugation and \(\Lambda\) in the exponent
denotes that only elements which are antisymmetric with respect to permutations
are allowed. The Factor \(\left(\mathcal{H}^{\pm}\right)^0\) is understood as
\(\mathbb{C}\). I will denote the sectors of Fock space of fixed particle
numbers by \(\mathcal{F}_{m,p}:= \left(\mathcal{H}^+ \right)^{\Lambda m}
\otimes \left(\overline{\mathcal{H}^-} \right)^{\Lambda p}\). The element of
\(\mathcal{F}_{0,0}\) of norm \(1\) will be denoted by \(\Omega\).  The
annihilation operator \(a\) acts on an arbitrary sector of Fock space
\(\mathcal{F}_{m,p}\), for any \(m,p\in\mathbb{N}_0\) as

\begin{equation}
\begin{aligned}
a: &\mathcal{H}\otimes \mathcal{F}_{m,p} \rightarrow \mathcal{F}_{m-1,p}\oplus \mathcal{F}_{m,p-1}\\
&\phi \otimes \alpha \mapsto \langle \mathrm{P}_+ \phi (x),\alpha (x,\cdot,\dots)\rangle_x + \langle \mathrm{P}_- \phi (x),\alpha (\cdot,\dots, \cdot, x)\rangle_x,
\end{aligned}
\end{equation}
where \(\langle, \rangle_x\) denotes that the scalar product of \(\mathcal{H}\) is to be taken with respect to \(x\) and \(\mathrm{P}_{\pm}\) denotes the projector onto \(\mathcal{H}^+\) and \(\mathcal{H}^-\) respectively. The vacuum sector is mapped to the zero element of Fock space. 

Now we turn to the construction of the \(S\)-matrix, the second quantised analogue of \(U\). This construction is carried out axiomatically. The first axiom makes sure that the following diagram, and the analogue for the adjoint of the annihilation operator commute.
\begin{equation}
\begin{CD}								%heuristics with infinite wedge space?
\mathcal{F}     @>S^A>>  \mathcal{F}\\
@AAaA        @AAaA\\
\mathcal{H}\otimes \mathcal{F}     @>U^A\otimes S^A>>  \mathcal{H}\otimes \mathcal{F} 
\end{CD}
\end{equation}
\begin{axiom}
The \(S\) operator fulfils the ``lift condition''.
\begin{align}\label{lift_condition1}
\forall \phi\in \mathcal{H}:& \hspace{0.5cm} S^A \circ a(\phi)=a\left( U^A \phi \right)  \circ S^A,\tag{lift condition}\\
\forall \phi\in \mathcal{H}:& \hspace{0.5cm} S^A \circ a^*(\phi)=a^*\left( U^A \phi \right)  \circ S^A,\tag{adjoint lift condition}
\end{align}
where \(a^*\) is the adjoint of the annihilation operator, the creation operator. 
\end{axiom}
The scattering operator is then expanded in an informal 
power series

\begin{equation}\label{S_expansion}
S^A=\id_\mathcal{F}+\sum_{l=1}^\infty \frac{1}{l!} T_l(A).
\end{equation}
In order to fully characterise \(S^A\) it is enough to characterise all of the \(T_l\) operators. For \(k\in \mathbb{N}\)
the operators \(T_k (A)\) are also defined for \(k\) non-identical arguments by homogeneity of
\(T_k(A)\) to be symmetric in its arguments. For ease of notation we define \(T_0:=\id_\mathcal{F}\).
Using the \eqref{lift_condition1} one can easily derive commutation relations for the operators 
\(T_m\), which are given by

\begin{equation}
\left[T_m(A) , a^\# (\phi)\right]= \sum_{j=1}^{m} \begin{pmatrix} m \\ j \end{pmatrix} a^\# \left(Z_j (A) \phi \right) T_{m-j}(A), \label{Tk_commutation_relation}
\end{equation}
where \(a^\#\) is either \(a\) or \(a^*\).
The matrix elements of the expansion coefficients \(T_l\) of \eqref{S_expansion} can therefore be constructed from the matrix elements of the lower expansion coefficients \(T_k\) with \(k<l\) and the vacuum expectation value of \(T_l\). As will be shown in section \ref{sec: odd orders}, the vacuum expectation value of all odd orders can naturally be chosen to zero, due to charge conjugation symmetry. I will be using the method of Eppstein and Glaser (see \cite{epstein1973role, scharf2014finite})  to find the vacuum expectation value of the even orders.

Besides the scattering operator I will also need the expansion coefficients of its adjoint. 
\begin{equation}\label{S_adjoint}
\left(S^A\right)^*=\id_\mathcal{F}+\sum_{l=1}^\infty \frac{1}{l!} \tilde{T}_l(A)
\end{equation}

Since the scattering operator has to be unitary, it is not difficult to find the following expression for the coefficients of its adjoint

\begin{equation}
\forall m>0:\quad \sum_{k=0}^m \begin{pmatrix}
m\\k
\end{pmatrix} T_{m-k} (A) \tilde{T}_{k}(A)=0.
\end{equation}

Thus to find the adjoint coefficient of order \(n\), it suffices to know the coefficients of \(S\) itself up to order \(n\). 

\section{Construction of Recursive Equation for \(T_m\)}

In the following I derive a recursive equation for the coefficients of the expansion 
of the second quantized scattering operator. The starting point of this derivation is 
the commutator of \(T_m\), equation \eqref{Tk_commutation_relation_creator}.

\subsection{Heuristics}

Why at this point one might suspect that such a representation exists is, because
looking at equation \eqref{Tk_commutation_relation_creator} for a while, one
comes to the conclusion that if one replaces \(T_m\) by 
\begin{equation}
T_m - \frac{1}{2} \sum_{k=1}^{m-1} \begin{pmatrix} m \\ k\end{pmatrix} T_k T_{m-k},
\end{equation}
no \(T_k\) with \(k>m-2\) will occur on the right hand side of the resulting equation.
So if one subtracts the right polynomial in \(T_k\) for suitable \(k\) one might achieve
a commutator which contains only the creation respectively annihilation operator 
concatenated with some one particle operator. From our treatment of \(T_1\)
\noch{place proper reference to definition of G operator} we know which
operators have such commutation relations. 

So having this in Mind we start with the ansatz

\begin{equation}\label{Def Gamma_m}
\Gamma_m := \sum_{g=2}^m \sum_{\stackrel{b\in\mathbb{N}^g}{|b|=n}} c_{b} \prod_{k=1}^g T_{b_k}.
\end{equation}

Now in order to show that \(T_m\) and \(\Gamma_m\) agree up to operators which have a commutation 
relation of the form \eqref{G commutator}, we calculate \(\left[ T_m-\Gamma_m, a^\#(\varphi_n) \right]\)
for arbitrary \(n\in\mathbb{Z}\) and try to choose the coefficients \(c_b\) of \eqref{Def Gamma_m}
such that all contributions vanish which do not have the form \(a^\# \left( \prod_k Z_{\alpha_k}\right)\)
for any suitable \((\alpha_k)_k\subset \mathbb{N} \). If one does so, one is led to a system of equations
of which I wrote down a few to give an overview of its structure. The objects \(\alpha_k, \beta_l\) 
in the system of equations can be any natural Number for any \(k,l\in\mathbb{N}\).

\begin{align*}
&0 =c_{\alpha_1,\beta_1} + c_{\beta_1,\alpha_1}+ \binom{ \alpha_1 + \beta_1}{ \alpha_1} \\
&0 = c_{\alpha_1,\alpha_2,\beta_1} + c_{\beta_1,\alpha_1,\alpha_2} + c_{\alpha_2,\alpha_1,\beta_1} + 
\binom{\alpha_2 + \beta_1}{ \alpha_2} c_{\alpha_1,\alpha_2+\beta_1} \\
&\hspace{2cm}  +\binom{\alpha_1+\beta_1}{\alpha_1} c_{\alpha_1+\beta_1,\alpha_2}\\
&0= c_{\alpha_1,\alpha_2,\alpha_3,\beta_1} + c_{\alpha_1,\alpha_2,\beta_1,\alpha_3} 
+ c_{\alpha_1,\beta_1,\alpha_2,\alpha_3} + c_{\beta_1,\alpha_1,\alpha_2,\alpha_3}\\
&+\binom{\alpha_1+\beta_1}{\beta_1} 
c_{\alpha_1+\beta_1,\alpha_2,\alpha_3} 
+ \binom{\alpha_2+\beta_1}{\beta_1} c_{\alpha_1,\alpha_2+\beta_1,\alpha_3}\\
&\hspace{2cm} + \binom{\alpha_3+\beta_1}{\beta_1} c_{\alpha_1,\alpha_2,\alpha_3+\beta_1}\\
&0= c_{\alpha_1,\alpha_2,\beta_1,\beta_2} +c_{\alpha_1,\beta_1,\alpha_2,\beta_2} +
c_{\beta_1,\alpha_1,\alpha_2,\beta_2} +c_{\alpha_1,\beta_1,\beta_2,\alpha_2} \\
&+c_{\beta_1,\alpha_1,\beta_2,\alpha_2} +c_{\beta_1,\beta_2,\alpha_1,\alpha_2} 
+\binom{\alpha_1+\beta_1}{\alpha_1} (c_{\alpha_1+\beta_1,\alpha_2,\beta_2} \\
&+ c_{\alpha_1+\beta_1,\beta_2,\alpha_2})  
+\binom{\alpha_1+\beta_2}c_{\beta_1,\alpha_1+\beta_2,\alpha_1} \\
&+\binom{\alpha_2+\beta_1}{\alpha_2} c_{\alpha_1,\alpha_2+\beta_1,\beta_2}
+ \binom{\alpha_2+\beta_2}{\alpha_2} (c_{\alpha_1,\beta_1,\alpha_2+\beta_2}\\
&+ c_{\beta_1,\alpha_1,\alpha_2+\beta_2})
+ \binom{\alpha_1+\beta_1}{\alpha_1} \binom{\alpha_2+\beta_2}{\alpha_2}
 c_{\alpha_1+\beta_1,\alpha_2+\beta_2}\\
&0=c_{\alpha_1,\beta_1,\beta_2,\beta_3,\beta_4} 
+ c_{\beta_1,\alpha_1,\beta_2,\beta_3,\beta_4} 
+ c_{\beta_1,\beta_2,\alpha_1,\beta_3,\beta_4} \\
&\hspace{1cm}+ c_{\beta_1,\beta_2,\beta_3,\alpha_1,\beta_4} 
+ c_{\beta_1,\beta_2,\beta_3,\beta_4,\alpha_1} \\
&+\binom{\alpha_1+\beta_1}{\alpha_1} c_{\alpha_1+\beta_1,\beta_2,\beta_3,\beta_4}
+ \binom{\alpha_1+\beta_2}{\alpha_1} c_{\beta_1,\alpha_1+\beta_2,\beta_3,\beta_4}\\
&+ \binom{\alpha_1+\beta_3}{\alpha_1} c_{\beta_1,\beta_2,\alpha_1+\beta_3,\beta_4}
+ \binom{\alpha_1+\beta_4}{\alpha_1} c_{\beta_1,\beta_2,\beta_3,\alpha_1+\beta_4}\\
&0= c_{\alpha_1,\alpha_2,\beta_1,\beta_2,\beta_3} 
+c_{\alpha_1,\beta_1,\alpha_2,\beta_2,\beta_3} 
+c_{\beta_1,\alpha_1,\alpha_2,\beta_2,\beta_3} \\
&+c_{\alpha_1,\beta_1,\beta_2,\alpha_2,\beta_3} 
+c_{\beta_1,\alpha_1,\beta_2,\alpha_2,\beta_3} 
+c_{\beta_1,\beta_2,\alpha_1,\alpha_2,\beta_3} \\
&+c_{\alpha_1,\beta_1,\beta_2,\beta_3,\alpha_2} 
+c_{\beta_1,\alpha_1,\beta_2,\beta_3,\alpha_2} 
+c_{\beta_1,\beta_2,\alpha_1,\beta_3,\alpha_2} \\
&+c_{\beta_1,\beta_2,\beta_3,\alpha_1,\alpha_2} 
+\binom{\alpha_1+\beta_1}{\beta_1} (c_{\alpha_1+\beta_1,\alpha_2,\beta_2,\beta_3}\\
&+c_{\alpha_1+\beta_1,\beta_2,\alpha_2,\beta_3}
+c_{\alpha_1+\beta_1,\beta_2,\beta_3,\alpha_2})\\
&+\binom{\alpha_2+\beta_1}{\beta_1} c_{\alpha_1,\alpha_2+\beta_1,\beta_2,\beta_3}\\
&+\binom{\alpha_2+\beta_2}{\beta_2} (c_{\beta_1,\alpha_1,\alpha_2+\beta_2,\beta_3}
+c_{\alpha_1,\beta_1,\alpha_2+\beta_2,\beta_3})\\
&+\binom{\alpha_1+\beta_2}{\beta_2} (c_{\beta_1,\alpha_1+\beta_2,\alpha_2,\beta_3}
+c_{\beta_1,\alpha_1+\beta_2,\beta_3,\alpha_2})\\
&+\binom{\alpha_2+\beta_3}{\beta_3}( c_{\alpha_1,\beta_1,\beta_2,\alpha_2+\beta_3}
+c_{\beta_1,\alpha_1,\beta_2,\alpha_2+\beta_3}\\
&+c_{\beta_1,\beta_2,\alpha_1,\alpha_2+\beta_3})
+\binom{\alpha_1+\beta_3}{\beta_3} c_{\beta_1,\beta_2,\alpha_1+\beta_3,\alpha_2}\\
&+\binom{\alpha_1+\beta_1}{\alpha_1} \binom{\alpha_2+\beta_2}{\alpha_2} 
c_{\alpha_1+\beta_1,\alpha_2+\beta_2,\beta_3}\\
&+\binom{\alpha_1+\beta_2}{\alpha_1} \binom{\alpha_2+\beta_3}{\alpha_2} 
c_{\beta_1,\alpha_1+\beta_2,\alpha_2+\beta_3}\\
&+\binom{\alpha_1+\beta_1}{\alpha_1} \binom{\alpha_2+\beta_3}{\alpha_2} 
c_{\alpha_1+\beta_1,\beta_2,\alpha_2+\beta_3}\\
& \hspace{3cm} \vdots
\end{align*}

Solving the first few equations and plugging the solution into the consecutive
 equations one can see that at least the first equations are solved by 
\begin{equation}
c_{\alpha_1,\dots, \alpha_k} = \frac{(-1)^k}{k} \begin{pmatrix}\sum_{l=1}^k \alpha_l\\ \alpha_1\ \alpha_2 \ \cdots \alpha_k\end{pmatrix},
\end{equation}
where the last factor is the multinomial coefficient of the indices \(\alpha_1,\dots, \alpha_k\in\mathbb{N}\).

\subsection{Theorem and Proof}

The above considerations lead us to the following

\begin{Thm}\label{thm: T_n recursive}
For any \(n\in\mathbb{N}\) the nth expansion coefficient of the second quantized scattering operator \(T_n\) is given by
\begin{align}\notag
&T_n = \sum_{g=2}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^g}{g} 
\binom{n}{\vec{b}} \prod_{l=1}^g T_{b_l} + C_n \id_{\mathcal{F}} \\ \label{T_n recursive}
&+ G\left( \sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^{g+1}}{g} 
\binom{n}{\vec{b}} \prod_{l=1}^g Z_{b_l}  \right),
\end{align}
for some \(C_n\in \mathbb{C}\) which depends on the external field \(A\). The last summand will henceforth
be abbreviated by \(G_n\).
\end{Thm}

\textbf{Proof:} The way we will prove this is to compute the commutator of the difference between \(T_n\) and
the first summand of \eqref{T_n recursive} with the creation and annihilation operator of an element of the
basis of \(\mathcal{H}\). This will turn out to be exactly equal
to the corresponding commutator
of the second summand of \eqref{T_n recursive}, since two operators on Fockspace only
have the same commutator with general creation and annihilation operators if they
agree up to multiples of the identity this will conclude our proof. 

 In order to simplify the notation as much as possible, 
I will denote by \(a^\# z\) either \(a(z(\varphi_p))\) or
 \(a^*(z(\varphi_p))\) for any one particle operator \(z\) and any element
 \(\varphi_p\) of the orthonormal basis \((\varphi_p)_{p\in\mathbb{Z}}\) of
 \(\mathcal{H}\). (We need not decide between creation and annihilation 
 operator, since the expressions all agree)
 
In order to organize the bookkeeping of all the summands which arise from iteratively
making use of the commutation rule \eqref{Tk_commutation_relation} we organize them 
by the looking at a spanning set of the possible terms that arise my choice is

\begin{equation}\label{combinatorics span}
\left\{ \left. a^\# \prod_{k=1}^{m_1} Z_{\alpha_k} \prod_{k=1}^{m_2}T_{\beta_k} \right|
m_1\in\mathbb{N},m_2\in\mathbb{N}_0, \alpha\in \mathbb{N}^{m_1}, 
\beta \in \mathbb{N}^{m_2}, |\alpha|+|\beta|=n\right\}_.
\end{equation}

As a first step of computing the commutator in question we look at the summand
corresponding to a fixed value of the summation index \(g\) of 

\begin{equation}\label{combinatorics total sum of T}
-\sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^g}{g} 
\binom{n}{\vec{b}} \prod_{l=1}^g T_{b_l}.
\end{equation}

 We need to bring this object into the form of a sum
of terms which are multiples of elements of the set \eqref{combinatorics span}.
This we will commit ourselves to for the next few pages. First we apply
the product rule for the commutator:

\begin{align*}
&\left[ \sum_{\stackrel{\vec{l}\in\mathbb{N}^g}{|\vec{l}|=n}} \frac{(-1)^g}{g} \binom{n}{\vec{l}}
 \prod_{k=1}^g T_{l_k},a^\#\right]\\
 &= \sum_{\stackrel{\vec{l}\in\mathbb{N}^g}{|\vec{l}|=n}} \frac{(-1)^g}{g} \binom{n}{\vec{l}}
 \sum_{\tilde{k}=1}^g \prod_{j=1}^{\tilde{k}-1} T_{l_j} 
 \left[ T_{l_{\tilde{k}}},a^\#\right] \prod_{j=\tilde{k}+1}^g T_{l_j}\\
&=\sum_{\stackrel{\vec{l}\in\mathbb{N}^g}{|\vec{l}|=n}} \frac{(-1)^g}{g} \binom{n}{\vec{l}}
 \sum_{\tilde{k}=1}^g \prod_{j=1}^{\tilde{k}-1} T_{l_j} 
\sum_{\sigma_{\tilde{k}}=1}^{l_{\tilde{k}}} \binom{l_{\tilde{k}}}{\sigma_{\tilde{k}}} 
a^\# Z_f T_{l_{\tilde{k}}-\sigma_{\tilde{k}}} \prod_{j=\tilde{k}+1}^g T_{l_j},
\end{align*}
in the second step we used \eqref{Tk_commutation_relation}. Now we commute
all the \(T_l\)s to the left of \(a^\#\) to its right:

\begin{equation}\label{combinatorics ordered product}
= \sum_{\stackrel{\vec{l}\in\mathbb{N}^g}{|l|=n}} \frac{(-1)^g}{g} \binom{n}{\vec{l}}
\sum_{\tilde{k}=1}^g \sum_{\stackrel{\forall 1\le j <\tilde{k}}{0\le \sigma_{j}\le l_j}}
\sum_{\sigma_{\tilde{k}}=1}^{l_{\tilde{k}}} \prod_{j=1}^{\tilde{k}} \binom{l_j}{\sigma_j}
a^\# \prod_{j=1}^{\tilde{k}} Z_{\sigma_j} \prod_{j=1}^{\tilde{k}} T_{l_j-\sigma_j}
\prod_{j=\tilde{k}+1}^g T_{l_j}.
\end{equation}
At this point we notice that the multinomial coefficient can be combined with all
the binomial coefficients to form a single multinomial coefficient of degree
\(g+\tilde{k}\). Incidentally this is also the amount of \(Z\) operators plus the amount
of \(T\) operators in each product. Moreover the indices of the Multinomial index
agree with the indices of the \(Z\) and \(T\) operators in the product. Because of 
this, we see that if we fix an element of the spanning set \eqref{combinatorics span}
\(a^\# \prod_{k=1}^{m_1} Z_{\alpha_k} \prod_{k=1}^{m_2}T_{\beta_k}\), each 
summand of \eqref{combinatorics ordered product} which is a multiple of
this element, has the prefactor

\begin{equation}
\frac{(-1)^g}{g} \binom{n}{\alpha_1 \ \cdots \alpha_{m_1} \ \beta_1 \cdots \beta_{m_2}}
\end{equation}

no matter which summation index \(l\in\mathbb{N}^g\) it corresponds to. In order
to do the matching one may ignore the indices \(\sigma_j\) and \(l_j-\sigma_j\) 
which vanish, because the corresponding operators \(Z_0\) and \(T_0\) are equal to
the identity operator on \(\mathcal{H}\) respectively Fockspace. 

Since we know that 

\begin{align*}
\left[ G\left(\sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^g}{g} 
\binom{n}{\vec{b}}  \prod_{l=1}^g Z_{b_l}\right), a^\#\right]\\
= a^\# \sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^g}{g} 
\binom{n}{\vec{b}}  \prod_{l=1}^g Z_{b_l}
\end{align*}
holds, all that is left to show is that 
\begin{align}\label{combinatorics total commutator}
&\left[\sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^g}{g} 
\binom{n}{\vec{b}} \prod_{l=1}^g T_{b_l}, a^\#\right]\\\notag
&= a^\# \sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^g}{g} 
\binom{n}{\vec{b}}  \prod_{l=1}^g Z_{b_l}
\end{align}
holds. For which we need to count the summands which are multiples of each element of \eqref{combinatorics span}
 corresponding to each \(g\) in \eqref{combinatorics total sum of T}. So let us fix some element
 \(K(m_1,m_2)\) of \eqref{combinatorics span} corresponding to some \(m_1\in\mathbb{N},
 m_2\in\mathbb{N}_0, \alpha \in \mathbb{N}^{m_1}\) and \( \beta\in \mathbb{N}^{m_2}\).
Rephrasing this problem, we can ask which products
\begin{equation}
\prod_{l=1}^g T_{\gamma_l}
\end{equation}
 for suitable \(g\) and \((\gamma_l)_l\) produce multiples of \(K(m_1,m_2)\)? We will call the number of 
 total contributions which are multiples of \(\#K(m_1,m_2)\).  Looking at the commutation relations 
\eqref{Tk_commutation_relation} we split the set of indices \(\{\gamma_1\dots \gamma_g\}\) into
three sets \(A,B\) and \(C\), where the commutation relation has to be used in such a way, that

\begin{align*}
&\forall k: \gamma_k \in A \iff \exists j\le m_1: \gamma_k = \alpha_j, \\
\wedge& \forall k: \gamma_k \in B \iff \exists j\le m_2: \gamma_k = \beta_j\\
\wedge & \forall k: \gamma_k \in C \iff \exists j\le m_1, l\le m_2: \gamma_k = \alpha_j + \beta_l
\end{align*}
 holds. Unfortunately not any splitting corresponds to a contribution and not any
 order of multiplication of a legal splitting corresponds to a contribution either.
 However we can be sure that \(\prod_{j} T_{\alpha_j} \prod_j T_{\beta_j}\) gives
 a contribution and we may apply the commutation relations backwards to obtain any
 valid combination. This results in the following game:
 
Starting from the string 
\begin{equation}
A_1A_2\dots A_{m_1} B_1 B_2 \dots B_{m_2},
\end{equation}
how many strings can we produce by applying the following rules?
\begin{enumerate}
\item You may replace any occurrence of \(A_k B_j\) by \(B_j A_k\) for any \(j\) and \(k\).
\item You may replace any occurrence of \(A_k B_j\) by \(C_{k,j}\) for any \(j\) and \(k\).
\end{enumerate}
Where we have to count the number of times we applied the second rule, or equivalently
the number \(\#C\) of \(C\)'s in the resulting string, because the summation index \(g\) in 
\eqref{combinatorics total sum of T} corresponds to \(m_1+m_2-\#C\). 

Fix \(\#C \in\{0,\dots ,\min(m_1,m_2)\}\). A valid string has \(m_1+m_2-\#C\) characters,
because the number of its \(C\)s is \(\#C\), its number of \(A\)s is \(m_1-\#C\) and 
its number of \(B\)s is \(m_2-\#C\). Ignoring the labelling of the \(A\)s, \(B\)s and \(C\)s 
there are \(\binom{m_1+m_2-\#C}{\#C \ (m_1 - \#C) \ (m_2-\#C)}\) such strings. Now if
we consider one such string without labelling, e.g.

\begin{equation}
C A A B A C C B B A C B B A B B B B,
\end{equation}

there is only one correct labelling to be restored, namely the one where each (first index of 
any) \(C\) and \(A\) receive increasing labels from left to right and analogously for (the second 
 index of any) \(C\) and \(B\), resulting in
 
\begin{equation}
 C_{1,1} A_2 A_3 B_2 A_4 C_{5,3} C_{6,4} B_{5} B_6 A_7 C_{8,7} B_8 B_9 A_9 B_{10} B_{11} B_{12} B_{13}.
\end{equation}

So any unlabelled  string corresponds to exactly one labelled string which in turn corresponds to 
exactly one choice of operator product \(\prod T\). 
So returning to our Operators, we found the number \(\#K(m_1,m_2)\) it is

\begin{equation}\label{combinatorics binomial sum}
\#K(m_1,m_2) =-\sum_{g=\max(m_1,m_2)}^{m_1+m_2} \frac{(-1)^g}{g} \binom{g}{(m_1+m_2-g) \ (g-m_1) \ (g - m_2)},
\end{equation}
where the total minus sign comes from the total minus sign in front of \eqref{combinatorics total commutator}
with respect to \eqref{T_n recursive}. 

Now a quick route to evaluate this sum requires us to slightly generalize the standard definition of
binomial coefficient to the one in \cite{graham1994concrete}:

\begin{Def}
For \(a\in\mathbb{C}, b\in\mathbb{Z}\) we define

\begin{equation}
\binom{a}{b} := \left\{\begin{matrix}
\prod_{l=0}^{b-1} \frac{a-l}{l+1} \quad \text{for } b\ge 0\\
0 \hspace{1.7cm} \text{otherwise.}
\end{matrix}\right.
\end{equation}
\end{Def}

Defining the binomial coefficient for negative lower index to be zero has the merit, that one can extend the
range of validity of many rules and sums involving binomial coefficients, also one does not have 
to worry about the range of summation in many cases.

As a first step to evaluate \eqref{combinatorics binomial sum} we split the trinomial coefficient into binomial
ones and make use of the absorption identity

\begin{equation}\tag{absorption}\label{absorption}
\forall a \in \mathbb{C}\ \forall b \in \mathbb{Z}: b \binom{a}{b} = a \binom{a-1}{b-1} 
\end{equation}

for \(m_2\neq 0\) as follows

\begin{align*}
&\#K(m_1,m_2) \\
&=-\sum_{g=\max(m_1,m_2)}^{m_1+m_2} \frac{(-1)^g}{g} \binom{g}{(m_1+m_2-g) \ (g-m_1) \ (g - m_2)}\\
&=-\sum_{g=\max(m_1,m_2)}^{m_1+m_2} \frac{(-1)^g}{g} \binom{g}{m_2}\binom{m_2}{g-m_1}\\
&\stackrel{\eqref{absorption}}{=}-\sum_{g=\max(m_1,m_2)}^{m_1+m_2} \frac{(-1)^g}{m_2} \binom{g-1}{m_2-1}\binom{m_2}{g-m_1}\\
&=\frac{-1}{m_2} \sum_{g=\max(m_1,m_2)}^{m_1+m_2} (-1)^g \binom{g-1}{m_2-1}\binom{m_2}{g-m_1}\\
&=\frac{-1}{m_2} \sum_{g\in\mathbb{Z}} (-1)^g \binom{g-1}{m_2-1}\binom{m_2}{g-m_1}\\
&\stackrel{*}{=} \frac{-1}{m_2} (-1)^{m_2-m_1} \binom{m_1-1}{-1} = 0,
\end{align*}
where for the marked equality we used summation rule (5.24) of \cite{graham1994concrete}. 
So all the coefficients vanish that fulfil \(m_2\neq 0\). And the sum for the remaining case
is trivial, since there is just one summand. So the left hand side of 
\eqref{combinatorics total commutator} can be evaluated

\begin{align*}
\left[\sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n}}\frac{(-1)^g}{g} 
\binom{n}{\vec{b}} \prod_{l=1}^g T_{b_l}, a^\#\right]\\
= \sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|b|=n}} \frac{(-1)^g}{g} \binom{n}{\vec{b}}  a^\# \prod_{l=1}^g Z_{b_l},
\end{align*}

which is exactly equal to the right hand side of \eqref{combinatorics total commutator}. \(\Box\)

\section{Solution to Recursive Equation}

So we found a recursive equation for the \(T_n\)s, now we need to solve it. 
In order to do so we need the following lemma about combinatorial distributions

\begin{Lemma}\label{stirling lemma}
For any \(g\in\mathbb{N},k\in\mathbb{N}\)
\begin{equation}
\sum_{\stackrel{\vec{g}\in\mathbb{N}^g}{|\vec{g}|=k}} \binom{k}{\vec{g}}=\sum_{l=0}^g (-1)^l (g-l)^k \binom{g}{l}
\end{equation}
holds. The reader interested in terminology may be eager to know, that the right hand side is equal to
 \(g!\) times the Stirling 
number of the second kind \(\left\{\begin{matrix}k\\g\end{matrix}\right\}\).
\end{Lemma}
\textbf{Proof:} We would like to apply the multinomial theorem but there are all the summands missing where at least
one of the entries of \(\vec{g}\) is zero, so we add an appropriate expression of zero. We also give the expression in
question a name, since we will later on arrive at a recursive expression.
\begin{multline}
F(g,k):=\sum_{\stackrel{\vec{g}\in\mathbb{N}^g}{|\vec{g}|=k}} \binom{k}{\vec{g}}
= \sum_{\stackrel{\vec{g}\in\mathbb{N}_0^g}{|\vec{g}|=k}} \binom{k}{\vec{g}}
- \sum_{\stackrel{\stackrel{\vec{g}\in\mathbb{N}_0^g}{|\vec{g}|=k}}{\exists l: g_l=0}} \binom{k}{\vec{g}}\\
= g^k 
 - \sum_{\stackrel{\stackrel{\vec{g}\in\mathbb{N}_0^g}{|\vec{g}|=k}}{\exists l: g_l=0}} \binom{k}{\vec{g}}
=g^k 
- \sum_{n=1}^{g-1} \sum_{\stackrel{\vec{g}\in\mathbb{N}_0^g}{|\vec{g}|=k}}
 \binom{k}{\vec{g}} 1_{\exists! i_1\dots i_n : \forall i_l\neq i_k \wedge \forall l :g_l=0}
\end{multline}
where in the last line the indicator function is to enforce there being exactly n different indices \(l\) for which \(g_l=0\)
holds. Now now since it does not matter which entries of the vector vanish because the multinomial coefficient 
is symmetric and its value identical to the corresponding multinomial coefficient where the vanishing entries
are omitted, we can further simplify the sum:

\begin{equation*}
F(g,k)= g^k -  \sum_{n=1}^{g-1} \binom{g}{n} \sum_{\stackrel{\vec{g}\in\mathbb{N}^g}{|\vec{g}|=k}}
 \binom{k}{\vec{g}}
\end{equation*}

The inner sum turns out to be \(F(g-n,k)\), so we found the recursive relation for \(F\):
\begin{equation}\label{combinatorics solution recursive}
F(g,k)= g^k -  \sum_{n=1}^{g-1} \binom{g}{n} F(g-n,k).
\end{equation}

By iteratively applying this equation, we find the following formula, which we will now prove by induction

\begin{multline}\label{combinatorics induction}
\forall d\in\mathbb{N}_0: F(g,k)=\sum_{l=0}^d (-1)^l (g-l)^k \binom{g}{l}\\
+(-1)^{d+1} \sum_{n=1}^{g-d-1} \binom{n+d-1}{d} \binom{g}{n+d} F(g-d-n,k).
\end{multline}

We already showed the start of the induction, so what's left is the induction step. Before we do so the
following remark is in order: We are only interested in the case \(d=g\) and the formula seems meaningless
for \(d>g\); however, the additional summands in the left sum vanish, where as the the right sum is empty
for these values of \(d\) since the  upper bound of the summation index is lower than its lower bound.

For the induction step, pick \(d\in\mathbb{N}_0\), we pull the first summand out of the second sum 
of \eqref{combinatorics induction},
on this summand we apply the recursive relation \eqref{combinatorics solution recursive} resulting in

\begin{multline}
F(g,k)=\sum_{l=0}^{d+1} (-1)^l (g-l)^k \binom{g}{l}\\
+(-1)^{d+1} \sum_{n=2}^{g-d-1} \binom{n+d-1}{d} \binom{g}{n+d} F(g-d-n,k)\\
-(-1)^{d+1} \binom{g}{d+1} \sum_{n=1}^{g-d-2} \binom{g-d-1}{n} F(g-d-1-n,k)\\
=\sum_{l=0}^{d+1} (-1)^l (g-l)^k \binom{g}{l}\\
+(-1)^{d+1} \sum_{n=1}^{g-d-2} \binom{n+d}{d} \binom{g}{n+d+1} F(g-d-1-n,k)\\
-(-1)^{d+1} \binom{g}{d+1} \sum_{n=1}^{g-d-2} \binom{g-d-1}{n} F(g-d-1-n,k).
\end{multline}
After the index shift we can combine the last two sums. 

\begin{multline}
F(g,k)= \sum_{l=0}^{d+1} (-1)^l (g-l)^k \binom{g}{l}\\
+ \sum_{n=1}^{g-d-2}\left[\binom{g}{d+1} \binom{g-d-1}{n} - \binom{n+d}{d} \binom{g}{n+d+1} \right] 
\\(-1)^{d+2} F(g-d-1-n,k).
\end{multline}


In order to combine the two binomials we disassemble \(\binom{g}{d+1}\) into a product and use
 \eqref{absorption}  \(d+1\) times, yielding
\begin{multline}
F(g,k)= \sum_{l=0}^{d+1} (-1)^l (g-l)^k \binom{g}{l}\\
+ \sum_{n=1}^{g-d-2}\left[\binom{n+d+1}{d+1} - \binom{n+d}{d}\right] \binom{g}{n+d+1} 
\\(-1)^{d+2} F(g-d-1-n,k)\\
=\sum_{l=0}^{d+1} (-1)^l (g-l)^k \binom{g}{l}\\
+(-1)^{d+2}  \sum_{n=1}^{g-d-2} \binom{n+d}{d+1} \binom{g}{n+d+1}  F(g-d-1-n,k),
\end{multline}
where we used the addition formula for binomials:

\begin{equation}
\forall n\in \mathbb{C} \forall k \in \mathbb{Z}: \binom{n}{k} = \binom{n-1}{k} + \binom{n-1}{k-1}.
\end{equation}
This concludes the proof by induction. By setting \(d=g\) in equation \eqref{combinatorics induction} 
we arrive at the desired result. \qed

Using the previous lemma, we are able to show the next

\begin{Lemma}\label{combinatorics weak conjecture lemma 2}
For any \(k \in \mathbb{N}\backslash \{1\}\) the following equation holds
\begin{equation}
\sum_{g=1}^k \frac{(-1)^g}{g} \sum_{\stackrel{\vec{g}\in\mathbb{N}^g}{|\vec{g}|=k}}\binom{k}{\vec{g}}=0.
\end{equation}
\end{Lemma}
\textbf{Proof:} Let \(k\in\mathbb{N}\backslash\{1\}\), as a first step we apply lemma \ref{stirling lemma}.
We change the order of summation, use \eqref{absorption}, extend the range of summation and shift 
summation index  to arrive at

\begin{multline}
\sum_{g=1}^k \frac{(-1)^g}{g} \sum_{l=0}^g (-1)^l (g-l)^k \binom{g}{l}
= \sum_{g=1}^k \frac{1}{g} \sum_{l=0}^g (-1)^{g-l} (g-l)^k \binom{g}{g-l}\\
= \sum_{g=1}^k \sum_{p=0}^g (-1)^{p} p^k \frac{1}{g} \binom{g}{p}
=\sum_{g=1}^k \sum_{p=0}^g (-1)^{p} p^k \frac{1}{p} \binom{g-1}{p-1}\\
=\sum_{g=1}^k \sum_{p\in\mathbb{Z}} (-1)^{p} p^{k-1}\binom{g-1}{p-1}
=\sum_{p\in\mathbb{Z}} (-1)^{p} p^{k-1} \sum_{g=1}^k \binom{g-1}{p-1}\\
=\sum_{p\in\mathbb{Z}} (-1)^{p} p^{k-1} \sum_{g=0}^{k-1} \binom{g}{p-1}.
\end{multline}

Now we use equation (5.10) of \cite{graham1994concrete}:

\begin{equation}\tag{upper summation}
\forall m,n\in\mathbb{N}_0: \sum_{k=0}^n \binom{k}{m} = \binom{n+1}{m+1},
\end{equation}
which can for example be proven by induction on \(n\).

We furthermore rewrite the power of the summation index \(p\) in terms of the derivative of an 
exponential and change order summation and differentiation. This results in

\begin{multline*}
\sum_{g=1}^k \frac{(-1)^g}{g} \sum_{l=0}^g (-1)^l (g-l)^k \binom{g}{l}
=\sum_{p\in\mathbb{Z}} (-1)^{p} p^{k-1}  \binom{k}{p}\\
=\sum_{p=0}^k (-1)^{p} \left. \frac{\partial^{k-1}}{\partial \alpha^{k-1}} e^{\alpha p}\right|_{\alpha=0}  \binom{k}{p}
=\left. \frac{\partial^{k-1}}{\partial \alpha^{k-1}}  \sum_{p=0}^k (-1)^{p} e^{\alpha p} \binom{k}{p}\right|_{\alpha=0} \\
=\left. \frac{\partial^{k-1}}{\partial \alpha^{k-1}}  \left( 1-e^{\alpha p}\right)^k \right|_{\alpha=0} 
=(-1)^k \left. \frac{\partial^{k-1}}{\partial \alpha^{k-1}} \left( \sum_{l=1}^\infty \frac{(\alpha p)^l}{l!} \right) ^k \right|_{\alpha=0} \\
=(-1)^k \left. \frac{\partial^{k-1}}{\partial \alpha^{k-1}} ((\alpha p)^k + \mathcal{O} ((\alpha p)^{k+1}) ) \right|_{\alpha=0} =0.
\end{multline*}
\qed




I am now in a position to state the solution to the recursive equation \eqref{T_n recursive}
and have us prove together that it is in fact a solution. 

\begin{Thm}
For \(n\in\mathbb{N}\) equation \eqref{T_n recursive} is solved by 

\begin{equation}\label{recursive solution}
T_n = \sum_{g=1}^n \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|= n}} \sum_{\vec{d}\in {\{0,1\}}^g} 
\frac{1}{g!} \binom{n}{\vec{b}} \prod_{l=1}^g F_{b_l,d_l},
\end{equation}
where \(F\) is given by
\begin{equation}\label{eq resursive weak solution}
F_{a,b} = \left\{ \begin{matrix}G_a \quad \text{für } b=0 \\ C_a \quad \text{für } b=1  \end{matrix} \right._. 
\end{equation}
For the readers convenience we remind her, that \(G_a\) is defined by \eqref{T_n recursive}. Whereas
the constants \(C_n\) depend on the vacuum expectation value of \(T_n\) as well as on the products
on the right hand side of \eqref{recursive solution} and are yet either to be found in terms of or 
eliminated in favour of \(\langle T_n \rangle \). 
\end{Thm}

\textbf{Proof:} The structure of this proof will be induction over \(n\). For \(n=1\) the whole expression
on the right hand side collapses to \(C_1 + G_1\), which we already know to be equal to \(T_1\). For
arbitrary \(n+1\in\mathbb{N}\backslash\{1\}\) we apply the recursive equation \eqref{T_n recursive}
once and use the induction hypothesis for all \(k\le n\) and thereby arrive at the rather convoluted 
expression

\begin{multline}\label{recursive weak conjecture proof}
T_{n+1} \stackrel{\eqref{T_n recursive}}{=} 
G_{n+1} + C_{n+1}+ \sum_{g=2}^{n+1} \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n+1}} 
\frac{(-1)^g}{g} \binom{n+1}{\vec{b}} \prod_{l=1}^g T_{b_l}\\
\stackrel{\text{induction hyp}}{=} G_{n+1} + C_{n+1}+ \sum_{g=2}^{n+1} \sum_{\stackrel{\vec{b}\in\mathbb{N}^g}{|\vec{b}|=n+1}} 
\frac{(-1)^g}{g} \binom{n+1}{\vec{b}} \prod_{l=1}^g \\
\sum_{g_l=1}^{b_l} \sum_{\stackrel{\vec{c}_l \in\mathbb{N}^{g_l}}{|\vec{c}_l|=b_l}} \sum_{\vec{e}_l \in \{0,1\}^{g_l}}
\frac{1}{g_l!} \binom{b_l}{\vec{c}_l} \prod_{k=1}^{g_l} F_{c_{l,k},e_{l,k}}.
\end{multline}

If we were to count the contributions of this sum to a specific product \(\prod F_{c_j,e_j}\) for some choice of 
\((c_j)_j, (e_j)_j\) we would first recognize that all the multinomial factors in \eqref{recursive weak conjecture proof}
combine to a single one whose indices are given by the first indices of all the \(F\) factors involved.
Other than this factor each contribution adds \(\frac{(-1)^g}{g} \prod_{l=1}^g \frac{1}{g_l!}\) to the sum. So we 
need to keep track of how many contributions there are and which distributions of \(g_l\) they belong to. 

Fix some product \(\prod F :=\prod_{j=1}^{\tilde{g}} F_{\tilde{b}_j,\tilde{d}_j}\). In the sum 
\eqref{recursive weak conjecture proof} we pick some initial short product of length \(g\) and split each
factor into \(g_l\) pieces to arrive at one of length \(\tilde{g}\) if the product is to contribute to
\(\prod F\). So clearly \(\sum_{l=1}^g g_l = \tilde{g}\) holds for any contribution to \(\prod F\). 
The reverse is also true, for any
\(g\) and \(g_1, \dots, g_g\in\mathbb{N}\) such that \(\sum_{l=1}^g g_l=\tilde{g}\) holds
the corresponding expression in \eqref{recursive weak conjecture proof} contributes to 
\(\prod F\). Furthermore \(\prod F\) and \(g\), \(g_1,\dots g_g\) is enough to uniquely
determine the summand of \eqref{recursive weak conjecture proof} the contribution
belongs to. For an illustration of this splitting see

\begin{align*}
\underbrace{\underbrace{F^1_{3,1} F^2_{2,0} F^3_{7,1}}_{g_1=3} \underbrace{F^4_{5,0}}_{g_2=1} \underbrace{F^5_{4,1} F^6_{2,1}}_{g_3=2} \underbrace{F^7_{1,1} F^8_{3,0} F^9_{4,1}}_{g_4=3} \underbrace{F^{10}_{4,1} F^{11}_{1,0}}_{g_5=2}}_{g=5}\\
 g_1+g_2+g_3+g_4+g_5=11=\tilde{g},
\end{align*}

where I labelled the factors in the upper right index for the readers convenience. We recognize
that the sum we are about to perform is by no means unique for each order of \(n\) but only 
depends on the number of appearing factors and the number of splittings performed on
them. By the preceding argument we need 

\begin{equation}\label{combinatorics weak conjecture reformulated into equation}
\sum_{g=2}^{\tilde{g}} \frac{(-1)^g}{g} \sum_{\stackrel{\vec{g}\in\mathbb{N}^g}{|\vec{g}|=\tilde{g}}} \prod_{l=1}^g \frac{1}{g_l!}
= \frac{1}{\tilde{g}!} 
\end{equation}


to hold for \(\tilde{g}>1\), in order to find agreement with the proposed solution \eqref{eq resursive weak solution}.
Now proving \eqref{combinatorics weak conjecture reformulated into equation} is done by 
realizing, that one can include the right hand side into the sum as the \(g=1\) summand, dividing
the equation by \(\tilde{g}!\) and using lemma \ref{combinatorics weak conjecture lemma 2}
with \(k=\tilde{g}\). The remaining case, \(\tilde{g}=1\), can directly be
read off of \eqref{recursive weak conjecture proof}. \qed

\begin{Corollary}\label{Corollary T_n by G's and C's}
For \(n\in\mathbb{N}\), \(T_n\) can be written as

\begin{equation}
\frac{1}{n!} T_n = \sum_{\stackrel{1\le c+g\le n}{c,g\in\mathbb{N}_0}} 
\sum_{\stackrel{\stackrel{\vec{g}\in\mathbb{N}^g}{\vec{c}\in\mathbb{N}^c}}{|\vec{c}| + |\vec{g}|=n}} 
\frac{1}{c! g!} \prod_{l=1}^c \frac{1}{c_l!} C_{c_l} \prod_{l=1}^g \frac{1}{g_l!} G_{g_l}.
\end{equation}
Please note that for ease of notation we defined \(\mathbb{N}^0:= \{0\}\).
\end{Corollary}
\textbf{Proof:} By an argument completely analogous to the combinatorial argument in the proof of theorem 
\eqref{thm: T_n recursive} we see that we can entangle the \(F\)s in \eqref{recursive solution}
into \(G\)s and \(C\)s if we multiply by a factor of \(\binom{c+g}{c}\) where \(c\) is the 
number of \(C\)s and \(g\) is the number of \(G\)s giving

\begin{multline}
T_n = \sum_{\stackrel{1\le c+g\le n}{c,g\in\mathbb{N}_0}}
\sum_{\stackrel{\stackrel{\vec{g}\in\mathbb{N}^g}{\vec{c}\in\mathbb{N}^c}}{|\vec{c}| + |\vec{g}|=n}} 
\binom{c+g}{c} \frac{1}{(c+g)!} \binom{n}{\vec{g}\oplus \vec{c}}
\prod_{l=1}^c  C_{c_l} \prod_{l=1}^g G_{g_l},
\end{multline}

which directly reduces to the equation we wanted to prove, by plugging in the multinomials in terms of
factorials. 

\begin{Corollary}
As a formal power series, the second quantized scattering operator can be written in the form
\begin{equation}
S= e^{\sum_{l\in\mathbb{N}} \frac{C_{l}}{l!}}
 e^{\sum_{l\in\mathbb{N}} \frac{G_{l}}{l!}},
\end{equation}
which the author finds quite amusing.
\end{Corollary}
\textbf{Proof:} We plug corollary \ref{Corollary T_n by G's and C's} into the defining Series for the \(T_n\)s
giving

\begin{multline}
S= \sum_{n\in\mathbb{N}_0} \frac{1}{n!} T_n \\
=\id_{\mathcal{F}}+ \sum_{n\in\mathbb{N}} \sum_{\stackrel{1\le c+g\le n}{c,g\in\mathbb{N}_0}} 
\sum_{\stackrel{\stackrel{\vec{g}\in\mathbb{N}^g}{\vec{c}\in\mathbb{N}^c}}{|\vec{c}| + |\vec{g}|=n}} 
\frac{1}{c! g!} \prod_{l=1}^c \frac{1}{c_l!} C_{c_l} \prod_{l=1}^g \frac{1}{g_l!} G_{g_l}\\
=\id_{\mathcal{F}}+  \sum_{\stackrel{1\le c+g }{c,g\in\mathbb{N}_0}} 
\sum_{\stackrel{\vec{g}\in\mathbb{N}^g}{\vec{c}\in\mathbb{N}^c}} 
\frac{1}{c! g!} \prod_{l=1}^c \frac{1}{c_l!} C_{c_l} \prod_{l=1}^g \frac{1}{g_l!} G_{g_l}\\
=\sum_{c,g\in\mathbb{N}_0}
\sum_{\stackrel{\vec{g}\in\mathbb{N}^g}{\vec{c}\in\mathbb{N}^c}} 
\frac{1}{c! g!} \prod_{l=1}^c \frac{1}{c_l!} C_{c_l} \prod_{l=1}^g \frac{1}{g_l!} G_{g_l}\\
=\sum_{c\in\mathbb{N}_0} \frac{1}{c!} \sum_{\vec{c}\in\mathbb{N}^c} \prod_{l=1}^c \frac{1}{c_l!} C_{c_l}
\sum_{g\in\mathbb{N}_0} \frac{1}{g!} \sum_{\vec{g}\in\mathbb{N}^g} \prod_{l=1}^g \frac{1}{g_l!} G_{g_l}\\
=\sum_{c\in\mathbb{N}_0} \frac{1}{c!} \prod_{l=1}^c \sum_{c_l\in\mathbb{N}} \frac{1}{c_l!} C_{c_l}
\sum_{g\in\mathbb{N}_0} \frac{1}{g!}  \prod_{l=1}^g \sum_{g_l\in\mathbb{N}} \frac{1}{g_l!} G_{g_l}\\
=\sum_{c\in\mathbb{N}_0} \frac{1}{c!} \left( \sum_{l\in\mathbb{N}} \frac{1}{l!} C_{l}\right)^c
\sum_{g\in\mathbb{N}_0} \frac{1}{g!}  \left( \sum_{l\in\mathbb{N}} \frac{1}{l!} G_{l}\right)^g\\
=e^{\sum_{l\in\mathbb{N}} \frac{1}{l!} C_{l}} e^{\sum_{l\in\mathbb{N}} \frac{1}{l!} G_{l}}.
\end{multline}
\qed



\section{Main Conjecture}\label{sec:main result}
Now I can state the primary objective of my thesis in terms of the following
\begin{Conj}\label{main_result}
For all smooth four-potentials \(A\in C_{c}^\infty(\mathbb{R}^4)\otimes
\mathbb{C}^4\),
\(e \in \mathbb{R}\), and for all \(\psi, \phi \in
\mathcal{F}\) the following limit exists
\begin{equation}
\lim_{n\rightarrow \infty} \left\langle \psi, \sum_{k=0}^n  \frac{e^n}{n!} T_n (A)\phi \right\rangle.
\end{equation}
\end{Conj}

Such a uniform convergence would be optimal. In case, it can not be achieved, a
weaker form of this conjecture in which $|e|$ has to be chosen sufficiently
small and the possible scattering states $\Psi,\Phi$ have to be restricted to a
certain regularity would still be physically interesting.

The main difficulty in proving this theorem is the large number of  possible
summands in the determinant-like structure of the term of \(n\)-th order.  I am
optimistic about finding the proof of conjecture \ref{main_result} for several
reasons:

\begin{enumerate}
\item For the summand involving \(T_n\) one gets a factor of \(\frac{1}{n!}\)
    from the simplex. In the expression for \(T_n\) there are \(n\) time
    integrals, and in the integrand the temporal variables are ordered. Since
    there are \(n!\) possible orderings each particular order contributes only
    one part in \(n!\). This argument can be made precise and has been
    translated into momentum space, where it was already used to estimate the
    one-particle scattering operator, see section \ref{sec:well-def}.
\item The operators \(T_n\) posses the property called ``charge conservation'',
    i.e. \(T_n\) maps any element of the \(b,p\) particle sector of Fock space
    to \(c,o\) particle sectors fulfilling \(c-o=b-p\). Hence many possible
    transitions are forbidden by the structure of the operators \(T_n\).
\item The iterative character of the operators \(T_n\) illustrated by equations \eqref{Tk_commutation_relation}
    suggests that the control of $T_1$ and $T_2$, discussed in sections \ref{sec: first order} and 
    \ref{sec: second order}, is sufficient to also control the $n$-th
    order. This behavior is also suggested by the renormalisability of QED (see \cite[Chapter
    4.3]{scharf2014finite}) which states that only finite many types of renormalisations
    are needed.
\item Many of the remaining possible transitions are forbidden by the
    antisymmetry of the fermionic Fock space.
\end{enumerate}

After a successful proof of the main conjecture this method can be generalised in a canonical manner to yield a direct construction of a more general time evolution operator, as was mentioned in the introduction this is especially desirable in the non-perturbative regime of QED.
In the rest of this section I will present the results about $T_n$ for $n=1$,
$n=2$, and all other odd $n$.

\subsection{Explicit Representations}\label{sec: explicit}

I introduce the operator \(G\) as follows. I denote by \(Q\) the following set \(Q:=\{f: \mathcal{H}\rightarrow \mathcal{H} \text{ linear} \mid i \cdot f \text{ is selfadjoint}\}\).
\begin{Def}
 Let then \(G\) be the following function
\begin{align*}\tag{Def G} \label{Def G}
G: \quad & Q\rightarrow \left( \mathcal{F}\rightarrow \mathcal{F} \right)\\
& f\mapsto \sum_{n\in\mathbb{N}}a^*(f \varphi_n) a(\varphi) - \sum_{n\in -\mathbb{N}} a(\varphi_n) a^*(f \varphi_n).
\end{align*}
\end{Def}
\begin{lemma}
For any \(q\in Q\) the operator \(G(q)\) fulfils the commutation relation
\begin{equation}\label{G commutator}
\forall n \in \mathbb{Z}: \left[ G(q), a^\# (\varphi_n)\right] = a^\# (q (\varphi_n)).
\end{equation}
\end{lemma}
The proof of this lemma follows by direct calculation and exploitation of the commutation relations 
of \(a\) and \(a^*\). \noch{vielleicht Beweis einfügen}

The first expansion coefficient of the scattering operator, \(T_1\), is then given by 
\begin{equation}\label{expT1}
T_1(A)=G(Z_1(A)),
\end{equation}
given \(\langle T_2 \rangle \in \mathbb{C}\), the second order by 
\begin{equation}\label{expT2}
T_2=G(Z_2-Z_1Z_1)+T_1T_1 -\tr \left(Z_{\stackrel{1}{-+}}Z_{\stackrel{1}{+-}}\right) + \langle T_2\rangle,
\end{equation}
and the third order by
\begin{equation}
T_3=G\left( Z_3 - \frac{3}{2}Z_2Z_1 - \frac{3}{2}Z_1Z_2 + 2 Z_1Z_1Z_1\right)+ \frac{3}{2}T_2T_1 + \frac{3}{2} T_1T_2 - 2 T_1T_1T_1 
.\end{equation}

Let \(b \in\mathbb{R}\) be arbitrary, there is a \(C\in\mathbb{C}\) such that \(T_4\) is given by
\begin{align}\tag*{}
&T_4:= 2 T_1T_3 + 2 T_3 T_1 + 3 T_2 T_2 - b T_1 T_1 T_2 - b T_2 T_1 T_1 - 2 ( 6-b) T_1 T_2 T_1 \\\label{explicitT4}
&+ 6 T_1 T_1 T_1 T_1+G\left(Z_4-2Z_1Z_3-2Z_3Z_1-3Z_2Z_2\right.\\\tag*{}
&\left.+bZ_1^2Z_2+2(6-b)Z_1Z_2Z_1+bZ_2Z_1^2-6Z_1^4 \right)  + C
.\end{align}
\noch{habe noch keinen guten Kandidaten für \(T_n\)...}

These expressions can easily be verified by means of the commutation rules \eqref{Tk_commutation_relation}.


\subsection{Results About All Odd Orders}\label{sec: odd orders}
In order to show that any serious candidate for the construction of the scattering-matrix fulfils \(\langle \Omega, T_{2n+1}\Omega \rangle =0\) for any \(n\in\mathbb{N}_0\), I also lift the charge conjugation operator to Fock space.
\subsubsection{ Lifting the Charge Conjugation Operator}


I will define the second quantised charge conjugation operator \(\mathfrak{C}\) on all of Fock space analogously to the way I am currently in the process of defining the second quantised S-matrix operator.
The operator \(\mathfrak{C}: \mathcal{F}\rightarrow \mathcal{F}\) is defined to be the linear bounded operator on Fock space fulfilling the ''lift condition``
\begin{equation}
\begin{aligned}
\forall \phi\in \mathcal{H}: \hspace{0.5cm} &a\left( C \phi \right)  \circ \mathfrak{C}=\mathfrak{C} \circ a^*(\phi),\\
\hspace{0.5cm} &a^*\left( C \phi \right)  \circ \mathfrak{C}=\mathfrak{C} \circ a(\phi),
\end{aligned}
\end{equation}
where \(C\) is the charge conjugation operator on the one particle Hilbert space. The operator \(\mathfrak{C}\) is furthermore defined to fulfil
\begin{equation}
\mathfrak{C}\Omega=\Omega.
\end{equation}

\begin{lemma}\label{basic_properties}
{\large Properties of} \(\mathfrak{C}\):

The lifted operator \(\mathfrak{C}\) has the following important properties.
\begin{align}
\mathfrak{C} \mathfrak{C} = \id \label{self_invers}\\
\mathfrak{C}^* \mathfrak{C} = \id \label{unitarity}
\end{align}
\end{lemma}
The proof of this lemma consists of fairly lengthy but straightforward computations.

\subsubsection{Commutation of Charge Conjugation and Scattering Operators}
I first introduce another operator and use it to find the commutation properties of the charge conjugation operator with the scattering operator. Consider the commutating diagram in the one-particle picture.
\begin{equation}
\begin{CD}							
 \mathcal{H}     @>U^A>>  \mathcal{H}\\
@VVCV        @VVCV\\
\overline{\mathcal{H}}    @>U^{-A}>> \overline{ \mathcal{H}} .
\end{CD}
\end{equation}
Inspired by this diagram I introduce for each four potential \(A\) the one particle operator \(K: \mathcal{H}\rightarrow\mathcal{H}\) with \(K=U^AC=C U^{-A}\). It is easy to see that \(K\) is unitary and \( P_- K P_+\)and \( P_+ K P_-\) are Hilbert-Schmidt operators, due to the analogous property of the one particle scattering Operator, for more details see \cite{ivp0}. This means that \(K\) has a second quantised analogue \(\tilde{K}\) that is unique up to a phase. The operator is then defined as follows


\begin{align}
\tilde{K} &: \mathcal{F}_{\mathcal{H}^{+}\oplus \overline{\mathcal{H}^{-}}} \rightarrow  \mathcal{F}_{\overline{\mathcal{H}}^{+}\oplus \mathcal{H}^{-}}\\
\forall \psi \in \mathcal{H}&: \quad \tilde{K}a^{\#}(\psi)=a^{\#}(K\psi)\tilde{K},
\end{align}

where \(a^{\#}\) can be either \(a\) or \(a^*\).

\begin{axiom}\label{SC_CS} The two unknown phases between \(\tilde{K}\) and \(S^A \mathfrak{C}\) and \(\mathfrak{C}S^{-A}\) agree, i.e.
\begin{equation}
    \exists \phi[A]\in\mathbb R:  \mathfrak{C}S^A=e^{i\phi[A]} \tilde{K}=S^{-A} \mathfrak{C}.
\end{equation}
\end{axiom}
I have now collected enough tools to prove the following
\begin{lemma}\label{vacuum_expectation_odd}
It follows from axiom \ref{SC_CS} that for all four potentials \(A\)
\begin{equation}
\forall n\in\mathbb{N}_0: \langle \Omega, T_{2n+1}(A) \Omega \rangle=0
\end{equation}
holds. I.e. the vacuum expectation value of all odd expansion coefficients of \eqref{S_expansion} vanishes.
\end{lemma}
The proof of lemma \ref{vacuum_expectation_odd} uses homogeneity of degree
\(2n+1\) of \(T_{2n+1}\), and the properties of operator \(\mathfrak{C}\).

\subsection{ Explicit Bound of the First Order}\label{sec: first order}
 The bound of \(T_1(A)\) on a sector of arbitrary but fixed particle number of Fock space \(\mathcal{F}_{m,p}\) for any \(m,p\in\mathbb{N}_0\) can be found to be

\begin{equation}
\left\|  T_1(A)\Big|_{\mathcal{F}_{m,p}}\right\|\le  \sqrt{m p \alpha +(m \beta +p \gamma)^2+(m+1) (p+1) \delta},
\end{equation}
 
 for some positive numbers \(\alpha,\beta,\gamma,\delta \in \mathbb{R}^+\). This bound is found by exploiting the commutation properties of \(T_1\) and the determinant like structure of the scalar product of Fock space.



\subsection{Results about the Second Order}\label{sec: second order}
Historically it was found that it is notoriously difficult to give a mathematically well defined description of \(T_2\). This can now be achieved by means of the method of Epstein und Glaser \cite{epstein1973role}. Knowing the explicit form of \(T_2\),
 \eqref{expT2} all that is left to define this operator is to find its vacuum expectation value. 
This is achieved by
\begin{axiom}\label{ax:causality} Any disturbance of the electromagnetic field should not influence the behaviour of the system previous to its existence. More precisely, the second quantised scattering-matrix should fulfil
\begin{equation}\label{causality}
\left( S^f \right)^{-1} S^{f+g}= \left( S^0 \right)^{-1} S^{g}\tag{causality},
\end{equation}
 for any four potentials \(f\) and \(g\) such that the support of \(f\) is not earlier than the support of \(g\). That is, \eqref{causality} should hold whenever
 \begin{equation}\label{f>g}
 \supp (f) \succ \supp (g) :\iff  \not\exists p\in\supp (f)  \text{ } \exists l\in \supp (g): (p-l)^2 \ge 0 \wedge p^0 \le l^0
 \end{equation}
is fulfilled.
\end{axiom}


 Equation \eqref{causality} also holds when I choose slightly different functions. Let \(\varepsilon, \delta\in \mathbb{R}\), and let \(g,f\) be such that \eqref{causality} is satisfied  then also
\begin{equation}\label{causality_mult}
\left( S^{\varepsilon f} \right)^{-1} S^{\varepsilon f+ \delta g}= \left( S^0 \right)^{-1} S^{\delta g}
\end{equation}
holds. Expanding equation \eqref{causality_mult} differentiating with respect to \(\varepsilon\) and \(\delta\) once, one gets

\begin{equation}\label{defA_1}
0=\tilde{T}_1 (f) T_1(g) + T_2(f,g)=:A_1(f,g).
\end{equation}

Exchanging \(f\) and \(g\) in equations \eqref{f>g} and \eqref{causality_mult} and taking the same derivatives, one gets 
\begin{equation}\label{defR_1}
0=\tilde{T}_1 (g) T_1(f) + T_2(f,g)=:R_1(f,g).
\end{equation}
I now extent the domain of \(A_1\) and \(R_1\) to all possible sets of two four-potentials and define another operator valued distribution by 

\begin{equation}\label{defD_1}
D_1(f,g):=A(f,g)-R(f,g)=\tilde{T}_1 (f) T_1(g)-\tilde{T}_1 (g) T_1(f) .
\end{equation}

It can be inferred from above that \(D_1(f,g)\) is zero if \(f\succ g\) and \(f\prec g\) are both true. Thus to obtain \(T_2\), I first compute \(D_1\) using only \(T_1\) and \(\tilde{T}_1\), then I decompose \(D_1\) into parts fulfilling the support properties of \(A_1\) and \(R_1\). Finally I subtract from the obtained operator \(A_1 (f,g)\) the expression \(\tilde{T}_1(f)T_1(g)\). I will only work with vacuum expectation values, since it is easier and suffices to define \(T_2\) uniquely.

Using \(\tilde{T}_1=-T_1\), and the closed expression \eqref{expT1} for \(T_1\) and the commutation relations of the annihilation and creation operators one obtains

\begin{equation}
\langle \Omega, D_1(f,g) \Omega \rangle= - \tr \left( P_- Z_1(f)P_+ Z_1(g)P_- \right)+  \tr \left(P_- Z_1(g) P_+ Z(f)P_- \right).
\end{equation}

%Expressing the traces in terms of integrals and using equation \eqref{define_Zk} one obtains
%\begin{equation}
%\begin{aligned}
%\tr \left(\Z{-+}{f}\Z{+-}{g} \right)
%=  \int_{\mathcal{M}^-} i_{p_1} \left( \mathrm{d}^4p_1 \right)\int_{\mathcal{M}^+} i_{p_2} \left( \mathrm{d}^4p_2 \right) \\
%\tr \left(\frac{\slashed{p_1}+m}{2m} \slashed{f}(p_1-p_2)\frac{\slashed{p_2}+m}{2m} \slashed{g}(p_2-p_1) \right).
%\end{aligned}
%\end{equation}

Expressing the traces in terms of integrals, using equation \eqref{explicit_Zk} together with a lengthy calculation reveals that 
\begin{multline}\label{D1_vacc}
\langle \Omega, D_1(f,g) \Omega \rangle
=-\frac{2\pi m^2}{3} \int_{\stackrel{k\in\mathbb{R}^4, k\in\text{Future}}{k^2>4m^2}} \sqrt{1-\frac{4m^2}{k^2}}(k^2+2m^2) \\
(g^{\alpha \beta}-\frac{k^\alpha k^\beta}{k^2})\left(f_\alpha (k) g_\beta (-k) - f_\alpha (-k) g_\beta (k)\right) \text{ }\mathrm{d}^4k\\
=-\frac{8 \pi m^4}{3} \int_{k\in\mathbb{R}^4}  d^{\alpha \beta}(k) f_\alpha(k) g_\beta (-k)\text{ }\mathrm{d}^4k,
\end{multline}
holds, where \(d\) is given by
\begin{equation}
d^{\alpha \beta}(k):= I\left( \frac{k^2}{4m^2}\right) 1_{k^2>4m^2}(k) [\theta (k_0)- \theta (-k_0)] \left(g^{\alpha \beta}-\frac{k^\alpha k^\beta}{k^2}\right)
\end{equation}
and \(I\) is given by
\begin{equation}
I(\kappa):=\sqrt{1-\frac{1}{\kappa}}\left(\kappa +\frac{1}{2}\right).
\end{equation}

By \(\text{Causal}_{\pm}\subset \mathbb{R}^4\) I denote the set such that all its elements fulfil \(\zeta\in \text{Causal} \Rightarrow \zeta^2\ge 0\wedge \zeta^0 \in \mathbb{R}^{\pm}\).
Now, to split up the distribution the following theorem comes in handy; it can be found as Theorem IX.16 in \cite{reed1975methods}.

\begin{Thm}\label{Paley-Wiener} Paley-Wiener theorem for causal distributions:
\begin{itemize}

\item[(A)] Let \(T\in \mathcal{S}'(\mathbb{R}^4)\) with \(\supp (T) \subseteq \text{Causal}_{\pm}\) and let \(\hat{T}\) denote its Fourier transform. Then the following is true:
\begin{itemize}
\item[(i)] \(\hat{T}(l+i\eta)\) is analytic for \(l, \eta\in\mathbb{R}^4\) and \(\eta^2>0 \in \text{Causal}^\circ_{\pm}\) and \(\hat{T}\) is the boundary value in the sense of \(\mathcal{S}'\).
\item[(ii)] There is a polynomial \(P\) and an \(n\in\mathbb{N}\) such that
\begin{equation}
\left| \hat{T}(l+i\eta) \right|\le \left| P(l+i\eta )\right| (1+ \text{dist}(\eta, \partial \text{Causal}_{\pm})^{-n}).
\end{equation}
\end{itemize}
\item[(B)] Let \(\hat{F}(l+i\eta)\) be analytic for \(l\in\mathbb{R}^4\) and \(\eta\in\text{Causal}^\circ_{\pm}\) and let \(\hat{F}\) fulfil:
\begin{itemize}
\item[(i)] For all \(\eta_0\in \text{Causal}^\circ_{\pm}\) there is a polynomial \(P_{\eta_0}\) such that for all \(l\in\mathbb{R}^4\) and \(\eta \in \text{Causal}^\circ_{\pm}\)

\begin{equation}
|\hat{F}(l+i(\eta + \eta_0))|\le | P_{\eta_0}(l,\eta)|.
\end{equation}

\item[(ii)] There is an \(n\in\mathbb{N}\) such that for all \(\eta_0\in\text{Causal}^\circ_{\pm}\) there is a polynomial \(Q_{\eta_0}\) with 
\begin{equation}
\forall \varepsilon>0: |\hat{F}(l+i \varepsilon\eta_0)|\le \frac{ | Q_{\eta_0}(l)|}{\varepsilon^n}.
\end{equation}
\end{itemize}
Then there is a \(T\in \mathcal{S}'\) with \(\supp T\subset \text{Causal}_{\pm}\) such that \(T\) is the boundary value of \(\hat{F}(l+i\eta)\) in the sense of \(\mathcal{S}'\), the relation between \(\hat{F}\) and \(T\) being
\begin{equation}
\hat{F}(l+i \eta)= \frac{1}{(2\pi)^2} \int \mathrm{d}^4x e^{-\eta x} e^{i l x} T(x)
\end{equation}
for all \(l\in\mathbb{R}^4\), \(\eta \in \text{Causal}^\circ_{\pm}\) and \(x\in \supp (T)\).
\end{itemize}
\end{Thm}

As an ansatz for the splitting I take

\begin{equation}
\hat{D}_{\pm}^{\alpha \beta}: \mathbb{R}^4 +i \cdot \text{Causal}^º_{\pm} \rightarrow \mathbb{C}, \quad k \mapsto (g^{\alpha \beta} -\frac{k^\alpha k^\beta }{k^2}) J\left( \frac{k^2}{4m^2}\right),
\end{equation}
where 

\begin{equation}\label{dispersion_integral}
J: \mathbb{C}\backslash \mathbb{R}^+_0 \rightarrow \mathbb{C}, \quad J(\kappa) := \frac{\kappa^2}{2\pi i} \int_1^\infty \mathrm{d}s \sqrt{1-\frac{1}{s}} \frac{s+\frac{1}{2}}{s^2 (s-\kappa)}
\end{equation}

and \(\sqrt{\cdot}\) denotes the principal value of the square root with its branch cut at \(\mathbb{R}^-_0\). Therefore \(J\) is well defined on its domain. Furthermore, \(k=l +i\text{ }\varepsilon \text{ } \eta\) with \(l\in\mathbb{R}^4\) and \(\eta \in \text{Causal}^º_{\pm}\) implies:
\begin{equation}
k^2\in \mathbb{R}\Rightarrow k^2=l^2-\eta^2 +i \text{ }\varepsilon l^\alpha \eta_\alpha \in \mathbb{R} \Rightarrow \left( l \perp \eta \wedge \eta^2>0 \Rightarrow l^2 \le 0  \Rightarrow k^2<0\right).
\end{equation}
Hence the argument of the square root \(1-\frac{1}{s}\) stays away from the branch cut and the denominator is never zero, therefore the integral on the right-hand side of equation \eqref{dispersion_integral} exists. Furthermore, \(D^{\alpha \beta}_{\pm} (k)\) is holomorphic on its domain.

It can be shown using standard techniques of complex analysis that

\begin{equation}\label{d_at_boundary}
d^{\alpha \beta} (l)=\lim_{\varepsilon \searrow 0} \left( D^{\alpha \beta}_+ (l+i\varepsilon\eta)-D^{\alpha \beta}_- (l-i\varepsilon\eta) \right)
\end{equation}
holds for almost all \(l\in\mathbb{R}^4\). 

 
%In the following we show that

%\begin{equation}
%d^{\alpha \beta} (l)=\lim_{\varepsilon \searrow 0} \left( D^{\alpha \beta}_+ (l+i\varepsilon\eta)-D^{\alpha \beta}_- (l-i\varepsilon\eta) \right)
%\end{equation}
%holds for almost all \(l\in\mathbb{R}^4\). We divide the proof of this into four cases.

%\begin{itemize}
%\item[(a)] \(\kappa = \frac{l^2}{4m^2}>1\) and \(l^0>0\): This implies \(l^\alpha \eta_%\alpha >0\), and thus,

%\begin{equation}
%\begin{aligned}
%&\lim_{\varepsilon \searrow 0} \left( D^{\alpha \beta}_+ (l+i\varepsilon\eta)-D^{\alpha \beta}_- (l-i\varepsilon\eta) \right)
%= \left(g^{\alpha \beta} - \frac{l^\alpha l^\beta}{l^2} \right)\\
% &\lim_{\varepsilon \searrow 0} \left( J\left(\frac{l^2}{4m^2} +i \varepsilon \right)- J%\left(\frac{l^2}{4m^2} -i \varepsilon \right)\right)
% = \left(g^{\alpha \beta} - \frac{l^\alpha l^\beta}{l^2}\right) \sqrt{1+\frac{1}{\kappa}}\left( \kappa + \frac{1}{2}\right)
%\end{aligned}
%\end{equation}


%\end{itemize}

Using similar techniques and Euler substitutions one finds the boundary value of \(\hat{D}^{\alpha \beta}_{\pm}\). For almost all \(l\in\mathbb{R}^4\) and \(\eta \in \text{Causal}^º_{\pm}\) it holds that

\begin{multline}\label{splitting_result}
\lim_{\varepsilon \rightarrow 0} \hat{D}^{\alpha \beta}_{\pm}(l+i\varepsilon \eta)=\\
 \left(g^{\alpha \beta} - \frac{l^{\alpha} l^{\beta}}{l^2}\right)\left[ \pm \id_{l^2>4m^2} \text{sgn}(l^0) \frac{1}{2}\sqrt{1-  \frac{4m^2}{l^2}} \left( \frac{l^2}{4m^2}+\frac{1}{2}\right)  \right. \\
\left.  +\frac{1}{2\pi i}\left( 1+\frac{5}{3} \frac{l^2}{4m^2}-\left(1+ \frac{l^2} {2m^2}\right)\sqrt{1-\frac{4m^2}{l^2}}\arctan \left( \sqrt{\frac{l^2}{4m^2-l^2}} \right)\right)   \right]_.
\end{multline}
This is not true for the arguments fulfilling \(l^2=4m^2\); however, this is irrelevant since \(\hat{D}_{\pm}\) is to be understood as a distribution which means that changes on sets of Lebesgue measure zero are of no concern.


In order not to convolute notation too much we define 
\(\lim_{\varepsilon\rightarrow 0} \hat{D}^{\alpha \beta}_{\pm}(l+i\varepsilon \eta)=: \hat{D}^{\alpha \beta}_{\pm}(l)\), that is we extend 
the holomorphic functions \(\hat{D}^{\alpha \beta}_{\pm}\) to the boundary of their domain. 

Furthermore we will make use of the abbreviations 
\begin{align}
\mathcal{K}^{\alpha \beta} (k) &:=  \left(g^{\alpha \beta}  - \frac{k^{\alpha} k^{\beta}}{k^2}\right) \\
\mathcal{R}(k) &:= \id_{k^2>4m^2} \sqrt{1-  \frac{4m^2}{k^2}} \left( \frac{k^2}{4m^2}+\frac{1}{2}\right)\\\notag
\tau (k) &:=\frac{1}{\pi i}\left( 1 +\frac{5}{3} \frac{k^2}{4m^2}-\left(1+ \frac{k^2} {2m^2}\right) \cdot \right.\\
 &\left.  \cdot \sqrt{1-\frac{4m^2}{k^2}}\arctan \left( \sqrt{\frac{k^2}{4m^2-k^2}} \right)\right)_.
\end{align}

Theorem \ref{Paley-Wiener} guarantees 
us that \(\supp D^{\alpha \beta}_{\pm}\subset \text{Causal}_{\pm}\) holds. We use this property to compare the support properties of
 \(\supp D^{\alpha \beta}_{\pm}\) with the ones of \(A_1\) and \(R_1\) using the following short calculation: Substituting 
 \(\supp D^{\alpha \beta}_{+}\) for \(d^{\alpha \beta}\) in \eqref{D1_vacc} we see that

\begin{multline}\label{D+translation}
\langle \Omega, D_+ (f,g)\Omega \rangle :=-\frac{8 \pi m^4}{3} \int_{\mathbb{R}^4} D^{\alpha \beta}_{+}(k) f_\alpha (k) g_\beta (-k) d^4 k=\\
- \frac{2 m^4}{3 \pi} \int_{\mathbb{R}^4} \int_{\mathbb{R}^4} D^{\alpha \beta} (z-y) f_\alpha (y) g_\beta (z) d^4 z d^4 y
\end{multline}
holds. So we see that \(D_+\) vanishes whenever the support of \(g\) is earlier than the support of \(f\) or they lie 
acausally or a mixture of these conditions. That is it vanishes whenever \(\supp f\succ \supp g\) holds, which 
is exactly the support property of \(A_1\).  Since the analogous treatment holds for \(D_-\) and \(R_1\) and we 
know 

\begin{equation*}
\langle \Omega, D\Omega \rangle=\langle \Omega,A_1\Omega \rangle-\langle \Omega,R_1\Omega \rangle=\langle \Omega,D_+\Omega \rangle - \langle \Omega,D_-\Omega \rangle,
\end{equation*}

we identify \(\langle \Omega,A_1\Omega \rangle=\langle \Omega,D_+\Omega \rangle\) and 
\(\langle \Omega,R_1\Omega \rangle=\langle \Omega,D_-\Omega \rangle\). Going back to the definition 
of \(A_1\) we can now find \(\langle \Omega, T_2 \Omega \rangle \). We reuse the calculation resulting 
in \eqref{D1_vacc} to find

\begin{multline}
\langle \Omega, T_2 (f,g) \Omega \rangle = \langle \Omega, A_1 (f,g) \Omega \rangle - 
\langle \Omega, T_1^\dagger (f) T_1^\dagger (g) \Omega \rangle \\
= - \frac{8 \pi m^4}{3} \int_{\mathbb{R}^4} D_+^{\alpha \beta} (k) f_\alpha (k) g_\beta (-k) d^4k- \frac{8 \pi m^4}{3} \int_{\{k\in\mathbb{R}^4 \mid k^0>0, k^2 > 4 m^2 \}}\\
 \sqrt{1-\frac{4m^2}{k^2}} \left( \frac{k^2}{4m^2} + \frac{1}{2}\right) 
\left( g^{\alpha \beta} - \frac{k^\alpha k^\beta }{k^2}\right) f_\alpha (-k) g_\beta (k) d^4 k\\
= - \frac{8 \pi m^4}{3} \int_{\mathbb{R}^4} f_\alpha (k) g_\beta (-k) \mathcal{K}^{\alpha \beta} (k)
\left[ \mathcal{R}(k) \left\{ \frac{1}{2} \text{sgn} \left(k^0\right) + 1_{k^0<0}\right\} \right.\\
\left. +  \frac{1}{2}\tau(k) \right]
= \int_{\mathbb{R}^4} f_\alpha (k) g_\beta (-k) t_2^{\alpha \beta} (k) d^4 k,
\end{multline}
with 
\begin{equation}
t^{\alpha \beta}_2 (k) := - \frac{4 \pi m^4}{3}\mathcal{K}^{\alpha \beta }(k) \left[\mathcal{R}(k) + \tau(k) \right]
\end{equation}


\chapter{Mathematical Justification}


\appendix

\chapter[One Particle S-Matrix; Explicit Bounds][Explicit Bounds]{One Particle S-Matrix; Explicit Bounds}

We find the estimates of \(Z_k\) by using \eqref{explicit_Zk}. Let \(\psi \in \mathcal{H}\) arbitrary, \(\Sigma\) be an arbitrary spacelike hypersurface in Minkowski space,
\begin{multline*}
\left<\psi\right| \left. Z_k\phi(y)\right>= \int_{\Sigma} \bar{\psi}(y)\hspace{0.2cm} i_{\gamma} (\text{d}^4 y) (- i)  \int_{\mathcal{M}}\frac{i_p(\text{d}^4p_1)}{(2\pi)^{3}} \frac{\slashed{p}_1+m}{2m} e^{-ip_1y}  \\
  \prod_{l=2}^{k} \left[ \int_{\mathbb{R}^4+i \epsilon e_0}\frac{\text{d}^4p_l}{(2\pi)^{4}} \slashed{A}(p_{l-1}-p_l)  (\slashed{p}_l-m)^{-1}  
 \right]\\
 \int_{\mathcal{M}}  i_p(\text{d}^4p_{k+1})\slashed{A}(p_{k}-p_{k+1})\hat{\phi}(p_{k+1})
 =  - i  \int_{\mathcal{M}}\frac{i_p(\text{d}^4p_1)}{(2\pi)^{\frac{3}{2}}}\bar{\psi}(p_1)\hspace{0.2cm}  \frac{\slashed{p}_1+m}{2m}   \\
  \prod_{l=2}^{k} \left[ \int_{\mathbb{R}^4+i \epsilon e_0}\frac{\text{d}^4p_l}{(2\pi)^{4}} \slashed{A}(p_{l-1}-p_l)  (\slashed{p}_l-m)^{-1}  
 \right]\\
 \int_{\mathcal{M}}  i_p(\text{d}^4p_{k+1})\slashed{A}(p_{k}-p_{k+1})\hat{\phi}(p_{k+1})
  =  - i  \int_{\mathcal{M}}\frac{i_p(\text{d}^4p_1)}{(2\pi)^{\frac{3}{2}}}\overline{ \frac{\slashed{p}_1+m}{2m}\psi}(p_1)   \\
  \prod_{l=2}^{k} \left[ \int_{\mathbb{R}^4+i \epsilon e_0}\frac{\text{d}^4p_l}{(2\pi)^{4}} \slashed{A}(p_{l-1}-p_l)  (\slashed{p}_l-m)^{-1}  
 \right]\\
 \int_{\mathcal{M}}  i_p(\text{d}^4p_{k+1})\slashed{A}(p_{k}-p_{k+1})\hat{\phi}(p_{k+1})
   =  - i  \int_{\mathcal{M}}\frac{i_p(\text{d}^4p_1)}{(2\pi)^{\frac{3}{2}}}\overline{\psi}(p_1)   \\
  \prod_{l=2}^{k} \left[ \int_{\mathbb{R}^4+i \epsilon e_0}\frac{\text{d}^4p_l}{(2\pi)^{4}} \slashed{A}(p_{l-1}-p_l)  (\slashed{p}_l-m)^{-1}  
 \right]\\
 \int_{\mathcal{M}}  i_p(\text{d}^4p_{k+1})\slashed{A}(p_{k}-p_{k+1})\hat{\phi}(p_{k+1})
\end{multline*}

We therefore find for the operator norm of \(Z_k\):
\begin{multline}\label{Z_k estimate}
\|Z_k\|=\sup_{\psi,\phi \in \mathcal{H}}\frac{\left|\left<\psi\right| \left. Z_k\phi(y)\right>\right|}{\|\psi\| \|\phi\|}
=\sup_{\psi,\phi \in \mathcal{H}}\left|\int_{\mathcal{M}}\frac{i_p(\text{d}^4p_1)}{(2\pi)^{\frac{3}{2}}}\frac{\overline{\psi}(p_1)}{\|\psi\|}\right. \\
\left. \prod_{l=2}^{k} \left[ \int_{\mathbb{R}^4+i \epsilon e_0}\frac{\text{d}^4p_l}{(2\pi)^{4}} \slashed{A}(p_{l-1}-p_l)  (\slashed{p}_l-m)^{-1}  
 \right]\right.\\
 \left. \int_{\mathcal{M}}  i_p(\text{d}^4p_{k+1})\slashed{A}(p_{k}-p_{k+1})\frac{\hat{\phi}(p_{k+1})}{\|\phi\|}\right|\\
\letext{C.S.I.} \sup_{\phi\in\mathcal{H}}\int_{\mathcal{M}}\frac{i_p(\text{d}^4p_1)}{(2\pi)^{3}}
  \left|\prod_{l=2}^{k} \left[ \int_{\mathbb{R}^4+i \epsilon e_0}\frac{\text{d}^4p_l}{(2\pi)^{4}} \slashed{A}(p_{l-1}-p_l)  (\slashed{p}_l-m)^{-1}  
 \right]\right.\\
\left. \int_{\mathcal{M}}  i_p(\text{d}^4p_{k+1})\slashed{A}(p_{k}-p_{k+1})\frac{\hat{\phi}(p_{k+1})}{\|\phi\|}\right|^2\\
 \le \sup_{\phi\in\mathcal{H}}\int_{\mathcal{M}}\frac{i_p(\text{d}^4p_1)}{(2\pi)^{3}}
  \left(\prod_{l=2}^{k} \left[ \int_{\mathbb{R}^4+i \epsilon e_0}\frac{\text{d}^4p_l}{(2\pi)^{4}} \|\slashed{A}(p_{l-1}-p_l)\|_{\text{spec}}\right.\right.\\
  \left.\left. \| (\slashed{p}_l-m)^{-1}\|_{\text{spec}} \right]
 \int_{\mathcal{M}}  i_p(\text{d}^4p_{k+1})\|\slashed{A}(p_{k}-p_{k+1})\|_{\text{spec}}\frac{\|\hat{\phi}(p_{k+1})\|}{\|\phi\|}\right)^2\\
 \le \sup_{\lambda\in\mathbb{R}^4+i \epsilon e_0}\| (\slashed{\lambda}_l-m)^{-1}\|_{\text{spec}}^{k-1}   \sup_{\phi\in\mathcal{H}}\int_{\mathcal{M}}\frac{i_p(\text{d}^4p_1)}{(2\pi)^{3}}
  \left(\prod_{l=2}^{k} \left[ \int_{\mathbb{R}^4+i \epsilon e_0}\frac{\text{d}^4p_l}{(2\pi)^{4}}\right.\right.\\
  \left.\left. \|\slashed{A}(p_{l-1}-p_l)\|_{\text{spec}}  \right]
 \int_{\mathcal{M}}  i_p(\text{d}^4p_{k+1})\|\slashed{A}(p_{k}-p_{k+1})\|_{\text{spec}}\frac{\|\hat{\phi}(p_{k+1})\|}{\|\phi\|}\right)^2\\
 \letext{section \ref{Sec:SBound:Sec:richtig_abschaetzen}} \left(\frac{2}{\epsilon}\right)^{k-1}   \sup_{\phi\in\mathcal{H}}\int_{\mathcal{M}}\frac{i_p(\text{d}^4p_1)}{(2\pi)^{3}}
  \left(\prod_{l=2}^{k} \left[ \int_{\mathbb{R}^4+i \epsilon e_0}\frac{\text{d}^4p_l}{(2\pi)^{4}}\right.\right.\\
\left.\left.   \|\slashed{A}(p_{l-1}-p_l)\|_{\text{spec}}  \right]
 \int_{\mathcal{M}}  i_p(\text{d}^4p_{k+1})\|\slashed{A}(p_{k}-p_{k+1})\|_{\text{spec}}\frac{\|\hat{\phi}(p_{k+1})\|}{\|\phi\|}\right)^2.
 \end{multline}
 Where we assumed \(\varepsilon\) to be large enough so that the estimate in section 
 \ref{Sec:SBound:Sec:richtig_abschaetzen} holds.  The following estimation is only valid 
 for \(k>1\). We now apply the theorem of Parley and Wiener (e.g. \cite{reed1975methods})
  to all occurrences of  \(\slashed{A}\).  Since \(A\) is compactly supported in Minkowski
  spacetime its Fourier transform fulfills:
  \begin{equation}
  \forall N \in \mathbb{N}: \exists C_N\in\mathbb{R}: \forall p \in \mathbb{C}^4 \| \slashed{\hat{A}}\|(p) \le \frac{C_N 8 \pi }{1+|p|^N} e^{\frac{1}{2}\left|\Im p \right| \text{diam}(A) },
  \end{equation}
where diam\((A)\) is the diameter of the support of \(A\) in Minkowski spacetime 
and the constant in the numerator was slightly modified to simplify our notation. 
 \begin{multline}
\le \left(\frac{C_N}{\epsilon}\right)^{k-1} e^{\epsilon \text{diam}(\text{supp}(A)) } \frac{1}{\pi^2}  \sup_{\phi\in\mathcal{H}}\int_{\mathcal{M}} i_p(\text{d}^4p_1) \\ 
 \left(\prod_{l=2}^{k} \left[ \int_{\mathbb{R}^4+i \epsilon e_0} \text{d}^4p_l \frac{1}{(1+|p_{l-1}-p_l |)^N}
 \right] \int_{\mathcal{M}}  i_p(\text{d}^4p_{k+1})\frac{1}{(1+|p_{k}-p_{k+1} |)^N}\frac{\|\hat{\phi}(p_{k+1})\|}{\|\phi\|}\right)^2\\
= \left(\frac{C_N}{\epsilon}\right)^{k-1} e^{\epsilon \text{diam}(\text{supp}(A)) } \frac{1}{\pi^2}\\
  \sup_{\phi\in\mathcal{H}} \left\| 
\CONV_{\stackrel{l=2}{\mathbb{R}^4}}^{k} \left[  \frac{1}{(1+|\cdot |)^N}
  \hspace{0.2cm} ,\hspace{0.2cm}  \frac{1}{(1+|\cdot |)^N}\stackrel{\mathcal{M}}{\Conv}\frac{\|\hat{\phi}(\cdot)\|}{\|\phi\|}\right] \right\|_{\mathcal{L}^2(\mathcal{M})}
  \end{multline}
  We are going to use Young's inequality for convolution operators acting 
  \(L^2(\mathcal{M})\rightarrow L^2(\mathcal{M})\), the appropriate 
  lemma is found in appendix \ref{Sec:Young}
  \begin{multline}
  \letext{Young Inequ. \textcolor{red}{Raum?!} } \left(\frac{1}{\sqrt{2} \epsilon}\right)^{k-1} e^{\epsilon \text{diam}(\text{supp}(A)) } \frac{ C_N^k}{\pi^{4k-1}}  \left\|\frac{1}{(1+|\cdot |)^N} \right\|_{\mathcal{L}(\mathbb{R}^4)}^{k-1}\\
    \left\|\frac{1}{(1+|\cdot |)^N} \right\|_{\mathcal{L}(\mathcal{M})}   \sup_{\phi\in\mathcal{H}} \left\| 
\frac{\|\hat{\phi}(\cdot)\|}{\|\phi\|}\right\|_{\mathcal{L}^2(\mathcal{M})}\\
= \left(\frac{1}{\sqrt{2} \epsilon}\right)^{k-1} e^{\epsilon \text{diam}(\text{supp}(A)) } \frac{ C_N^k}{\pi^{4k-1}}  \left\|\frac{1}{(1+|\cdot |)^N} \right\|_{\mathcal{L}(\mathbb{R}^4)}^{k-1}  \left\|\frac{1}{(1+|\cdot |)^N} \right\|_{\mathcal{L}(\mathcal{M})} 
\end{multline}
Where \(C_N\) is the constant obtained by application of the theorem of Parley an Wiener, \(\epsilon\) is still an arbitrary positive number. This is why we now optimise over this parameter. In order to simplify the notation we define \(a:=\text{diam}(\text{supp}(A))\), \(b:= k-1\), \(f:=\left\|\frac{1}{(1+|\cdot |)^N} \right\|_{\mathcal{L}(\mathbb{R}^4)}\), \(g:=\left\|\frac{1}{(1+|\cdot |)^N} \right\|_{\mathcal{L}(\mathcal{M})} \) .
\begin{equation}
h: \mathbb{R}^+\rightarrow \mathbb{R},\hspace{1cm}\epsilon\mapsto\frac{e^{a \epsilon}}{\epsilon^b}
\end{equation}
h is a smooth positive function which diverges at zero and at infinity, so it must attain a minimum somewhere in between. We find this minimum by elementary calculus:
\begin{equation}
h'(\epsilon)\equaltext{!}0 \iff -b \frac{e^{a \epsilon}}{\epsilon^{b+1}}+ a \frac{e^{a \epsilon}}{\epsilon^b}=0 \iff -b +a \epsilon=0 \iff \epsilon= \frac{b}{a}
\end{equation}
Therefore the value of the minimum is:
\begin{equation}
\inf_{\epsilon\in\mathbb{R}^+} h(\epsilon)= h(\frac{b}{a})=\frac{e^b}{\left(\frac{b}{a}\right)^b}=\frac{(a e)^b}{b^b}
\end{equation}
Which means for the operator norm of \(Z_k\),  \(k>1\):
\begin{equation}
\|Z_k\|\le \left(\frac{1}{\sqrt{2} }\right)^{k-1} \frac{\left(e a\right)^{k-1}}{(k-1)^{k-1}} \frac{ C_N^k}{\pi^{4k-1}}  f^{k-1}  g
\end{equation}
This means that we can find the operator norm of the \(S\) operator, once we have read off the operator norm of \(Z_1\). In order to do so, we start at the end of  \eqref{Z_k estimate} and use the Young inequality right away to find:
\begin{equation}
\|Z_1\|\le \left\| \|\slashed{A}\|_{spec} \right\|_{\mathcal{L}^1(\mathcal{M})}
\end{equation}

Which is finite, because \(A\) is compactly supported, which means that its Fouriertransform falls off at infinity faster than any polynomial.
We will be using the well known upper bound for the factorial of an arbitrary number:
\begin{equation}\label{sterling}
n!\le \sqrt{2\pi n} \left(\frac{n}{e}\right)^n e^\frac{1}{12n}
\end{equation}
We will employ the abbreviation \(w= \frac{ a C_N f}{\pi^4\sqrt{2} }\)
\begin{multline}
\|S\|=\left\|\sum_{k=0}^\infty Z_k \right\|\le \sum_{k=0}^\infty \left\|Z_k \right\| \le 1+ \left\| \|\slashed{A}\|_{spec} \right\|_{\mathcal{L}^1(\mathcal{M})}+ \sum_{k=2}^\infty \left(\frac{1}{\sqrt{2} }\right)^{k-1} \frac{\left(e a\right)^{k-1}}{(k-1)^{k-1}} \frac{ C_N^k}{\pi^{4k-1}}  f^{k-1}  g\\
=1+ \left\| \|\slashed{A}\|_{spec} \right\|_{\mathcal{L}^1(\mathcal{M})}+g\frac{ C_N}{\pi^{3}} \sum_{k=2}^\infty \frac{\left(w e\right)^{k-1}}{(k-1)^{k-1}}
=1+ \left\| \|\slashed{A}\|_{spec} \right\|_{\mathcal{L}^1(\mathcal{M})}+g\frac{ C_N}{\pi^{3}} \sum_{k=1}^\infty \frac{w^{k}}{\left(\frac{k}{e}\right)^{k}}   \\
\letext{\eqref{sterling}}1+ \left\| \|\slashed{A}\|_{spec} \right\|_{\mathcal{L}^1(\mathcal{M})}+g\frac{ C_N}{\pi^{3}} \sum_{k=1}^\infty \frac{w^{k}}{k!} e^{\frac{1}{12k}} \sqrt{2\pi k} \\
\stackrel{e^{\frac{1}{12k}}\le \sqrt{k} e^{\frac{1}{12}}}{\le} \hspace{0.3cm} 1+ \left\| \|\slashed{A}\|_{spec} \right\|_{\mathcal{L}^1(\mathcal{M})}+e^{\frac{1}{12}} g\frac{ C_N\sqrt{2}}{\pi^{\frac{5}{2}}} \sum_{k=1}^\infty \frac{w^{k}}{k!}  k\\
=1+ \left\| \|\slashed{A}\|_{spec} \right\|_{\mathcal{L}^1(\mathcal{M})}+e^{\frac{1}{12}} g\frac{w C_N\sqrt{2}}{\pi^{\frac{5}{2}}} \sum_{l=0}^\infty \frac{w^{l}}{l!} 
=1+ \left\| \|\slashed{A}\|_{spec} \right\|_{\mathcal{L}^1(\mathcal{M})}+e^{\frac{1}{12}} g\frac{w C_N\sqrt{2}}{\pi^{\frac{5}{2}}} e^{w}\\
=1+ \left\| \|\slashed{A}\|_{spec} \right\|_{\mathcal{L}^1(\mathcal{M})}+ 2 \pi^{\frac{3}{2}}a g f  C^2_N e^{\frac{ a C_N f}{\pi^4\sqrt{2}}+\frac{1}{12}}\\
= 1+ \left\| \|\slashed{A}\|_{spec} \right\|_{\mathcal{L}^1(\mathcal{M})}+ \left\|\frac{1}{(1+|\cdot |)^N} \right\|_{\mathcal{L}(\mathbb{R}^4)} \left\|\frac{1}{(1+|\cdot |)^N} \right\|_{\mathcal{L}(\mathcal{M})} 2 \pi^{\frac{3}{2}}\text{diam}(\text{supp}(A))    C^2_N \\
 e^{\frac{ \text{diam}(\text{supp}(A)) C_N \left\|\frac{1}{(1+|\cdot |)^N} \right\|_{\mathcal{L}(\mathbb{R}^4)}}{\pi^4\sqrt{2}}+\frac{1}{12}} <\infty
\end{multline}

\section{Bound on \(\|(\slashed{\lambda}-m)^{-1}\|_{\text{spec}}\)}\label{Sec:SBound:Sec:richtig_abschaetzen}
In this section we will find an upper bound on  the supremum over all \(\lambda \in \mathbb{R}^4+i\varepsilon e_0\) of
\begin{equation}
\|(\slashed{\lambda}-m)^{-1}\|_{\text{spec}}= 
\left\| \frac{\slashed{\lambda}+m}{\lambda^2 - m^2}\right\|_{\text{spec}}.
\end{equation}
In order to do so, we will find a lower bound on the inverse of the expression in question. 
To simplify the notation call \(\left(\Re \lambda^0 \right)^2= x\ge 0\) and write out \(\Im \lambda= \varepsilon e_0\) explicitly.
Since the problem is symmetric in \(\lambda^0\) this suffices.
Furthermore, since nothing depends on the direction of \(\vec{\lambda}\), the problem is really just 
two-dimensional. Therefore we define \(r:=\|\vec{\lambda}\|^2>0\) and will only speak of these
quantities from now on. The object to minimize is
\begin{equation}
f_0(x,r):=\frac{\sqrt{(x-r -\varepsilon^2 -m^2)^2+ 4 \varepsilon^2 x}}{\sqrt{x+\varepsilon^2+r}+m}.
\end{equation}
We continue with the triangular inequality in the denominator and the concavity of the square root in the numerator giving.
\begin{multline}
f_0(x,r)\ge f_1(x,r):=\frac{\frac{1}{\sqrt{2}} \left|x-r-\varepsilon^2-m^2 \right| + \frac{1}{\sqrt{2}} 2 \varepsilon \sqrt{x}}{\sqrt{x}+\sqrt{r}+m+\varepsilon}\\
=\frac{1}{\sqrt{2}}\frac{\left|x-r-\varepsilon^2-m^2 \right| + 2 \varepsilon \sqrt{x}}{\sqrt{x}+\sqrt{r}+m+\varepsilon}.
\end{multline}

In order to find the minimum of this expression we will use the following strategy. First we find stationary points in  
\(M^+:=\{(x,r)\in{\mathbb{R}^+}^2\mid x>r+\varepsilon^2+m^2\}\) and 
\(M^+:=\{(x,r)\in{\mathbb{R}^+}^2\mid x<r+\varepsilon^2+m^2\}\), since there may be Minima on the boundary 
between these sets we also minimize \(f_1\) in \(M^0:=\{(x,r)\in{\mathbb{R}^+}^2\mid x=r+\varepsilon^2+m^2\}\).
Finally, since there might be no minimum, we find estimates on the boundary of \(M^+\cup M^-\cup M^0\).
\begin{enumerate}[label=case \alph*),]
\item \(x>r+\varepsilon^2+m^2\):

The gradient of \(f_1\) is
\begin{multline*}
\sqrt{2}(\sqrt{x}+\sqrt{r}+m+\varepsilon)^2 \nabla f_1(x,r)= \\
\begin{pmatrix}
\frac{1}{2}\sqrt{x}+\sqrt{r}+m+\varepsilon+\varepsilon\frac{m+\sqrt{r}}{\sqrt{x}}+\frac{r+m^2+3\varepsilon^2}{2\sqrt{x}}\\
-\sqrt{x}-\frac{\sqrt{r}}{2}-m-\varepsilon -\frac{x-\varepsilon^2-m^2}{2\sqrt{r}}-\varepsilon\sqrt{\frac{x}{r}}
\end{pmatrix},
\end{multline*}
since the first element of this vector is always positive, there are no stationary points in this case.

\item \(x<r+\varepsilon^2+m^2\): 

Here the gradient takes the form
\begin{multline*}
\sqrt{2}(\sqrt{x}+\sqrt{r}+m+\varepsilon)^2 \nabla f_1(x,r)= \\
\begin{pmatrix}
\frac{-\sqrt{x}}{2} -\sqrt{r}-m-\varepsilon+\varepsilon\frac{\sqrt{r}+ m}{\sqrt{x}}-\frac{m^2-\varepsilon^2+r}{2\sqrt{x}}\\
+\sqrt{x}+\frac{\sqrt{r}}{2}+m+\varepsilon -\frac{m^2+\varepsilon^2 -x}{2\sqrt{r}}-\varepsilon\sqrt{\frac{x}{r}}
\end{pmatrix}\\
=\begin{pmatrix}
\frac{-1}{\sqrt{x}} \left(\frac{x}{2}+\frac{r}{2} +\sqrt{x r}-\varepsilon (\sqrt{r}+ m)+\sqrt{x}(m+\varepsilon)+\frac{m^2-\varepsilon^2}{2}\right)\\
\frac{1}{\sqrt{r}}\left(\frac{x}{2}+\frac{r}{2}+\sqrt{xr}+\sqrt{r}(m+\varepsilon) -\varepsilon\sqrt{x}-\frac{m^2+\varepsilon^2 }{2}\right)
\end{pmatrix}_,
\end{multline*}
we can read off the relation 
\begin{equation}\label{AppA:case2}
\sqrt{x^*}=\sqrt{r^*}+\frac{m}{2}\frac{1-\frac{m}{\varepsilon}}{1+\frac{m}{2\varepsilon}}=:\sqrt{r}+\frac{m}{2} c,
\end{equation}
 which holds for stationary points \((x^*,r^*)\)  and use it to solve for them.
  If we want to make sure that the stationary point stays within \(M^-\) we have to ensure
  that \(x^*<r^*+m^2+\varepsilon^2\) for \((x^*,r^*\) being a solution to \(\nabla f_1(x,r)=0\).
This results in the condition
\begin{equation*}
r< \frac{1}{m^2}\left[ \frac{\varepsilon^2+m^2(1-\frac{1}{4}c^2)}{c}\right]^2=\frac{\varepsilon^4}{m^2}+ \mathcal{O}(\varepsilon^2).
\end{equation*}
Since for the estimation of the one particle scattering matrix we are interested in the 
 regime where \(\varepsilon\), this is the relevant estimation. We will shortly see that 
 \(r^*=\mathcal{O}(\varepsilon^2)\), therefore we need not worry about the stationary point
 being outside of \(M^-\) for \(\varepsilon\) large. Indeed, plugging the relation
  \eqref{AppA:case2} into \(\nabla f_1(x^*,r^*) \stackrel{!}{=}0\) and solving for \(r^*\) we find
  
\begin{equation*}
\sqrt{r^*}=-\frac{m}{4}(c+1) +\frac{1}{2} \sqrt{\varepsilon^2 + \frac{\varepsilon c m}{4} + \frac{m^2}{4}(5+2c)}.
\end{equation*}

One can immediately see that the right hand side is actually positive once one 
has restored the summand \(\frac{m^2}{4}(c+1)^2\) in the discriminant. 
By substituting Taylor's where appropriate we find for \(x^*,r^*\):
\begin{align*}
&\sqrt{r^*}=\frac{\varepsilon}{2}- \frac{3}{8} m + \frac{m^2}{\varepsilon} \frac{91}{128} + \mathcal{O}\left(\frac{1}{\varepsilon}\right)\\
&\sqrt{x^*}=\frac{\varepsilon}{2}+ \frac{1}{8} m - \frac{m^2}{\varepsilon} \frac{5}{128} + \mathcal{O}\left(\frac{1}{\varepsilon}\right)\\
&x^*=\frac{\varepsilon^2}{4} + \varepsilon m \frac{1}{8} - m^2 \frac{3}{128}+ \mathcal{O}\left(\frac{1}{\varepsilon}\right)\\
&r^*=\frac{\varepsilon^2}{4} - \varepsilon m \frac{3}{8} + m^2 \frac{109}{128}+ \mathcal{O}\left(\frac{1}{\varepsilon}\right),
\end{align*}
yielding for the stationary point
\begin{equation}
f_1(x^*,r^*)= \frac{\varepsilon}{\sqrt{2}} + \frac{m}{4 \sqrt{2}}+ \mathcal{O}\left(\frac{1}{\varepsilon}\right).
\end{equation}
 
\item \label{bla} \(x=r+\varepsilon^2+m^2\):

Plugging this into \(f_1\) gives us
\begin{equation*}
f_1(r+\varepsilon^2+m^2,r)=:f_2(r)= \sqrt{2} \varepsilon \frac{\sqrt{r+\varepsilon^2+m^2}}{\sqrt{r+\varepsilon^2+m^2}+\sqrt{r}+m+\varepsilon}
\end{equation*}
to minimise. The derivative of this function is given by
\begin{multline*}
\sqrt{2} (\sqrt{r+\varepsilon^2+m^2}+\sqrt{r}+m+\varepsilon)^2 f_2'(r)\\
= \varepsilon \left( \frac{\sqrt{r}+m+\varepsilon}{\sqrt{r+m^2+\varepsilon^2}}- \sqrt{1+\frac{m^2+\varepsilon^2}{r}}\right).
\end{multline*}
From the derivative we can read off that the function has a minimum at 
\(\sqrt{r^*}=\varepsilon \frac{1+\frac{m^2}{\varepsilon^2}}{1+\frac{m}{\varepsilon}}\).
We estimate this value, its square and the minimum to be
\begin{align}\tag*{}
\sqrt{r^*}&= \varepsilon - m + \mathcal{O}\left( \frac{1}{\varepsilon}\right)\\ \tag*{}
r^*&= \varepsilon^2 - 2 \varepsilon m +\mathcal{O}\left( 1\right)\\
f_2(r^*)&= (2-\sqrt{2})\varepsilon - (3-2\sqrt{2})m + \mathcal{O}\left( \frac{1}{\varepsilon}\right).
\end{align}

\item \(x=0\):

In this case \(f_1\) simplifies to
\begin{equation*}
f_1(0,r)=:f_3(r)=\frac{1}{\sqrt{2}} \frac{r+\varepsilon^2+m^2}{\sqrt{r}+m+\varepsilon},
\end{equation*}
its derivative is given by
\begin{equation*}
2\sqrt{2}(\sqrt{r}+m+\varepsilon)^2 \sqrt{r} f_3'(r)=r + 2 \sqrt{r}(m+\varepsilon) - \varepsilon^2-m^2.
\end{equation*}
So we read off that \(f_3\) has a minimum at \(\sqrt{r^*}=-m-\varepsilon + \sqrt{2} \sqrt{\varepsilon^2+\varepsilon m+m^2}\).
The same approximations as above yield
\begin{align}\tag*{}
\sqrt{r^*}&=\varepsilon (\sqrt{2}-1)- \frac{2-\sqrt{2}}{2} m + \mathcal{O}\left( \frac{1}{\varepsilon}\right)\\\tag*{}
r^*&= \varepsilon^2 (3-2\sqrt{2}) - (3\sqrt{2}  - 4) \varepsilon m + \mathcal{O}(1)\\
f_3(r^*)&= \varepsilon (2+ \sqrt{2}) - m \left( \frac{3}{2\sqrt{2}} -1 \right) + \mathcal{O}\left( \frac{1}{\varepsilon}\right).
\end{align}

\item \(x\rightarrow \infty\):

In this case \(f_1\) diverges to \(+\infty\).

\item \(r=0\):

In this case \(f_1\) reduces to

\begin{equation*}
f_1(x,0):=f_4(x)= \frac{1}{\sqrt{2}} \frac{ |x-\varepsilon^2-m^2| + 2 \varepsilon \sqrt{x}}{\sqrt{x}+m+\varepsilon},
\end{equation*}
for \(x\neq \varepsilon^2+m^2\) its derivative is given by
\begin{multline*}
\sqrt{2x} (\sqrt{x}+m+\varepsilon)^2 f_4'(x)\\
= \frac{1}{2}\text{sgn}(x-\varepsilon^2-m^2)(x+\varepsilon^2+m^2)\\
+\sqrt{x} (m+\varepsilon) \text{sgn}(x-\varepsilon^2-m^2) + \varepsilon m + \varepsilon^2.
\end{multline*}
For \(\varepsilon\) large this function has a minimum at the
 kink and a maximum between 0 and \(\varepsilon^2+m^2\).  So
 we take note of the minimum at the kink and
  the minimum for \(x\rightarrow 0\). These values are
\begin{align}
f_4(0)&= \frac{\varepsilon}{\sqrt{2}} \frac{1+\frac{m^2}{\varepsilon^2}}{1+\frac{m}{\varepsilon}}= \frac{\varepsilon}{\sqrt{2}} - \frac{m}{\sqrt{2}}+ \mathcal{O}\left(\frac{1}{\varepsilon}\right)\\
f_4(\varepsilon^2+m^2)&=
\frac{\sqrt{2} \varepsilon }{1+ \frac{1+ \frac{m}{\varepsilon}}{\sqrt{1+\frac{m^2}{\varepsilon^2}}}}= \frac{\varepsilon}{\sqrt{2}} - \frac{m}{2\sqrt{2}} + \mathcal{O}\left(\frac{1}{\varepsilon}\right)
\end{align}
 
\item  \(r\rightarrow \infty\):

In this case holds \(f_1\rightarrow \infty\). 

\item simultaneous limits \(x,r\rightarrow \infty\). The non trivial limits \(\sqrt{x}=\sqrt{r}+c'\) 
for \(c'\in\mathbb{R}\) and \(x-r=c''\) for \(c''\in\mathbb{R}\) all give limits equal to or greater than
\(\frac{\varepsilon}{\sqrt{2}}\). 
 
\end{enumerate}
Therefore the global minimum is the minimum of \ref{bla}, which is 
\((2-\sqrt{2})\varepsilon - (3-2\sqrt{2})m + \mathcal{O}\left(\frac{1}{\varepsilon}\right)\).
So for \(\varepsilon\) large enough relative to \(m\) we found the 
lower bound \(\frac{\varepsilon}{2}\) of \(f_1\). So overall
\begin{equation}
\sup_{\lambda\in \mathbb{R}^4+\varepsilon i e_0} \left\| (\slashed{\lambda} - m)^{-1} \right\|_{spec} \le \frac{2}{\varepsilon}
\end{equation}
holds.


\section{Young's Inequality on \(L^2(\mathcal{M})\)}\label{Sec:Young}



\backmatter

%\chapter{Bibliography concerning the state of the art, the research objectives, and the work program} 
%\vspace*{-0.68cm}

%\begingroup
%\renewcommand{\section}[2]{}
\bibliographystyle{amsplain}
\bibliography{ref}

%\bibliography{../../aarbeit/felix}
%\include{bib}
%\endgroup

\chapter{Cooperating Researchers} 
Prof. Dr. Franz Merkl (LMU)\\
Junior Research Group Leader Dr. Dirk Deckert (LMU)


\end{document}

